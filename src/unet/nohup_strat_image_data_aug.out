----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2, 0.2]
Stratified: image
Batch size: 16
Train: 6816 images, Val: 2125 images, Test: 1722 images
Train: 63.92%, Val: 19.93%, Test: 16.15%
Shape of images and masks:
Image shape: torch.Size([16, 4, 256, 256])
Creating model...
Model settings:
Encoder name: efficientnet-b7
Pretrained: None
Classes: 6
Creating optimizer...
Training settings:
Learning rate: 0.0001
Criterion: Dice
Optimizer: Adam
Training...
Epoch 1/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 1/80: train loss 0.7522, val loss 0.5339
Epoch 1/80: train mIoU 0.1432, val mIoU 0.1149
Time: 0:14:19.678629
Epoch 2/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 2/80: train loss 0.6788, val loss 0.5162
Epoch 2/80: train mIoU 0.1978, val mIoU 0.1737
Time: 0:14:13.700171
Epoch 3/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 3/80: train loss 0.6218, val loss 0.4962
Epoch 3/80: train mIoU 0.2428, val mIoU 0.1957
Time: 0:14:31.062986
Epoch 4/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 4/80: train loss 0.5811, val loss 0.5062
Epoch 4/80: train mIoU 0.2699, val mIoU 0.1807
Time: 0:14:14.257695
Epoch 5/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 5/80: train loss 0.5599, val loss 0.4865
Epoch 5/80: train mIoU 0.2943, val mIoU 0.2050
Time: 0:14:14.234075
Epoch 6/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 6/80: train loss 0.5435, val loss 0.4768
Epoch 6/80: train mIoU 0.3032, val mIoU 0.2204
Time: 0:14:15.154513
Epoch 7/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 7/80: train loss 0.5219, val loss 0.4640
Epoch 7/80: train mIoU 0.3239, val mIoU 0.2569
Time: 0:14:14.234011
Epoch 8/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 8/80: train loss 0.5196, val loss 0.4596
Epoch 8/80: train mIoU 0.3341, val mIoU 0.2612
Time: 0:14:15.334344
Epoch 9/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 9/80: train loss 0.5045, val loss 0.4790
Epoch 9/80: train mIoU 0.3440, val mIoU 0.2278
Time: 0:14:12.607118
Epoch 10/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 10/80: train loss 0.4991, val loss 0.4750
Epoch 10/80: train mIoU 0.3498, val mIoU 0.2472
Time: 0:14:13.641253
Epoch 11/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 11/80: train loss 0.4832, val loss 0.4667
Epoch 11/80: train mIoU 0.3574, val mIoU 0.2705
Time: 0:14:12.433540
Epoch 12/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 12/80: train loss 0.4802, val loss 0.4853
Epoch 12/80: train mIoU 0.3626, val mIoU 0.2046
Time: 0:14:13.489409
Epoch 13/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 13/80: train loss 0.4742, val loss 0.4889
Epoch 13/80: train mIoU 0.3703, val mIoU 0.1992
Time: 0:14:13.291396
Epoch 14/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 14/80: train loss 0.4755, val loss 0.4772
Epoch 14/80: train mIoU 0.3717, val mIoU 0.2201
Time: 0:14:13.763090
Epoch 15/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 15/80: train loss 0.4659, val loss 0.4589
Epoch 15/80: train mIoU 0.3824, val mIoU 0.2575
Time: 0:14:14.347369
Epoch 16/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 16/80: train loss 0.4659, val loss 0.4738
Epoch 16/80: train mIoU 0.3749, val mIoU 0.2504
Time: 0:14:14.056902
Epoch 17/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 17/80: train loss 0.4627, val loss 0.4736
Epoch 17/80: train mIoU 0.3817, val mIoU 0.2412
Time: 0:14:13.958658
Epoch 18/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 18/80: train loss 0.4478, val loss 0.4695
Epoch 18/80: train mIoU 0.3935, val mIoU 0.2417
Time: 0:14:13.272703
Epoch 19/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 19/80: train loss 0.4545, val loss 0.4835
Epoch 19/80: train mIoU 0.3914, val mIoU 0.2338
Time: 0:14:13.682805
Epoch 20/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 20/80: train loss 0.4439, val loss 0.4801
Epoch 20/80: train mIoU 0.4041, val mIoU 0.2226
Time: 0:14:12.801856
Epoch 21/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 21/80: train loss 0.4437, val loss 0.4676
Epoch 21/80: train mIoU 0.4075, val mIoU 0.2689
Time: 0:14:12.964775
Epoch 22/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 22/80: train loss 0.4459, val loss 0.4553
Epoch 22/80: train mIoU 0.4016, val mIoU 0.2908
Time: 0:14:14.388872
Epoch 23/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 23/80: train loss 0.4317, val loss 0.4840
Epoch 23/80: train mIoU 0.4077, val mIoU 0.2163
Time: 0:15:02.244603
Epoch 24/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 24/80: train loss 0.4314, val loss 0.4571
Epoch 24/80: train mIoU 0.4139, val mIoU 0.2878
Time: 0:14:18.736020
Epoch 25/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 25/80: train loss 0.4321, val loss 0.4679
Epoch 25/80: train mIoU 0.4154, val mIoU 0.2674
Time: 0:14:13.627343
Epoch 26/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 26/80: train loss 0.4288, val loss 0.4742
Epoch 26/80: train mIoU 0.4165, val mIoU 0.2496
Time: 0:14:12.890486
Epoch 27/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 27/80: train loss 0.4192, val loss 0.4723
Epoch 27/80: train mIoU 0.4289, val mIoU 0.2602
Time: 0:14:13.024653
Epoch 28/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 28/80: train loss 0.4233, val loss 0.4712
Epoch 28/80: train mIoU 0.4228, val mIoU 0.2460
Time: 0:14:12.405030
Epoch 29/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 29/80: train loss 0.4166, val loss 0.4776
Epoch 29/80: train mIoU 0.4292, val mIoU 0.2374
Time: 0:14:12.755372
Epoch 30/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 30/80: train loss 0.4113, val loss 0.4686
Epoch 30/80: train mIoU 0.4387, val mIoU 0.2503
Time: 0:14:12.873415
Epoch 31/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 31/80: train loss 0.4149, val loss 0.4691
Epoch 31/80: train mIoU 0.4392, val mIoU 0.2548
Time: 0:14:13.546957
Epoch 32/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 32/80: train loss 0.4075, val loss 0.4730
Epoch 32/80: train mIoU 0.4434, val mIoU 0.2427
Time: 0:14:13.237864
Epoch 33/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 33/80: train loss 0.4005, val loss 0.4680
Epoch 33/80: train mIoU 0.4472, val mIoU 0.2488
Time: 0:14:13.665589
Epoch 34/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 34/80: train loss 0.4027, val loss 0.4620
Epoch 34/80: train mIoU 0.4533, val mIoU 0.2719
Time: 0:14:13.131671
Epoch 35/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 35/80: train loss 0.3970, val loss 0.4657
Epoch 35/80: train mIoU 0.4523, val mIoU 0.2518
Time: 0:14:12.382281
Epoch 36/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 36/80: train loss 0.4054, val loss 0.4584
Epoch 36/80: train mIoU 0.4456, val mIoU 0.2722
/home/bertille/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Time: 0:14:12.621724
Epoch 37/80
Training
Batch: 0  over  426
Batch: 50  over  426
Batch: 100  over  426
Batch: 150  over  426
Batch: 200  over  426
Batch: 250  over  426
Batch: 300  over  426
Batch: 350  over  426
Batch: 400  over  426
Validation
Batch: 0  over  133
Batch: 50  over  133
Batch: 100  over  133
Epoch 37/80: train loss 0.3963, val loss 0.4743
Epoch 37/80: train mIoU 0.4559, val mIoU 0.2491
Early stopping at epoch 37
Testing
Batch: 0  over  108
Batch: 50  over  108
Batch: 100  over  108
Test IoU by class: {0: 0.19876975027832464, 1: 0.17221469030695272, 2: 0.665421725729487, 3: 0.23779077614742572, 4: 0.31763807720973175, 5: 0.0534852906805651}
Test F1 by class: {0: 0.3316228996138336, 1: 0.2938278998395031, 2: 0.7991029724774599, 3: 0.3842180451328616, 4: 0.48213251074585106, 5: 0.10153970094069925}
Test mIoU: 0.27422005172541447
Test mF1: 0.39874067145836806
