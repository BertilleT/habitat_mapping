----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.68, 0.14]
Stratified: zone
Batch size: 16
No data augmentation
Train: 6345 images, Val: 2400 images, Test: 1918 images
Train: 59.50%, Val: 22.51%, Test: 17.99%
Creating model...
Model settings:
Encoder name: efficientnet-b7
Pretrained: None
Classes: 6
Creating optimizer...
Training settings:
Learning rate: 0.0001
Criterion: Dice
Optimizer: Adam
Training...
Epoch 1/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 1/50: train loss 0.6446, val loss 0.5031
Epoch 1/50: train mIoU 0.2778, val mIoU 0.0574
Time: 0:13:31.955526
Epoch 2/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 2/50: train loss 0.5140, val loss 0.3844
Epoch 2/50: train mIoU 0.3650, val mIoU 0.1986
Time: 0:13:32.114097
Epoch 3/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 3/50: train loss 0.4620, val loss 0.3777
Epoch 3/50: train mIoU 0.4016, val mIoU 0.2331
Time: 0:13:31.183960
Epoch 4/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 4/50: train loss 0.4448, val loss 0.3762
Epoch 4/50: train mIoU 0.4157, val mIoU 0.2329
Time: 0:13:33.766825
Epoch 5/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 5/50: train loss 0.4259, val loss 0.3770
Epoch 5/50: train mIoU 0.4365, val mIoU 0.2177
Time: 0:13:28.481743
Epoch 6/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 6/50: train loss 0.4132, val loss 0.3696
Epoch 6/50: train mIoU 0.4437, val mIoU 0.2322
Time: 0:13:27.659912
Epoch 7/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 7/50: train loss 0.4076, val loss 0.3877
Epoch 7/50: train mIoU 0.4536, val mIoU 0.2195
Time: 0:13:30.437694
Epoch 8/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 8/50: train loss 0.3990, val loss 0.3823
Epoch 8/50: train mIoU 0.4665, val mIoU 0.2180
Time: 0:13:27.968414
Epoch 9/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 9/50: train loss 0.3812, val loss 0.3805
Epoch 9/50: train mIoU 0.4782, val mIoU 0.2238
Time: 0:13:33.014000
Epoch 10/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 10/50: train loss 0.3890, val loss 0.3743
Epoch 10/50: train mIoU 0.4755, val mIoU 0.2433
Time: 0:13:31.492994
Epoch 11/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 11/50: train loss 0.3764, val loss 0.3643
Epoch 11/50: train mIoU 0.4885, val mIoU 0.2508
Time: 0:13:35.028168
Epoch 12/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 12/50: train loss 0.3671, val loss 0.3753
Epoch 12/50: train mIoU 0.4908, val mIoU 0.2391
Time: 0:13:37.300717
Epoch 13/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 13/50: train loss 0.3604, val loss 0.3699
Epoch 13/50: train mIoU 0.5028, val mIoU 0.2402
Time: 0:13:36.665554
Epoch 14/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 14/50: train loss 0.3641, val loss 0.3927
Epoch 14/50: train mIoU 0.4989, val mIoU 0.2138
Time: 0:13:36.314821
Epoch 15/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 15/50: train loss 0.3634, val loss 0.3732
Epoch 15/50: train mIoU 0.5048, val mIoU 0.2406
Time: 0:13:35.595830
Epoch 16/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 16/50: train loss 0.3596, val loss 0.3725
Epoch 16/50: train mIoU 0.5086, val mIoU 0.2480
Time: 0:13:36.966515
Epoch 17/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 17/50: train loss 0.3513, val loss 0.3793
Epoch 17/50: train mIoU 0.5187, val mIoU 0.2175
Time: 0:13:23.614126
Epoch 18/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 18/50: train loss 0.3441, val loss 0.3752
Epoch 18/50: train mIoU 0.5230, val mIoU 0.2344
Time: 0:13:13.646290
Epoch 19/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 19/50: train loss 0.3478, val loss 0.3824
Epoch 19/50: train mIoU 0.5239, val mIoU 0.2157
Time: 0:13:12.692570
Epoch 20/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 20/50: train loss 0.3439, val loss 0.3732
Epoch 20/50: train mIoU 0.5258, val mIoU 0.2381
Time: 0:13:12.992847
Epoch 21/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 21/50: train loss 0.3318, val loss 0.3695
Epoch 21/50: train mIoU 0.5412, val mIoU 0.2361
Time: 0:13:11.886269
Epoch 22/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 22/50: train loss 0.3278, val loss 0.3847
Epoch 22/50: train mIoU 0.5440, val mIoU 0.2232
Time: 0:13:16.245916
Epoch 23/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 23/50: train loss 0.3343, val loss 0.3707
Epoch 23/50: train mIoU 0.5423, val mIoU 0.2420
Time: 0:13:15.638045
Epoch 24/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 24/50: train loss 0.3263, val loss 0.3719
Epoch 24/50: train mIoU 0.5517, val mIoU 0.2400
Time: 0:13:14.187641
Epoch 25/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 25/50: train loss 0.3298, val loss 0.3726
Epoch 25/50: train mIoU 0.5532, val mIoU 0.2371
/home/bertille/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/bertille/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Time: 0:13:14.752370
Epoch 26/50
Training
Batch: 0  over  397
Batch: 50  over  397
Batch: 100  over  397
Batch: 150  over  397
Batch: 200  over  397
Batch: 250  over  397
Batch: 300  over  397
Batch: 350  over  397
Validation
Batch: 0  over  150
Batch: 50  over  150
Batch: 100  over  150
Epoch 26/50: train loss 0.3290, val loss 0.3706
Epoch 26/50: train mIoU 0.5546, val mIoU 0.2414
Early stopping at epoch 26
Testing
Batch: 0  over  120
Batch: 50  over  120
Batch: 100  over  120
Test IoU by class: {0: 0.21263820497709035, 1: 0.0993370751612196, 2: 0.27652195870246743, 3: 0.1061282356565337, 4: 0.2882189316483412, 5: 0.09691462856929223}
Test F1 by class: {0: 0.35070345648743206, 1: 0.1807217775251538, 2: 0.4332427763068655, 3: 0.1918913779351128, 4: 0.44746886506247896, 5: 0.1767040497868064}
Test mIoU: 0.17995983911915744
Test mF1: 0.2967887171839749
