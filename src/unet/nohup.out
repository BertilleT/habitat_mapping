----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2, 0.2]
Stratified: random
Batch size: 16
Train: 6397 images, Val: 2133 images, Test: 2133 images
Train: 59.99%, Val: 20.00%, Test: 20.00%
Shape of images and masks:
Image shape: torch.Size([16, 4, 256, 256])
Creating model...
Model settings:
Encoder name: efficientnet-b7
Pretrained: imagenet
Classes: 6
Creating optimizer...
Training settings:
Learning rate: 0.0001
Criterion: Dice
Optimizer: Adam
Training...
Epoch 1/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 1/100: train loss 0.6156, val loss 0.4734
Epoch 1/100: train mIoU 0.3006, val mIoU 0.4181
Time: 0:13:30.274795
Epoch 2/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 2/100: train loss 0.4334, val loss 0.3813
Epoch 2/100: train mIoU 0.4465, val mIoU 0.5187
Time: 0:13:26.963425
Epoch 3/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 3/100: train loss 0.3656, val loss 0.3521
Epoch 3/100: train mIoU 0.5061, val mIoU 0.5541
Time: 0:13:34.778302
Epoch 4/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 4/100: train loss 0.3214, val loss 0.3625
Epoch 4/100: train mIoU 0.5554, val mIoU 0.5245
Time: 0:13:35.655261
Epoch 5/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 5/100: train loss 0.3024, val loss 0.3323
Epoch 5/100: train mIoU 0.5821, val mIoU 0.5698
Time: 0:13:38.108884
Epoch 6/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 6/100: train loss 0.2837, val loss 0.3245
Epoch 6/100: train mIoU 0.6015, val mIoU 0.5830
Time: 0:13:37.117370
Epoch 7/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 7/100: train loss 0.2776, val loss 0.3383
Epoch 7/100: train mIoU 0.6171, val mIoU 0.5674
Time: 0:13:37.754966
Epoch 8/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 8/100: train loss 0.2657, val loss 0.3190
Epoch 8/100: train mIoU 0.6277, val mIoU 0.5881
Time: 0:13:36.362304
Epoch 9/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 9/100: train loss 0.2606, val loss 0.3269
Epoch 9/100: train mIoU 0.6454, val mIoU 0.5660
Time: 0:13:34.896098
Epoch 10/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 10/100: train loss 0.2359, val loss 0.3090
Epoch 10/100: train mIoU 0.6721, val mIoU 0.5973
Time: 0:13:37.391818
Epoch 11/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 11/100: train loss 0.2257, val loss 0.3064
Epoch 11/100: train mIoU 0.6876, val mIoU 0.6000
Time: 0:13:34.722573
Epoch 12/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 12/100: train loss 0.2251, val loss 0.3134
Epoch 12/100: train mIoU 0.6814, val mIoU 0.5917
Time: 0:13:36.455286
Epoch 13/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 13/100: train loss 0.2184, val loss 0.3036
Epoch 13/100: train mIoU 0.7037, val mIoU 0.5973
Time: 0:13:36.224346
Epoch 14/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 14/100: train loss 0.2049, val loss 0.3080
Epoch 14/100: train mIoU 0.7180, val mIoU 0.6020
Time: 0:13:36.109289
Epoch 15/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 15/100: train loss 0.2056, val loss 0.3030
Epoch 15/100: train mIoU 0.7246, val mIoU 0.6104
Time: 0:13:33.772478
Epoch 16/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 16/100: train loss 0.1892, val loss 0.3017
Epoch 16/100: train mIoU 0.7439, val mIoU 0.6045
Time: 0:13:37.943328
Epoch 17/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 17/100: train loss 0.1919, val loss 0.3057
Epoch 17/100: train mIoU 0.7470, val mIoU 0.6063
Time: 0:13:38.907247
Epoch 18/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 18/100: train loss 0.1739, val loss 0.2973
Epoch 18/100: train mIoU 0.7698, val mIoU 0.6173
Time: 0:13:35.209965
Epoch 19/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 19/100: train loss 0.1671, val loss 0.2921
Epoch 19/100: train mIoU 0.7783, val mIoU 0.6153
Time: 0:13:34.090071
Epoch 20/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 20/100: train loss 0.1610, val loss 0.2923
Epoch 20/100: train mIoU 0.7893, val mIoU 0.6159
Time: 0:13:32.084858
Epoch 21/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 21/100: train loss 0.1572, val loss 0.2929
Epoch 21/100: train mIoU 0.8002, val mIoU 0.6221
Time: 0:13:34.877721
Epoch 22/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 22/100: train loss 0.1526, val loss 0.2903
Epoch 22/100: train mIoU 0.7981, val mIoU 0.6249
Time: 0:13:23.918904
Epoch 23/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 23/100: train loss 0.1490, val loss 0.3033
Epoch 23/100: train mIoU 0.8134, val mIoU 0.6120
Time: 0:13:26.315801
Epoch 24/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 24/100: train loss 0.1427, val loss 0.2909
Epoch 24/100: train mIoU 0.8115, val mIoU 0.6241
Time: 0:13:36.725912
Epoch 25/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 25/100: train loss 0.1511, val loss 0.3289
Epoch 25/100: train mIoU 0.8001, val mIoU 0.5764
Time: 0:13:37.978383
Epoch 26/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 26/100: train loss 0.1474, val loss 0.2958
Epoch 26/100: train mIoU 0.8089, val mIoU 0.6175
Time: 0:13:36.268476
Epoch 27/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 27/100: train loss 0.1377, val loss 0.2954
Epoch 27/100: train mIoU 0.8199, val mIoU 0.6204
Time: 0:13:32.269468
Epoch 28/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 28/100: train loss 0.1460, val loss 0.3147
Epoch 28/100: train mIoU 0.8136, val mIoU 0.5882
Time: 0:13:30.530467
Epoch 29/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 29/100: train loss 0.1424, val loss 0.3071
Epoch 29/100: train mIoU 0.8126, val mIoU 0.5955
Time: 0:13:33.357283
Epoch 30/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 30/100: train loss 0.1321, val loss 0.2957
Epoch 30/100: train mIoU 0.8299, val mIoU 0.6140
Time: 0:13:29.450535
Epoch 31/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 31/100: train loss 0.1268, val loss 0.2939
Epoch 31/100: train mIoU 0.8452, val mIoU 0.6145
Time: 0:13:32.183042
Epoch 32/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 32/100: train loss 0.1194, val loss 0.2915
Epoch 32/100: train mIoU 0.8475, val mIoU 0.6195
Time: 0:13:35.644437
Epoch 33/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 33/100: train loss 0.1272, val loss 0.3014
Epoch 33/100: train mIoU 0.8425, val mIoU 0.6093
Time: 0:13:33.376728
Epoch 34/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 34/100: train loss 0.1194, val loss 0.2882
Epoch 34/100: train mIoU 0.8492, val mIoU 0.6220
Time: 0:13:32.384646
Epoch 35/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 35/100: train loss 0.1152, val loss 0.2898
Epoch 35/100: train mIoU 0.8573, val mIoU 0.6201
Time: 0:13:32.730274
Epoch 36/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 36/100: train loss 0.1105, val loss 0.3012
Epoch 36/100: train mIoU 0.8615, val mIoU 0.6179
Time: 0:13:31.807467
Epoch 37/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 37/100: train loss 0.1154, val loss 0.2826
Epoch 37/100: train mIoU 0.8618, val mIoU 0.6311
Time: 0:13:36.338165
Epoch 38/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 38/100: train loss 0.1109, val loss 0.2815
Epoch 38/100: train mIoU 0.8664, val mIoU 0.6315
Time: 0:13:39.512937
Epoch 39/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 39/100: train loss 0.1059, val loss 0.2804
Epoch 39/100: train mIoU 0.8688, val mIoU 0.6294
Time: 0:13:33.598739
Epoch 40/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 40/100: train loss 0.1102, val loss 0.2958
Epoch 40/100: train mIoU 0.8675, val mIoU 0.6194
Time: 0:13:31.311439
Epoch 41/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 41/100: train loss 0.1115, val loss 0.2959
Epoch 41/100: train mIoU 0.8675, val mIoU 0.6179
Time: 0:13:35.299128
Epoch 42/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 42/100: train loss 0.0987, val loss 0.2848
Epoch 42/100: train mIoU 0.8746, val mIoU 0.6318
Time: 0:13:19.249146
Epoch 43/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 43/100: train loss 0.1068, val loss 0.2941
Epoch 43/100: train mIoU 0.8739, val mIoU 0.6183
Time: 0:13:11.605063
Epoch 44/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 44/100: train loss 0.1041, val loss 0.2870
Epoch 44/100: train mIoU 0.8762, val mIoU 0.6264
Time: 0:13:32.057537
Epoch 45/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 45/100: train loss 0.1077, val loss 0.2897
Epoch 45/100: train mIoU 0.8784, val mIoU 0.6205
Time: 0:13:11.268280
Epoch 46/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 46/100: train loss 0.0961, val loss 0.2726
Epoch 46/100: train mIoU 0.8852, val mIoU 0.6442
Time: 0:13:12.785610
Epoch 47/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 47/100: train loss 0.0984, val loss 0.2833
Epoch 47/100: train mIoU 0.8876, val mIoU 0.6283
Time: 0:13:11.511825
Epoch 48/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 48/100: train loss 0.0945, val loss 0.2859
Epoch 48/100: train mIoU 0.8867, val mIoU 0.6308
Time: 0:13:11.541330
Epoch 49/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 49/100: train loss 0.1002, val loss 0.3024
Epoch 49/100: train mIoU 0.8854, val mIoU 0.6036
Time: 0:13:12.071603
Epoch 50/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 50/100: train loss 0.0999, val loss 0.2909
Epoch 50/100: train mIoU 0.8826, val mIoU 0.6204
Time: 0:13:10.560711
Epoch 51/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 51/100: train loss 0.0988, val loss 0.2839
Epoch 51/100: train mIoU 0.8821, val mIoU 0.6310
Time: 0:13:18.247570
Epoch 52/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 52/100: train loss 0.0935, val loss 0.2853
Epoch 52/100: train mIoU 0.8926, val mIoU 0.6283
Time: 0:13:17.432746
Epoch 53/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 53/100: train loss 0.0981, val loss 0.2829
Epoch 53/100: train mIoU 0.8912, val mIoU 0.6326
Time: 0:13:11.485145
Epoch 54/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 54/100: train loss 0.0926, val loss 0.2678
Epoch 54/100: train mIoU 0.8919, val mIoU 0.6427
Time: 0:13:13.218021
Epoch 55/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 55/100: train loss 0.0882, val loss 0.2805
Epoch 55/100: train mIoU 0.9024, val mIoU 0.6339
Time: 0:13:11.043851
Epoch 56/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 56/100: train loss 0.0877, val loss 0.2732
Epoch 56/100: train mIoU 0.9037, val mIoU 0.6338
Time: 0:13:11.223577
Epoch 57/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 57/100: train loss 0.0930, val loss 0.2793
Epoch 57/100: train mIoU 0.8931, val mIoU 0.6298
Time: 0:13:14.277266
Epoch 58/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 58/100: train loss 0.0971, val loss 0.2814
Epoch 58/100: train mIoU 0.8839, val mIoU 0.6371
Time: 0:13:11.339362
Epoch 59/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 59/100: train loss 0.0858, val loss 0.2858
Epoch 59/100: train mIoU 0.8968, val mIoU 0.6291
Time: 0:13:11.382174
Epoch 60/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 60/100: train loss 0.0853, val loss 0.2888
Epoch 60/100: train mIoU 0.8994, val mIoU 0.6160
Time: 0:13:11.053233
Epoch 61/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 61/100: train loss 0.0842, val loss 0.2938
Epoch 61/100: train mIoU 0.9014, val mIoU 0.6211
Time: 0:13:11.326263
Epoch 62/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 62/100: train loss 0.0858, val loss 0.2940
Epoch 62/100: train mIoU 0.9011, val mIoU 0.6222
Time: 0:13:11.527041
Epoch 63/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 63/100: train loss 0.0807, val loss 0.2967
Epoch 63/100: train mIoU 0.9054, val mIoU 0.6152
Time: 0:13:11.549554
Epoch 64/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 64/100: train loss 0.0813, val loss 0.2881
Epoch 64/100: train mIoU 0.9034, val mIoU 0.6247
Time: 0:13:10.679509
Epoch 65/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 65/100: train loss 0.0866, val loss 0.2889
Epoch 65/100: train mIoU 0.8979, val mIoU 0.6171
Time: 0:13:11.231067
Epoch 66/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 66/100: train loss 0.0858, val loss 0.2841
Epoch 66/100: train mIoU 0.8984, val mIoU 0.6265
Time: 0:13:11.334040
Epoch 67/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 67/100: train loss 0.0803, val loss 0.2978
Epoch 67/100: train mIoU 0.9047, val mIoU 0.6112
Time: 0:13:10.446358
Epoch 68/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 68/100: train loss 0.0758, val loss 0.2880
Epoch 68/100: train mIoU 0.9089, val mIoU 0.6201
/home/bertille/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/bertille/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Time: 0:13:10.805799
Epoch 69/100
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 69/100: train loss 0.0823, val loss 0.2871
Epoch 69/100: train mIoU 0.9082, val mIoU 0.6278
Early stopping at epoch 69
Testing
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Test IoU by class: {0: 0.4948041852248455, 1: 0.6781208901627164, 2: 0.764805078045882, 3: 0.741720031683381, 4: 0.662759252665591, 5: 0.5901412918780113}
Test F1 by class: {0: 0.6620321111161701, 1: 0.8081907497104853, 2: 0.8667303687642702, 3: 0.851709824990076, 4: 0.7971800506935842, 5: 0.7422501319754224}
Test mIoU: 0.6553917882767379
Test mF1: 0.7880155395416679
----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2, 0.2]
Stratified: zone
Batch size: 16
Train: 6308 images, Val: 2208 images, Test: 2147 images
Train: 59.16%, Val: 20.71%, Test: 20.14%
Shape of images and masks:
Image shape: torch.Size([16, 4, 256, 256])
Creating model...
Model settings:
Encoder name: efficientnet-b7
Pretrained: None
Classes: 6
Creating optimizer...
Training settings:
Learning rate: 0.0001
Criterion: Dice
Optimizer: Adam
Training...
Epoch 1/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 1/40: train loss 0.7836, val loss 0.5332
Epoch 1/40: train mIoU 0.1306, val mIoU 0.1479
Time: 0:13:54.955726
Epoch 2/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 2/40: train loss 0.7245, val loss 0.4675
Epoch 2/40: train mIoU 0.1611, val mIoU 0.2349
Time: 0:13:37.501337
Epoch 3/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 3/40: train loss 0.7070, val loss 0.4671
Epoch 3/40: train mIoU 0.1880, val mIoU 0.2405
Time: 0:14:02.779380
Epoch 4/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 4/40: train loss 0.6670, val loss 0.4536
Epoch 4/40: train mIoU 0.2179, val mIoU 0.2687
Time: 0:14:03.674877
Epoch 5/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 5/40: train loss 0.6381, val loss 0.4493
Epoch 5/40: train mIoU 0.2375, val mIoU 0.2915
Time: 0:13:30.505548
Epoch 6/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 6/40: train loss 0.6016, val loss 0.4431
Epoch 6/40: train mIoU 0.2673, val mIoU 0.2952
Time: 0:13:20.204411
Epoch 7/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 7/40: train loss 0.5872, val loss 0.4548
Epoch 7/40: train mIoU 0.2807, val mIoU 0.2920
Time: 0:13:17.676219
Epoch 8/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 8/40: train loss 0.5697, val loss 0.4448
Epoch 8/40: train mIoU 0.2920, val mIoU 0.3129
Time: 0:13:18.150641
Epoch 9/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 9/40: train loss 0.5546, val loss 0.4385
Epoch 9/40: train mIoU 0.3027, val mIoU 0.3241
Time: 0:13:19.124374
Epoch 10/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 10/40: train loss 0.5393, val loss 0.4516
Epoch 10/40: train mIoU 0.3199, val mIoU 0.3254
Time: 0:13:37.870381
Epoch 11/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 11/40: train loss 0.5307, val loss 0.4445
Epoch 11/40: train mIoU 0.3253, val mIoU 0.2989
Time: 0:14:05.164047
Epoch 12/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 12/40: train loss 0.5121, val loss 0.4439
Epoch 12/40: train mIoU 0.3395, val mIoU 0.3066
Time: 0:14:08.567044
Epoch 13/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 13/40: train loss 0.5129, val loss 0.4462
Epoch 13/40: train mIoU 0.3423, val mIoU 0.3180
Time: 0:13:33.667195
Epoch 14/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 14/40: train loss 0.4996, val loss 0.4520
Epoch 14/40: train mIoU 0.3531, val mIoU 0.3085
Time: 0:13:45.640431
Epoch 15/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 15/40: train loss 0.4990, val loss 0.4410
Epoch 15/40: train mIoU 0.3581, val mIoU 0.3354
Time: 0:14:10.462420
Epoch 16/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 16/40: train loss 0.4849, val loss 0.4504
Epoch 16/40: train mIoU 0.3701, val mIoU 0.3197
Time: 0:14:12.304608
Epoch 17/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 17/40: train loss 0.4861, val loss 0.4431
Epoch 17/40: train mIoU 0.3684, val mIoU 0.3104
Time: 0:14:08.028072
Epoch 18/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 18/40: train loss 0.4789, val loss 0.4862
Epoch 18/40: train mIoU 0.3735, val mIoU 0.2716
Time: 0:14:13.539087
Epoch 19/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 19/40: train loss 0.4699, val loss 0.4411
Epoch 19/40: train mIoU 0.3823, val mIoU 0.3421
Time: 0:14:06.985459
Epoch 20/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 20/40: train loss 0.4618, val loss 0.4707
Epoch 20/40: train mIoU 0.3946, val mIoU 0.2815
----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2, 0.2]
Stratified: random
Batch size: 16
Train: 6397 images, Val: 2133 images, Test: 2133 images
Train: 59.99%, Val: 20.00%, Test: 20.00%
Shape of images and masks:
Image shape: torch.Size([16, 4, 256, 256])
Creating model...
Model settings:
Encoder name: efficientnet-b7
Pretrained: None
Classes: 6
Traceback (most recent call last):
  File "unet.py", line 109, in <module>
    model.load_state_dict(torch.load(model_settings['path_to_last_model']))
  File "/home/bertille/.local/lib/python3.8/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/bertille/.local/lib/python3.8/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/bertille/.local/lib/python3.8/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '../../unet_256_l1/0_random_shuffling_seed1/models/unet_last.pt'
Time: 0:14:04.101728
Epoch 21/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 21/40: train loss 0.4575, val loss 0.4578
Epoch 21/40: train mIoU 0.3994, val mIoU 0.2941
Time: 0:13:52.060261
Epoch 22/40
Training
Batch: 0  over  395
Batch: 50  over  395
Batch: 100  over  395
Batch: 150  over  395
Batch: 200  over  395
Batch: 250  over  395
Batch: 300  over  395
Batch: 350  over  395
Validation
Batch: 0  over  138
Batch: 50  over  138
Batch: 100  over  138
Epoch 22/40: train loss 0.4493, val loss 0.4690
Epoch 22/40: train mIoU 0.4047, val mIoU 0.2749
