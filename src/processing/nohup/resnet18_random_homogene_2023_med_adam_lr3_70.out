INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.9 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations
----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: all
Patches: homogeneous
Batch size: 16
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  all  year
5716 homogeneous masks found
Only mediterranean zones kept
Number of masks: 5384
Train: 3429 images, Val: 1143 images, Test: 1144 images
Train: 59.99%, Val: 20.00%, Test: 20.01%
Creating model...
Model settings:
Pretrained: None
Classes: 6
The model is Resnet18
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: CrossEntropy
Optimizer: Adam
Training...
Epoch 1/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 1/70: train loss 1.3266, val loss 1.2418
Epoch 1/70: train mF1 0.3726, val mF1 0.4201
Time: 0:00:36.944504
Epoch 2/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 2/70: train loss 1.0995, val loss 1.0404
Epoch 2/70: train mF1 0.4459, val mF1 0.5061
Time: 0:00:33.873837
Epoch 3/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 3/70: train loss 1.0313, val loss 1.0149
Epoch 3/70: train mF1 0.4969, val mF1 0.4847
Time: 0:00:34.133793
Epoch 4/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 4/70: train loss 0.9662, val loss 1.1005
Epoch 4/70: train mF1 0.5182, val mF1 0.5251
Time: 0:00:35.878654
Epoch 5/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 5/70: train loss 0.9098, val loss 1.0536
Epoch 5/70: train mF1 0.5640, val mF1 0.5243
Time: 0:00:33.473148
Epoch 6/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 6/70: train loss 0.8712, val loss 1.1737
Epoch 6/70: train mF1 0.5960, val mF1 0.5316
Time: 0:00:34.962378
Epoch 7/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 7/70: train loss 0.8309, val loss 1.5631
Epoch 7/70: train mF1 0.6203, val mF1 0.4391
Time: 0:00:33.836295
Epoch 8/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 8/70: train loss 0.8006, val loss 1.0205
Epoch 8/70: train mF1 0.6222, val mF1 0.5885
Time: 0:00:33.274035
Epoch 9/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 9/70: train loss 0.7630, val loss 1.0506
Epoch 9/70: train mF1 0.6487, val mF1 0.5127
Time: 0:00:34.518960
Epoch 10/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 10/70: train loss 0.7382, val loss 0.7191
Epoch 10/70: train mF1 0.6757, val mF1 0.6884
Time: 0:00:33.511302
Epoch 11/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 11/70: train loss 0.6876, val loss 1.0493
Epoch 11/70: train mF1 0.6762, val mF1 0.5672
Time: 0:00:33.101617
Epoch 12/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 12/70: train loss 0.6731, val loss 0.7458
Epoch 12/70: train mF1 0.6988, val mF1 0.6543
Time: 0:00:35.468881
Epoch 13/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 13/70: train loss 0.6359, val loss 0.7096
Epoch 13/70: train mF1 0.7093, val mF1 0.6719
Time: 0:00:33.481925
Epoch 14/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 14/70: train loss 0.6047, val loss 0.7458
Epoch 14/70: train mF1 0.7272, val mF1 0.6839
Time: 0:00:33.217422
Epoch 15/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 15/70: train loss 0.5706, val loss 1.2133
Epoch 15/70: train mF1 0.7358, val mF1 0.6008
Time: 0:00:35.032336
Epoch 16/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 16/70: train loss 0.5211, val loss 0.9407
Epoch 16/70: train mF1 0.7608, val mF1 0.6122
Time: 0:00:33.321399
Epoch 17/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 17/70: train loss 0.4965, val loss 1.0658
Epoch 17/70: train mF1 0.7893, val mF1 0.6123
Time: 0:00:35.226116
Epoch 18/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 18/70: train loss 0.4624, val loss 0.7888
Epoch 18/70: train mF1 0.7962, val mF1 0.6924
Time: 0:00:34.420444
Epoch 19/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 19/70: train loss 0.4082, val loss 1.0807
Epoch 19/70: train mF1 0.8114, val mF1 0.6419
Time: 0:00:33.837740
Epoch 20/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 20/70: train loss 0.4023, val loss 0.8920
Epoch 20/70: train mF1 0.8223, val mF1 0.6500
Time: 0:00:35.116182
Epoch 21/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 21/70: train loss 0.3200, val loss 0.8770
Epoch 21/70: train mF1 0.8644, val mF1 0.6911
Time: 0:00:33.731106
Epoch 22/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 22/70: train loss 0.2738, val loss 0.8642
Epoch 22/70: train mF1 0.8750, val mF1 0.7031
Time: 0:00:33.665250
Epoch 23/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 23/70: train loss 0.2168, val loss 1.4313
Epoch 23/70: train mF1 0.9079, val mF1 0.6201
Time: 0:00:36.154732
Epoch 24/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 24/70: train loss 0.2100, val loss 1.2688
Epoch 24/70: train mF1 0.9008, val mF1 0.6465
Time: 0:00:33.973955
Epoch 25/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 25/70: train loss 0.1813, val loss 1.5138
Epoch 25/70: train mF1 0.9161, val mF1 0.5953
Time: 0:00:33.352504
Epoch 26/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 26/70: train loss 0.1417, val loss 1.2254
Epoch 26/70: train mF1 0.9470, val mF1 0.6672
Time: 0:00:35.526828
Epoch 27/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 27/70: train loss 0.1294, val loss 1.0449
Epoch 27/70: train mF1 0.9469, val mF1 0.7069
Time: 0:00:33.394812
Epoch 28/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 28/70: train loss 0.0932, val loss 1.2803
Epoch 28/70: train mF1 0.9639, val mF1 0.6845
Time: 0:00:34.768943
Epoch 29/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 29/70: train loss 0.0916, val loss 1.6483
Epoch 29/70: train mF1 0.9644, val mF1 0.6486
Time: 0:00:34.664732
Epoch 30/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 30/70: train loss 0.1397, val loss 1.1358
Epoch 30/70: train mF1 0.9465, val mF1 0.6853
Time: 0:00:33.860457
Epoch 31/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 31/70: train loss 0.1019, val loss 1.3421
Epoch 31/70: train mF1 0.9611, val mF1 0.6846
Time: 0:00:35.064952
Epoch 32/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 32/70: train loss 0.0697, val loss 1.5580
Epoch 32/70: train mF1 0.9705, val mF1 0.6776
Time: 0:00:34.087507
Epoch 33/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 33/70: train loss 0.0680, val loss 1.3912
Epoch 33/70: train mF1 0.9750, val mF1 0.6962
Time: 0:00:33.409850
Epoch 34/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 34/70: train loss 0.0544, val loss 1.4585
Epoch 34/70: train mF1 0.9803, val mF1 0.6643
Time: 0:00:35.829018
Epoch 35/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 35/70: train loss 0.0928, val loss 2.0891
Epoch 35/70: train mF1 0.9675, val mF1 0.6116
Time: 0:00:33.676180
Epoch 36/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 36/70: train loss 0.0636, val loss 1.4938
Epoch 36/70: train mF1 0.9792, val mF1 0.6870
Time: 0:00:33.524729
Epoch 37/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 37/70: train loss 0.0865, val loss 1.5461
Epoch 37/70: train mF1 0.9673, val mF1 0.6396
Time: 0:00:35.161533
Epoch 38/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 38/70: train loss 0.1124, val loss 1.3067
Epoch 38/70: train mF1 0.9528, val mF1 0.6950
Time: 0:00:33.359245
Epoch 39/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 39/70: train loss 0.0342, val loss 1.3692
Epoch 39/70: train mF1 0.9899, val mF1 0.6927
Time: 0:00:33.540308
Epoch 40/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 40/70: train loss 0.0772, val loss 1.2391
Epoch 40/70: train mF1 0.9783, val mF1 0.6969
Time: 0:00:36.185019
Epoch 41/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 41/70: train loss 0.0282, val loss 1.4993
Epoch 41/70: train mF1 0.9919, val mF1 0.7063
Time: 0:00:33.592162
Epoch 42/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 42/70: train loss 0.0219, val loss 1.3699
Epoch 42/70: train mF1 0.9922, val mF1 0.7148
Time: 0:00:34.662049
Epoch 43/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 43/70: train loss 0.0415, val loss 1.3037
Epoch 43/70: train mF1 0.9898, val mF1 0.7170
Time: 0:00:33.970345
Epoch 44/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 44/70: train loss 0.1014, val loss 1.5155
Epoch 44/70: train mF1 0.9633, val mF1 0.6693
Time: 0:00:33.384053
Epoch 45/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 45/70: train loss 0.0691, val loss 1.8443
Epoch 45/70: train mF1 0.9703, val mF1 0.5976
Time: 0:00:35.460478
Epoch 46/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 46/70: train loss 0.0601, val loss 1.3695
Epoch 46/70: train mF1 0.9771, val mF1 0.6828
Time: 0:00:34.104139
Epoch 47/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 47/70: train loss 0.0448, val loss 1.4944
Epoch 47/70: train mF1 0.9806, val mF1 0.6517
Time: 0:00:33.165695
Epoch 48/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 48/70: train loss 0.0438, val loss 1.3681
Epoch 48/70: train mF1 0.9881, val mF1 0.6977
Time: 0:00:35.383883
Epoch 49/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 49/70: train loss 0.0531, val loss 1.5453
Epoch 49/70: train mF1 0.9806, val mF1 0.7042
Time: 0:00:33.897291
Epoch 50/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 50/70: train loss 0.0345, val loss 1.4834
Epoch 50/70: train mF1 0.9901, val mF1 0.6815
Time: 0:00:33.573897
Epoch 51/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 51/70: train loss 0.0633, val loss 1.6191
Epoch 51/70: train mF1 0.9753, val mF1 0.6580
Time: 0:00:36.072645
Epoch 52/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 52/70: train loss 0.0694, val loss 1.2497
Epoch 52/70: train mF1 0.9746, val mF1 0.7128
Time: 0:00:33.832425
Epoch 53/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 53/70: train loss 0.0212, val loss 1.4438
Epoch 53/70: train mF1 0.9921, val mF1 0.7087
Time: 0:00:34.697114
Epoch 54/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 54/70: train loss 0.0096, val loss 1.4342
Epoch 54/70: train mF1 0.9980, val mF1 0.7135
Time: 0:00:34.003098
Epoch 55/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 55/70: train loss 0.0248, val loss 1.3765
Epoch 55/70: train mF1 0.9909, val mF1 0.7014
Time: 0:00:33.370865
Epoch 56/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 56/70: train loss 0.0393, val loss 1.4987
Epoch 56/70: train mF1 0.9864, val mF1 0.7028
Time: 0:00:35.816117
Epoch 57/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 57/70: train loss 0.0163, val loss 1.6656
Epoch 57/70: train mF1 0.9951, val mF1 0.6835
Time: 0:00:34.014592
Epoch 58/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 58/70: train loss 0.0640, val loss 1.6853
Epoch 58/70: train mF1 0.9774, val mF1 0.6527
Time: 0:00:33.569910
Epoch 59/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 59/70: train loss 0.0384, val loss 1.3646
Epoch 59/70: train mF1 0.9875, val mF1 0.6923
Time: 0:00:35.214028
Epoch 60/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 60/70: train loss 0.0227, val loss 1.5363
Epoch 60/70: train mF1 0.9920, val mF1 0.6911
Time: 0:00:33.596855
Epoch 61/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 61/70: train loss 0.0188, val loss 1.6314
Epoch 61/70: train mF1 0.9970, val mF1 0.6983
Time: 0:00:33.630741
Epoch 62/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 62/70: train loss 0.0736, val loss 1.6942
Epoch 62/70: train mF1 0.9729, val mF1 0.6482
Time: 0:00:36.548830
Epoch 63/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 63/70: train loss 0.0437, val loss 1.5505
Epoch 63/70: train mF1 0.9820, val mF1 0.6781
Time: 0:00:33.477875
Epoch 64/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 64/70: train loss 0.0229, val loss 1.4824
Epoch 64/70: train mF1 0.9926, val mF1 0.6795
Time: 0:00:34.258683
Epoch 65/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 65/70: train loss 0.0230, val loss 1.5501
Epoch 65/70: train mF1 0.9925, val mF1 0.6824
Time: 0:00:34.532925
Epoch 66/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 66/70: train loss 0.0226, val loss 1.6701
Epoch 66/70: train mF1 0.9936, val mF1 0.6806
Time: 0:00:33.215042
Epoch 67/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 67/70: train loss 0.0137, val loss 1.6053
Epoch 67/70: train mF1 0.9969, val mF1 0.6967
Time: 0:00:35.861356
Epoch 68/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 68/70: train loss 0.0446, val loss 1.7058
Epoch 68/70: train mF1 0.9855, val mF1 0.6663
Time: 0:00:34.032408
Epoch 69/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 69/70: train loss 0.0324, val loss 1.6174
Epoch 69/70: train mF1 0.9913, val mF1 0.6714
Time: 0:00:33.119819
Epoch 70/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 70/70: train loss 0.0579, val loss 3.1373
Epoch 70/70: train mF1 0.9808, val mF1 0.5484
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Time: 0:00:35.093475
Testing
Batch: 0  over  72
Batch: 50  over  72
Test F1 by class: [0.52777778 0.70783848 0.88719899 0.75728155 0.64238411 0.44736842]
Test mF1: 0.6616415540095016
Plot saved at: ../../results/resnet18_256_l1/random_shuffling/resnet18_random_homogene_lr3_mediteranean/metrics_test/test_preds.png
