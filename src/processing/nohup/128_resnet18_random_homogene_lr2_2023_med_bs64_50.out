INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.10 (you have 1.4.9). Upgrade using: pip install --upgrade albumentations
----------------------- UNet -----------------------
Patch size: 128
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: all
Patches: homogeneous
Batch size: 64
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  all  year
30093 homogeneous masks found
Image shape: torch.Size([64, 4, 128, 128]), Mask shape: torch.Size([64])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask: [2], dtype: torch.uint8
Train: 18055 images, Val: 6019 images, Test: 6019 images
Train: 60.00%, Val: 20.00%, Test: 20.00%
Creating model...
Model settings:
Pretrained: None
Classes: 6
The model is Resnet18
Creating optimizer...
Training settings:
Learning rate: 0.01
Criterion: CrossEntropy
Optimizer: AdamW
Using AdamW optimizer
Training...
Epoch 1/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 1/50: train loss 1.3781, val loss 2.1152
Epoch 1/50: train mF1 0.3535, val mF1 0.1774
Time: 0:01:30.358696
Epoch 2/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 2/50: train loss 1.1126, val loss 1.9687
Epoch 2/50: train mF1 0.4601, val mF1 0.2953
Time: 0:01:26.707589
Epoch 3/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 3/50: train loss 1.0256, val loss 1.1434
Epoch 3/50: train mF1 0.5082, val mF1 0.4484
Time: 0:01:29.508044
Epoch 4/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 4/50: train loss 0.9687, val loss 2.2498
Epoch 4/50: train mF1 0.5482, val mF1 0.2137
Time: 0:01:29.851594
Epoch 5/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 5/50: train loss 0.9093, val loss 2.3039
Epoch 5/50: train mF1 0.5868, val mF1 0.3517
Time: 0:01:29.623927
Epoch 6/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 6/50: train loss 0.8620, val loss 0.9211
Epoch 6/50: train mF1 0.6170, val mF1 0.5918
Time: 0:01:29.765809
Epoch 7/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 7/50: train loss 0.8180, val loss 0.9951
Epoch 7/50: train mF1 0.6390, val mF1 0.5694
Time: 0:01:29.288129
Epoch 8/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 8/50: train loss 0.7738, val loss 1.3146
Epoch 8/50: train mF1 0.6563, val mF1 0.4786
Time: 0:01:31.782719
Epoch 9/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 9/50: train loss 0.7514, val loss 1.0753
Epoch 9/50: train mF1 0.6750, val mF1 0.5538
Time: 0:01:30.642333
Epoch 10/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 10/50: train loss 0.7064, val loss 0.8479
Epoch 10/50: train mF1 0.6936, val mF1 0.6395
Time: 0:01:31.696881
Epoch 11/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 11/50: train loss 0.6729, val loss 2.1190
Epoch 11/50: train mF1 0.7096, val mF1 0.3961
Time: 0:01:29.070646
Epoch 12/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 12/50: train loss 0.6414, val loss 1.3014
Epoch 12/50: train mF1 0.7270, val mF1 0.5490
Time: 0:01:34.079231
Epoch 13/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 13/50: train loss 0.6085, val loss 0.9943
Epoch 13/50: train mF1 0.7451, val mF1 0.6091
Time: 0:01:30.228925
Epoch 14/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 14/50: train loss 0.5849, val loss 2.1260
Epoch 14/50: train mF1 0.7496, val mF1 0.3980
Time: 0:01:31.758155
Epoch 15/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 15/50: train loss 0.5367, val loss 0.8385
Epoch 15/50: train mF1 0.7711, val mF1 0.6581
Time: 0:01:31.820635
Epoch 16/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 16/50: train loss 0.4913, val loss 1.0543
Epoch 16/50: train mF1 0.7866, val mF1 0.6058
Time: 0:01:32.128115
Epoch 17/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 17/50: train loss 0.4336, val loss 1.3291
Epoch 17/50: train mF1 0.8093, val mF1 0.5407
Time: 0:01:32.990989
Epoch 18/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 18/50: train loss 0.3986, val loss 0.7949
Epoch 18/50: train mF1 0.8265, val mF1 0.6953
Time: 0:01:30.809253
Epoch 19/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 19/50: train loss 0.3104, val loss 1.2343
Epoch 19/50: train mF1 0.8655, val mF1 0.5959
Time: 0:01:33.558236
Epoch 20/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 20/50: train loss 0.2672, val loss 2.1984
Epoch 20/50: train mF1 0.8884, val mF1 0.4572
Time: 0:01:30.891177
Epoch 21/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 21/50: train loss 0.2420, val loss 1.2506
Epoch 21/50: train mF1 0.8986, val mF1 0.6265
Time: 0:01:32.544586
Epoch 22/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 22/50: train loss 0.2137, val loss 1.2975
Epoch 22/50: train mF1 0.9119, val mF1 0.6308
Time: 0:01:30.197619
Epoch 23/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 23/50: train loss 0.1376, val loss 1.3639
Epoch 23/50: train mF1 0.9407, val mF1 0.6638
Time: 0:01:32.561622
Epoch 24/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 24/50: train loss 0.1549, val loss 1.4262
Epoch 24/50: train mF1 0.9394, val mF1 0.6074
Time: 0:01:32.278286
Epoch 25/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 25/50: train loss 0.1483, val loss 2.2255
Epoch 25/50: train mF1 0.9373, val mF1 0.5340
Time: 0:01:31.733714
Epoch 26/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 26/50: train loss 0.1207, val loss 1.5047
Epoch 26/50: train mF1 0.9509, val mF1 0.6050
Time: 0:01:33.923437
Epoch 27/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 27/50: train loss 0.1228, val loss 1.8103
Epoch 27/50: train mF1 0.9519, val mF1 0.5627
Time: 0:01:30.892454
Epoch 28/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 28/50: train loss 0.1136, val loss 1.7157
Epoch 28/50: train mF1 0.9559, val mF1 0.6250
Time: 0:01:32.977361
Epoch 29/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 29/50: train loss 0.1052, val loss 1.3366
Epoch 29/50: train mF1 0.9578, val mF1 0.6650
Time: 0:01:30.227448
Epoch 30/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 30/50: train loss 0.1295, val loss 1.2783
Epoch 30/50: train mF1 0.9476, val mF1 0.6890
Time: 0:01:33.434667
Epoch 31/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 31/50: train loss 0.0861, val loss 1.7879
Epoch 31/50: train mF1 0.9665, val mF1 0.6488
Time: 0:01:30.735518
Epoch 32/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 32/50: train loss 0.0815, val loss 2.2621
Epoch 32/50: train mF1 0.9677, val mF1 0.5368
Time: 0:01:33.816335
Epoch 33/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 33/50: train loss 0.1194, val loss 1.4395
Epoch 33/50: train mF1 0.9562, val mF1 0.6646
Time: 0:01:32.872414
Epoch 34/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 34/50: train loss 0.0773, val loss 1.8006
Epoch 34/50: train mF1 0.9728, val mF1 0.5536
Time: 0:01:31.514052
Epoch 35/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 35/50: train loss 0.1054, val loss 1.4538
Epoch 35/50: train mF1 0.9568, val mF1 0.6668
Time: 0:01:32.290596
Epoch 36/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 36/50: train loss 0.0761, val loss 1.4132
Epoch 36/50: train mF1 0.9705, val mF1 0.6775
Time: 0:01:30.793781
Epoch 37/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 37/50: train loss 0.0881, val loss 2.1534
Epoch 37/50: train mF1 0.9655, val mF1 0.5645
Time: 0:01:32.568534
Epoch 38/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 38/50: train loss 0.0702, val loss 2.0658
Epoch 38/50: train mF1 0.9718, val mF1 0.5430
Time: 0:01:30.905968
Epoch 39/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 39/50: train loss 0.0978, val loss 1.7579
Epoch 39/50: train mF1 0.9606, val mF1 0.6051
Time: 0:01:32.864556
Epoch 40/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 40/50: train loss 0.0915, val loss 2.4366
Epoch 40/50: train mF1 0.9661, val mF1 0.5163
Time: 0:01:31.987426
Epoch 41/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 41/50: train loss 0.0945, val loss 1.9653
Epoch 41/50: train mF1 0.9618, val mF1 0.5987
Time: 0:01:33.305185
Epoch 42/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 42/50: train loss 0.1155, val loss 1.7459
Epoch 42/50: train mF1 0.9551, val mF1 0.6497
Time: 0:01:32.813452
Epoch 43/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 43/50: train loss 0.0736, val loss 2.6597
Epoch 43/50: train mF1 0.9729, val mF1 0.5376
Time: 0:01:32.711433
Epoch 44/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 44/50: train loss 0.0854, val loss 1.4002
Epoch 44/50: train mF1 0.9663, val mF1 0.6898
Time: 0:01:34.823759
Epoch 45/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 45/50: train loss 0.0549, val loss 1.5881
Epoch 45/50: train mF1 0.9783, val mF1 0.6298
Time: 0:01:32.250932
Epoch 46/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 46/50: train loss 0.0860, val loss 1.6550
Epoch 46/50: train mF1 0.9655, val mF1 0.6741
Time: 0:01:33.366884
Epoch 47/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 47/50: train loss 0.0640, val loss 1.7021
Epoch 47/50: train mF1 0.9758, val mF1 0.6108
Time: 0:01:29.787930
Epoch 48/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 48/50: train loss 0.0930, val loss 1.4868
Epoch 48/50: train mF1 0.9653, val mF1 0.6421
Time: 0:01:34.606407
Epoch 49/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 49/50: train loss 0.1020, val loss 1.3995
Epoch 49/50: train mF1 0.9625, val mF1 0.6812
Time: 0:01:31.605546
Epoch 50/50
Training
Batch: 0  over  283
Batch: 50  over  283
Batch: 100  over  283
Batch: 150  over  283
Batch: 200  over  283
Batch: 250  over  283
Validation
Batch: 0  over  95
Batch: 50  over  95
Epoch 50/50: train loss 0.0619, val loss 2.2662
Epoch 50/50: train mF1 0.9761, val mF1 0.5444
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Time: 0:01:31.869307
Testing
Batch: 0  over  95
Batch: 50  over  95
Test F1 by class: [0.58263305 0.70833333 0.85464726 0.79025323 0.70108696 0.58086957]
Test mF1: 0.7029705649614258
Plot saved at: ../../results/resnet18_128_l1/random_shuffling/128_resnet18_random_homogene_lr2_bs64_50/metrics_test/test_preds.png
