----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.7, 0.2]
Stratified: zone
Year: all
Patches: homogeneous
Batch size: 16
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  all  year
5716 homogeneous masks found
['zone5' 'zone171' 'zone12' 'zone37' 'zone156' 'zone22' 'zone129' 'zone50'
 'zone69' 'zone143' 'zone95' 'zone14' 'zone34' 'zone112' 'zone53'
 'zone154' 'zone54' 'zone104' 'zone120' 'zone7' 'zone10' 'zone30'
 'zone123' 'zone98' 'zone67' 'zone21' 'zone33' 'zone158' 'zone148'
 'zone84' 'zone15' 'zone48' 'zone16' 'zone1' 'zone44' 'zone136' 'zone26'
 'zone24' 'zone71' 'zone56' 'zone38' 'zone25' 'zone155' 'zone121'
 'zone162' 'zone132' 'zone78' 'zone2' 'zone101' 'zone19' 'zone75' 'zone68'
 'zone39' 'zone160' 'zone96' 'zone115' 'zone17' 'zone139' 'zone114'
 'zone65' 'zone59' 'zone113' 'zone45' 'zone126' 'zone47' 'zone165'
 'zone170' 'zone66' 'zone161' 'zone142' 'zone144' 'zone63' 'zone76'
 'zone74' 'zone41' 'zone90' 'zone20' 'zone51' 'zone73' 'zone106' 'zone133'
 'zone3' 'zone77' 'zone172' 'zone88' 'zone147' 'zone159' 'zone164'
 'zone100' 'zone134' 'zone137' 'zone28' 'zone6' 'zone80' 'zone85' 'zone97'
 'zone167' 'zone27' 'zone116' 'zone102' 'zone145']
Nbumber of unique zones: 101
Train, val and test zones saved in csv file at: ../../results/resnet18_256_l1/stratified_shuffling_by_zone/seed1/img_ids_by_set.csv
Image shape: torch.Size([16, 4, 256, 256]), Mask shape: torch.Size([16])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask: [2], dtype: torch.uint8
Train: 3288 images, Val: 1123 images, Test: 1305 images
Train: 57.52%, Val: 19.65%, Test: 22.83%
Creating model...
Model settings:
Pretrained: True
Classes: 6
The model is Resnet18
Pretrained weights loaded
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: CrossEntropy
Optimizer: Adam
Training...
Epoch 1/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 1/70: train loss 1.1227, val loss 2.2567
Epoch 1/70: train mF1 0.5124, val mF1 0.2344
Time: 0:00:33.171321
Epoch 2/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 2/70: train loss 0.8705, val loss 2.2934
Epoch 2/70: train mF1 0.6121, val mF1 0.3655
Time: 0:00:32.126874
Epoch 3/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 3/70: train loss 0.7625, val loss 1.6174
Epoch 3/70: train mF1 0.6581, val mF1 0.3610
Time: 0:00:32.150918
Epoch 4/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 4/70: train loss 0.7083, val loss 1.1985
Epoch 4/70: train mF1 0.6927, val mF1 0.4094
Time: 0:00:32.448762
Epoch 5/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 5/70: train loss 0.6402, val loss 1.6215
Epoch 5/70: train mF1 0.7142, val mF1 0.3524
Time: 0:00:32.330060
Epoch 6/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 6/70: train loss 0.5728, val loss 2.7495
Epoch 6/70: train mF1 0.7298, val mF1 0.3132
Time: 0:00:31.990767
Epoch 7/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 7/70: train loss 0.5579, val loss 2.4279
Epoch 7/70: train mF1 0.7581, val mF1 0.3206
Time: 0:00:32.052816
Epoch 8/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 8/70: train loss 0.5085, val loss 1.9075
Epoch 8/70: train mF1 0.7817, val mF1 0.3502
Time: 0:00:32.097751
Epoch 9/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 9/70: train loss 0.4261, val loss 1.9424
Epoch 9/70: train mF1 0.8106, val mF1 0.3550
Time: 0:00:31.730216
Epoch 10/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 10/70: train loss 0.4259, val loss 1.6452
Epoch 10/70: train mF1 0.8109, val mF1 0.3412
Time: 0:00:31.816129
Epoch 11/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 11/70: train loss 0.3179, val loss 2.2244
Epoch 11/70: train mF1 0.8614, val mF1 0.3763
Time: 0:00:31.708829
Epoch 12/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 12/70: train loss 0.2819, val loss 2.4322
Epoch 12/70: train mF1 0.8696, val mF1 0.3413
Time: 0:00:31.582702
Epoch 13/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 13/70: train loss 0.2739, val loss 3.1842
Epoch 13/70: train mF1 0.8857, val mF1 0.3408
Time: 0:00:31.740793
Epoch 14/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 14/70: train loss 0.2585, val loss 2.2760
Epoch 14/70: train mF1 0.8883, val mF1 0.3941
Time: 0:00:31.937042
Epoch 15/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 15/70: train loss 0.1970, val loss 2.4451
Epoch 15/70: train mF1 0.9122, val mF1 0.4371
Time: 0:00:32.137093
Epoch 16/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 16/70: train loss 0.1641, val loss 2.1802
Epoch 16/70: train mF1 0.9295, val mF1 0.3712
Time: 0:00:32.062246
Epoch 17/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 17/70: train loss 0.1620, val loss 2.4743
Epoch 17/70: train mF1 0.9331, val mF1 0.3999
Time: 0:00:32.007685
Epoch 18/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 18/70: train loss 0.1804, val loss 2.9286
Epoch 18/70: train mF1 0.9287, val mF1 0.3679
Time: 0:00:31.795885
Epoch 19/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 19/70: train loss 0.1148, val loss 2.4879
Epoch 19/70: train mF1 0.9470, val mF1 0.4115
Time: 0:00:31.829238
Epoch 20/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 20/70: train loss 0.1133, val loss 2.4527
Epoch 20/70: train mF1 0.9566, val mF1 0.3817
Time: 0:00:31.841175
Epoch 21/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 21/70: train loss 0.0803, val loss 3.1418
Epoch 21/70: train mF1 0.9655, val mF1 0.4260
Time: 0:00:32.007660
Epoch 22/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 22/70: train loss 0.0495, val loss 3.7770
Epoch 22/70: train mF1 0.9824, val mF1 0.4127
Time: 0:00:31.802901
Epoch 23/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 23/70: train loss 0.1212, val loss 3.1004
Epoch 23/70: train mF1 0.9570, val mF1 0.3406
Time: 0:00:31.465884
Epoch 24/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 24/70: train loss 0.1095, val loss 2.8510
Epoch 24/70: train mF1 0.9549, val mF1 0.3970
Time: 0:00:31.898668
Epoch 25/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 25/70: train loss 0.0703, val loss 3.4682
Epoch 25/70: train mF1 0.9717, val mF1 0.4201
Time: 0:00:32.275200
Epoch 26/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 26/70: train loss 0.0820, val loss 3.3457
Epoch 26/70: train mF1 0.9636, val mF1 0.3773
Time: 0:00:31.950564
Epoch 27/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 27/70: train loss 0.0616, val loss 3.2971
Epoch 27/70: train mF1 0.9768, val mF1 0.4039
Time: 0:00:32.043243
Epoch 28/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 28/70: train loss 0.0590, val loss 4.1914
Epoch 28/70: train mF1 0.9761, val mF1 0.3945
Time: 0:00:31.888359
Epoch 29/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 29/70: train loss 0.0808, val loss 3.6403
Epoch 29/70: train mF1 0.9715, val mF1 0.3941
Time: 0:00:31.738488
Epoch 30/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 30/70: train loss 0.0446, val loss 4.2482
Epoch 30/70: train mF1 0.9804, val mF1 0.3830
Time: 0:00:31.769860
Epoch 31/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 31/70: train loss 0.0454, val loss 4.0798
Epoch 31/70: train mF1 0.9829, val mF1 0.4073
Time: 0:00:31.901427
Epoch 32/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 32/70: train loss 0.0471, val loss 5.3911
Epoch 32/70: train mF1 0.9800, val mF1 0.3601
Time: 0:00:31.782878
Epoch 33/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 33/70: train loss 0.0812, val loss 3.0885
Epoch 33/70: train mF1 0.9702, val mF1 0.4053
Time: 0:00:31.393120
Epoch 34/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 34/70: train loss 0.0430, val loss 4.2659
Epoch 34/70: train mF1 0.9814, val mF1 0.4026
Time: 0:00:32.640123
Epoch 35/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 35/70: train loss 0.0653, val loss 3.4799
Epoch 35/70: train mF1 0.9809, val mF1 0.3760
Time: 0:00:32.213852
Epoch 36/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 36/70: train loss 0.0966, val loss 2.9901
Epoch 36/70: train mF1 0.9609, val mF1 0.4012
Time: 0:00:32.075542
Epoch 37/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 37/70: train loss 0.0355, val loss 3.1566
Epoch 37/70: train mF1 0.9858, val mF1 0.3939
Time: 0:00:32.117906
Epoch 38/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 38/70: train loss 0.0183, val loss 3.5372
Epoch 38/70: train mF1 0.9927, val mF1 0.3945
Time: 0:00:32.925549
Epoch 39/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 39/70: train loss 0.0580, val loss 4.2364
Epoch 39/70: train mF1 0.9774, val mF1 0.3587
Time: 0:00:32.460902
Epoch 40/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 40/70: train loss 0.0906, val loss 4.8399
Epoch 40/70: train mF1 0.9590, val mF1 0.3656
Time: 0:00:32.620186
Epoch 41/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 41/70: train loss 0.0628, val loss 3.7167
Epoch 41/70: train mF1 0.9790, val mF1 0.4255
Time: 0:00:32.428617
Epoch 42/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 42/70: train loss 0.0514, val loss 3.2130
Epoch 42/70: train mF1 0.9782, val mF1 0.4099
Time: 0:00:32.225522
Epoch 43/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 43/70: train loss 0.0187, val loss 3.8153
Epoch 43/70: train mF1 0.9938, val mF1 0.4110
Time: 0:00:32.795872
Epoch 44/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 44/70: train loss 0.0352, val loss 3.7618
Epoch 44/70: train mF1 0.9883, val mF1 0.3862
Time: 0:00:32.077535
Epoch 45/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 45/70: train loss 0.0777, val loss 3.6480
Epoch 45/70: train mF1 0.9705, val mF1 0.3735
Time: 0:00:32.590163
Epoch 46/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 46/70: train loss 0.0199, val loss 3.6580
Epoch 46/70: train mF1 0.9940, val mF1 0.3524
Time: 0:00:32.376811
Epoch 47/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 47/70: train loss 0.0233, val loss 3.6182
Epoch 47/70: train mF1 0.9929, val mF1 0.3990
Time: 0:00:31.809650
Epoch 48/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 48/70: train loss 0.0321, val loss 4.1317
Epoch 48/70: train mF1 0.9904, val mF1 0.4159
Time: 0:00:31.543189
Epoch 49/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 49/70: train loss 0.0458, val loss 3.7680
Epoch 49/70: train mF1 0.9828, val mF1 0.3746
Time: 0:00:31.757446
Epoch 50/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 50/70: train loss 0.0486, val loss 4.1190
Epoch 50/70: train mF1 0.9849, val mF1 0.3687
Time: 0:00:31.617049
Epoch 51/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 51/70: train loss 0.0355, val loss 4.5642
Epoch 51/70: train mF1 0.9873, val mF1 0.3280
Time: 0:00:32.124826
Epoch 52/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 52/70: train loss 0.0345, val loss 4.0434
Epoch 52/70: train mF1 0.9848, val mF1 0.3872
Time: 0:00:31.569889
Epoch 53/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 53/70: train loss 0.0268, val loss 3.8720
Epoch 53/70: train mF1 0.9917, val mF1 0.3746
Time: 0:00:31.931412
Epoch 54/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 54/70: train loss 0.0542, val loss 4.1344
Epoch 54/70: train mF1 0.9845, val mF1 0.3836
Time: 0:00:32.644614
Epoch 55/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 55/70: train loss 0.0685, val loss 4.3592
Epoch 55/70: train mF1 0.9687, val mF1 0.3856
Time: 0:00:31.731220
Epoch 56/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 56/70: train loss 0.0481, val loss 4.1251
Epoch 56/70: train mF1 0.9837, val mF1 0.3579
Time: 0:00:31.566536
Epoch 57/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 57/70: train loss 0.0450, val loss 4.7628
Epoch 57/70: train mF1 0.9866, val mF1 0.3583
Time: 0:00:32.105669
Epoch 58/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 58/70: train loss 0.0411, val loss 4.2630
Epoch 58/70: train mF1 0.9841, val mF1 0.3838
Time: 0:00:32.433863
Epoch 59/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 59/70: train loss 0.0382, val loss 4.1590
Epoch 59/70: train mF1 0.9879, val mF1 0.3938
Time: 0:00:31.368275
Epoch 60/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 60/70: train loss 0.0323, val loss 4.1089
Epoch 60/70: train mF1 0.9912, val mF1 0.3930
Time: 0:00:32.109679
Epoch 61/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 61/70: train loss 0.0307, val loss 3.8694
Epoch 61/70: train mF1 0.9896, val mF1 0.4077
Time: 0:00:31.297362
Epoch 62/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 62/70: train loss 0.0364, val loss 3.7563
Epoch 62/70: train mF1 0.9856, val mF1 0.4165
Time: 0:00:31.275101
Epoch 63/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 63/70: train loss 0.0213, val loss 3.6802
Epoch 63/70: train mF1 0.9970, val mF1 0.4181
Time: 0:00:31.684477
Epoch 64/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 64/70: train loss 0.0170, val loss 4.2858
Epoch 64/70: train mF1 0.9943, val mF1 0.3831
Time: 0:00:31.968251
Epoch 65/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 65/70: train loss 0.0415, val loss 4.8566
Epoch 65/70: train mF1 0.9817, val mF1 0.3932
Time: 0:00:31.398827
Epoch 66/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 66/70: train loss 0.0202, val loss 3.6814
Epoch 66/70: train mF1 0.9924, val mF1 0.4238
Time: 0:00:32.006514
Epoch 67/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 67/70: train loss 0.0249, val loss 4.4790
Epoch 67/70: train mF1 0.9895, val mF1 0.3703
Time: 0:00:31.551337
Epoch 68/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 68/70: train loss 0.0110, val loss 4.4148
Epoch 68/70: train mF1 0.9947, val mF1 0.3839
Time: 0:00:31.690138
Epoch 69/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 69/70: train loss 0.0311, val loss 4.3608
Epoch 69/70: train mF1 0.9916, val mF1 0.4085
Time: 0:00:31.979489
Epoch 70/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 70/70: train loss 0.0387, val loss 6.0971
Epoch 70/70: train mF1 0.9874, val mF1 0.4057
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Time: 0:00:31.390794
Testing
Batch: 0  over  82
Batch: 50  over  82
Test F1 by class: [0.1023622  0.36814159 0.63330457 0.703125   0.18421053 0.        ]
Test mF1: 0.3318573161447053
Plot saved at: ../../results/resnet18_256_l1/stratified_shuffling_by_zone/resnet18_strat_zone_homogene_lr3_pre_trained/metrics_test/test_preds.png
