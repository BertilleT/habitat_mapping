----------------------- UNet -----------------------
Patch size: 128
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.68, 0.2]
Stratified: zone
Year: all
Patches: all
Batch size: 64
Normalisation: channel_by_channel
Data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Nbumber of unique zones: 108
Train, val and test zones saved in csv file at: ../../results/resnet18_128_l1/stratified_shuffling_by_zone/seed1/img_ids_by_set.csv
Image shape: torch.Size([64, 4, 128, 128]), Mask shape: torch.Size([64, 7])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask unique values: [0. 1.], dtype: torch.float32
Mask:  tensor([0., 0., 1., 1., 0., 0., 1.])
Train: 25531 images, Val: 9017 images, Test: 8080 images
Train: 59.89%, Val: 21.15%, Test: 18.95%
Creating model...
Model settings:
Pretrained: False
Classes: 7
The model is Resnet18
Using BCEWithDigits criterion
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: BCEWithDigits
Optimizer: Adam
Training...
Epoch 1/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 1/40: train loss 0.4627, val loss 0.4979
Epoch 1/40: train mF1 0.3796, val mF1 0.2597
Time: 0:03:28.801961
Epoch 2/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 2/40: train loss 0.4302, val loss 0.5051
Epoch 2/40: train mF1 0.4458, val mF1 0.3524
Time: 0:03:28.271986
Epoch 3/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 3/40: train loss 0.4159, val loss 0.6131
Epoch 3/40: train mF1 0.4714, val mF1 0.2542
Time: 0:03:28.423132
Epoch 4/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 4/40: train loss 0.4038, val loss 0.5029
Epoch 4/40: train mF1 0.5031, val mF1 0.3203
Time: 0:03:27.045829
Epoch 5/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 5/40: train loss 0.3948, val loss 0.4941
Epoch 5/40: train mF1 0.5222, val mF1 0.3932
Time: 0:03:30.875747
Epoch 6/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 6/40: train loss 0.3873, val loss 0.4780
Epoch 6/40: train mF1 0.5361, val mF1 0.3057
Time: 0:03:24.945609
Epoch 7/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 7/40: train loss 0.3806, val loss 0.5031
Epoch 7/40: train mF1 0.5473, val mF1 0.3692
Time: 0:03:22.337457
Epoch 8/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 8/40: train loss 0.3747, val loss 0.4744
Epoch 8/40: train mF1 0.5591, val mF1 0.3064
Time: 0:03:15.784754
Epoch 9/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 9/40: train loss 0.3704, val loss 0.4787
Epoch 9/40: train mF1 0.5694, val mF1 0.3095
Time: 0:03:16.726057
Epoch 10/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 10/40: train loss 0.3639, val loss 0.5130
Epoch 10/40: train mF1 0.5779, val mF1 0.3680
Time: 0:03:16.845837
Epoch 11/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 11/40: train loss 0.3588, val loss 0.4644
Epoch 11/40: train mF1 0.5858, val mF1 0.3721
Time: 0:03:16.546767
Epoch 12/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 12/40: train loss 0.3543, val loss 0.5587
Epoch 12/40: train mF1 0.5926, val mF1 0.3188
Time: 0:03:14.901478
Epoch 13/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 13/40: train loss 0.3519, val loss 0.5776
Epoch 13/40: train mF1 0.5945, val mF1 0.3505
Time: 0:03:14.477791
Epoch 14/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 14/40: train loss 0.3456, val loss 0.6202
Epoch 14/40: train mF1 0.6058, val mF1 0.3477
Time: 0:03:15.751119
Epoch 15/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 15/40: train loss 0.3436, val loss 0.5523
Epoch 15/40: train mF1 0.6068, val mF1 0.3612
Time: 0:03:16.256330
Epoch 16/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 16/40: train loss 0.3379, val loss 0.5253
Epoch 16/40: train mF1 0.6189, val mF1 0.3536
Time: 0:03:15.437045
Epoch 17/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 17/40: train loss 0.3354, val loss 0.4830
Epoch 17/40: train mF1 0.6195, val mF1 0.3876
Time: 0:03:16.181012
Epoch 18/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 18/40: train loss 0.3315, val loss 0.4897
Epoch 18/40: train mF1 0.6286, val mF1 0.3637
Time: 0:03:16.156823
Epoch 19/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 19/40: train loss 0.3278, val loss 0.5040
Epoch 19/40: train mF1 0.6332, val mF1 0.3775
Time: 0:03:16.571277
Epoch 20/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 20/40: train loss 0.3248, val loss 0.6013
Epoch 20/40: train mF1 0.6390, val mF1 0.3732
Time: 0:03:14.416289
Epoch 21/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 21/40: train loss 0.3223, val loss 0.4634
Epoch 21/40: train mF1 0.6422, val mF1 0.4065
Time: 0:03:14.834721
Epoch 22/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 22/40: train loss 0.3181, val loss 0.6077
Epoch 22/40: train mF1 0.6485, val mF1 0.3734
Time: 0:03:19.982690
Epoch 23/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 23/40: train loss 0.3152, val loss 0.4724
Epoch 23/40: train mF1 0.6506, val mF1 0.3688
Time: 0:03:17.840234
Epoch 24/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 24/40: train loss 0.3128, val loss 0.5082
Epoch 24/40: train mF1 0.6562, val mF1 0.3650
Time: 0:03:16.403721
Epoch 25/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 25/40: train loss 0.3095, val loss 0.5367
Epoch 25/40: train mF1 0.6616, val mF1 0.3715
Time: 0:03:16.460954
Epoch 26/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 26/40: train loss 0.3060, val loss 0.5127
Epoch 26/40: train mF1 0.6686, val mF1 0.3910
Time: 0:03:18.028315
Epoch 27/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 27/40: train loss 0.3024, val loss 0.5113
Epoch 27/40: train mF1 0.6706, val mF1 0.3849
Time: 0:03:17.641573
Epoch 28/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 28/40: train loss 0.3005, val loss 0.5491
Epoch 28/40: train mF1 0.6749, val mF1 0.3808
Time: 0:03:18.349139
Epoch 29/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 29/40: train loss 0.2972, val loss 0.5079
Epoch 29/40: train mF1 0.6802, val mF1 0.3901
Time: 0:03:19.483711
Epoch 30/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 30/40: train loss 0.2942, val loss 0.5614
Epoch 30/40: train mF1 0.6827, val mF1 0.3962
Time: 0:03:16.925237
Epoch 31/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 31/40: train loss 0.2928, val loss 0.5452
Epoch 31/40: train mF1 0.6832, val mF1 0.3847
Time: 0:03:17.278497
Epoch 32/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 32/40: train loss 0.2890, val loss 0.5301
Epoch 32/40: train mF1 0.6906, val mF1 0.3968
Time: 0:03:17.300480
Epoch 33/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 33/40: train loss 0.2863, val loss 0.6265
Epoch 33/40: train mF1 0.6952, val mF1 0.4066
Time: 0:03:17.777120
Epoch 34/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 34/40: train loss 0.2847, val loss 0.5118
Epoch 34/40: train mF1 0.6958, val mF1 0.4006
Time: 0:03:16.151200
Epoch 35/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 35/40: train loss 0.2818, val loss 0.5119
Epoch 35/40: train mF1 0.7011, val mF1 0.3712
Time: 0:03:17.571498
Epoch 36/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 36/40: train loss 0.2793, val loss 0.5287
Epoch 36/40: train mF1 0.7019, val mF1 0.3800
Time: 0:03:18.233775
Epoch 37/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 37/40: train loss 0.2764, val loss 0.6244
Epoch 37/40: train mF1 0.7054, val mF1 0.3871
Time: 0:03:16.518825
Epoch 38/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 38/40: train loss 0.2734, val loss 0.5279
Epoch 38/40: train mF1 0.7144, val mF1 0.4242
Time: 0:03:17.034548
Epoch 39/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 39/40: train loss 0.2719, val loss 0.5997
Epoch 39/40: train mF1 0.7165, val mF1 0.3510
Time: 0:03:17.217453
Epoch 40/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 40/40: train loss 0.2677, val loss 0.5498
Epoch 40/40: train mF1 0.7204, val mF1 0.4156
/home/bertille/.local/lib/python3.8/site-packages/pydantic/main.py:347: UserWarning: Pydantic serializer warnings:
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Time: 0:03:17.660566
Testing
Batch: 0  over  127
Batch: 50  over  127
Batch: 100  over  127
Test F1 by class: [0.4946142  0.28206153 0.73327386 0.32315978 0.66666667 0.16
 0.56513125]
Test mF1: 0.4607010420052769
Plot saved at: ../../results/resnet18_128_l1/stratified_shuffling_by_zone/all/resnet18_multi_label_128_zone_40epochs_bs64_augmented/metrics_test/test_preds.png
Reassembling the patches...
Reassembling the patches...
Reassembling the patches...
