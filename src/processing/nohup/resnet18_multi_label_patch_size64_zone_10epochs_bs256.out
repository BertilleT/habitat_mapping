----------------------- UNet -----------------------
Patch size: 64
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.68, 0.2]
Stratified: zone
Year: all
Patches: all
Batch size: 256
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Nbumber of unique zones: 108
Train, val and test zones saved in csv file at: ../../results/resnet18_64_l1/stratified_shuffling_by_zone/seed1/img_ids_by_set.csv
Image: min: 0.0, max: 1.0
Mask unique values: [0. 1.]
Train: 102118 images, Val: 36030 images, Test: 32320 images
Train: 59.90%, Val: 21.14%, Test: 18.96%
Creating model...
Model settings:
Pretrained: False
Classes: 7
The model is Resnet18
Using BCEWithDigits criterion
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: BCEWithDigits
Optimizer: Adam
Training...
Epoch 1/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 1/10: train loss 0.3576, val loss 0.5702
Epoch 1/10: train mF1 0.4560, val mF1 0.1683
Time: 0:07:08.830830
Epoch 2/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 2/10: train loss 0.3139, val loss 0.7613
Epoch 2/10: train mF1 0.5492, val mF1 0.2092
Time: 0:07:13.432214
Epoch 3/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 3/10: train loss 0.2937, val loss 0.4645
Epoch 3/10: train mF1 0.5830, val mF1 0.2329
Time: 0:07:20.698728
Epoch 4/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 4/10: train loss 0.2781, val loss 0.6094
Epoch 4/10: train mF1 0.6100, val mF1 0.2452
Time: 0:07:18.264053
Epoch 5/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 5/10: train loss 0.2663, val loss 0.5135
Epoch 5/10: train mF1 0.6315, val mF1 0.2319
Time: 0:07:12.261038
Epoch 6/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 6/10: train loss 0.2522, val loss 0.4253
Epoch 6/10: train mF1 0.6572, val mF1 0.3434
Time: 0:07:13.655891
Epoch 7/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 7/10: train loss 0.2397, val loss 0.4940
Epoch 7/10: train mF1 0.6765, val mF1 0.2893
Time: 0:07:16.344539
Epoch 8/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 8/10: train loss 0.2251, val loss 0.5484
Epoch 8/10: train mF1 0.6997, val mF1 0.3085
Time: 0:07:13.556569
Epoch 9/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 9/10: train loss 0.2084, val loss 0.6314
Epoch 9/10: train mF1 0.7273, val mF1 0.3272
Time: 0:07:22.570694
Epoch 10/10
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 10/10: train loss 0.1888, val loss 0.5761
Epoch 10/10: train mF1 0.7563, val mF1 0.3195
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Time: 0:07:08.667437
Testing
Batch: 0  over  127
Batch: 50  over  127
Batch: 100  over  127
Test F1 by class: [0.30118195 0.12682363 0.53020974 0.2192345  0.45495351 0.09196912
 0.1517421 ]
Test mF1: 0.26801636415081237
Plot saved at: ../../results/resnet18_64_l1/stratified_shuffling_by_zone/all/resnet18_multi_label_64_zone_10epochs_bs256/metrics_test/test_preds.png
Reassembling the patches...
Reassembling the patches...
Reassembling the patches...
