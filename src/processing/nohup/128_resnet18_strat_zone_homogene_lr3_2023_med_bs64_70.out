----------------------- UNet -----------------------
Patch size: 128
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.71, 0.14]
Stratified: zone
Year: 2023
Patches: homogeneous
Batch size: 64
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  2023  year
30093 homogeneous masks found
18151  kept masks from 2023 zones
The data are from 2023
Only mediterranean zones kept
Number of masks: 16302
['zone137' 'zone113' 'zone104' 'zone148' 'zone134' 'zone120' 'zone154'
 'zone33' 'zone73' 'zone139' 'zone162' 'zone161' 'zone39' 'zone129'
 'zone160' 'zone112' 'zone38' 'zone45' 'zone44' 'zone101' 'zone6' 'zone26'
 'zone30' 'zone71' 'zone20' 'zone77' 'zone114' 'zone50' 'zone4' 'zone84'
 'zone41' 'zone144' 'zone126' 'zone59' 'zone145' 'zone159' 'zone57'
 'zone96' 'zone123' 'zone17' 'zone115' 'zone93' 'zone19' 'zone53'
 'zone157' 'zone116' 'zone34' 'zone37' 'zone155' 'zone95' 'zone27'
 'zone132' 'zone158' 'zone24' 'zone28' 'zone1' 'zone142' 'zone164'
 'zone100' 'zone12' 'zone133' 'zone51' 'zone16' 'zone15' 'zone63']
Nbumber of unique zones: 65
Train, val and test zones saved in csv file at: ../../results/resnet18_128_l1/stratified_shuffling_by_zone/seed1/img_ids_by_set.csv
Image shape: torch.Size([64, 4, 128, 128]), Mask shape: torch.Size([64])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask: [4], dtype: torch.uint8
Train: 9952 images, Val: 3238 images, Test: 3112 images
Train: 61.05%, Val: 19.86%, Test: 19.09%
Creating model...
Model settings:
Pretrained: None
Classes: 6
The model is Resnet18
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: CrossEntropy
Optimizer: Adam
Training...
Epoch 1/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 1/70: train loss 1.2479, val loss 2.5513
Epoch 1/70: train mF1 0.4587, val mF1 0.1554
Time: 0:00:50.831842
Epoch 2/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 2/70: train loss 0.9872, val loss 1.9661
Epoch 2/70: train mF1 0.5738, val mF1 0.2725
Time: 0:00:49.902491
Epoch 3/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 3/70: train loss 0.9045, val loss 2.0275
Epoch 3/70: train mF1 0.6247, val mF1 0.2710
Time: 0:00:49.398605
Epoch 4/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 4/70: train loss 0.8250, val loss 1.5105
Epoch 4/70: train mF1 0.6544, val mF1 0.2958
Time: 0:00:50.391718
Epoch 5/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 5/70: train loss 0.7888, val loss 4.5704
Epoch 5/70: train mF1 0.6753, val mF1 0.0891
Time: 0:00:49.476097
Epoch 6/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 6/70: train loss 0.7190, val loss 2.1243
Epoch 6/70: train mF1 0.7059, val mF1 0.2923
Time: 0:00:48.844500
Epoch 7/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 7/70: train loss 0.6782, val loss 3.0319
Epoch 7/70: train mF1 0.7185, val mF1 0.1548
Time: 0:00:50.187077
Epoch 8/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 8/70: train loss 0.6472, val loss 2.2108
Epoch 8/70: train mF1 0.7335, val mF1 0.2495
Time: 0:00:48.526468
Epoch 9/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 9/70: train loss 0.5892, val loss 1.3771
Epoch 9/70: train mF1 0.7575, val mF1 0.3521
Time: 0:00:49.660675
Epoch 10/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 10/70: train loss 0.5369, val loss 2.3477
Epoch 10/70: train mF1 0.7800, val mF1 0.2328
Time: 0:00:48.931884
Epoch 11/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 11/70: train loss 0.4818, val loss 1.3492
Epoch 11/70: train mF1 0.8003, val mF1 0.3719
Time: 0:00:50.960158
Epoch 12/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 12/70: train loss 0.4454, val loss 4.0496
Epoch 12/70: train mF1 0.8175, val mF1 0.1383
Time: 0:00:49.508186
Epoch 13/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 13/70: train loss 0.3736, val loss 9.2864
Epoch 13/70: train mF1 0.8477, val mF1 0.1014
Time: 0:00:50.399435
Epoch 14/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 14/70: train loss 0.3059, val loss 3.5515
Epoch 14/70: train mF1 0.8768, val mF1 0.2330
Time: 0:00:48.739030
Epoch 15/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 15/70: train loss 0.2512, val loss 3.5726
Epoch 15/70: train mF1 0.8996, val mF1 0.2394
Time: 0:00:51.443805
Epoch 16/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 16/70: train loss 0.2048, val loss 2.9313
Epoch 16/70: train mF1 0.9191, val mF1 0.3059
Time: 0:00:48.821649
Epoch 17/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 17/70: train loss 0.1439, val loss 2.5497
Epoch 17/70: train mF1 0.9410, val mF1 0.2926
Time: 0:00:50.568284
Epoch 18/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 18/70: train loss 0.1125, val loss 2.7165
Epoch 18/70: train mF1 0.9528, val mF1 0.2958
Time: 0:00:48.638717
Epoch 19/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 19/70: train loss 0.1295, val loss 3.4863
Epoch 19/70: train mF1 0.9502, val mF1 0.2641
Time: 0:00:50.767060
Epoch 20/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 20/70: train loss 0.0727, val loss 2.9161
Epoch 20/70: train mF1 0.9733, val mF1 0.3069
Time: 0:00:48.017303
Epoch 21/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 21/70: train loss 0.0757, val loss 4.2871
Epoch 21/70: train mF1 0.9713, val mF1 0.2369
Time: 0:00:49.619453
Epoch 22/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 22/70: train loss 0.0771, val loss 2.5825
Epoch 22/70: train mF1 0.9695, val mF1 0.3007
Time: 0:00:47.991660
Epoch 23/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 23/70: train loss 0.0695, val loss 4.3276
Epoch 23/70: train mF1 0.9738, val mF1 0.2849
Time: 0:00:50.917192
Epoch 24/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 24/70: train loss 0.0535, val loss 2.8533
Epoch 24/70: train mF1 0.9800, val mF1 0.3308
Time: 0:00:47.978413
Epoch 25/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 25/70: train loss 0.0504, val loss 2.5788
Epoch 25/70: train mF1 0.9798, val mF1 0.3372
Time: 0:00:49.715637
Epoch 26/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 26/70: train loss 0.0541, val loss 5.5222
Epoch 26/70: train mF1 0.9809, val mF1 0.2457
Time: 0:00:48.142078
Epoch 27/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 27/70: train loss 0.0557, val loss 3.4595
Epoch 27/70: train mF1 0.9811, val mF1 0.2575
Time: 0:00:49.840290
Epoch 28/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 28/70: train loss 0.0454, val loss 3.4977
Epoch 28/70: train mF1 0.9817, val mF1 0.2960
Time: 0:00:48.229117
Epoch 29/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 29/70: train loss 0.0601, val loss 3.8429
Epoch 29/70: train mF1 0.9802, val mF1 0.2754
Time: 0:00:49.300433
Epoch 30/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 30/70: train loss 0.0424, val loss 4.1156
Epoch 30/70: train mF1 0.9839, val mF1 0.2556
Time: 0:00:49.646525
Epoch 31/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 31/70: train loss 0.0380, val loss 3.4490
Epoch 31/70: train mF1 0.9853, val mF1 0.3165
Time: 0:00:48.540839
Epoch 32/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 32/70: train loss 0.0335, val loss 3.8689
Epoch 32/70: train mF1 0.9877, val mF1 0.3015
Time: 0:00:49.120235
Epoch 33/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 33/70: train loss 0.0145, val loss 3.4968
Epoch 33/70: train mF1 0.9947, val mF1 0.3146
Time: 0:00:48.074439
Epoch 34/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 34/70: train loss 0.0262, val loss 7.9394
Epoch 34/70: train mF1 0.9911, val mF1 0.1905
Time: 0:00:49.494977
Epoch 35/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 35/70: train loss 0.0694, val loss 3.4053
Epoch 35/70: train mF1 0.9744, val mF1 0.3548
Time: 0:00:48.277469
Epoch 36/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 36/70: train loss 0.0631, val loss 3.3292
Epoch 36/70: train mF1 0.9772, val mF1 0.3001
Time: 0:00:49.735950
Epoch 37/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 37/70: train loss 0.0275, val loss 3.0733
Epoch 37/70: train mF1 0.9905, val mF1 0.3581
Time: 0:00:48.195177
Epoch 38/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 38/70: train loss 0.0171, val loss 5.4014
Epoch 38/70: train mF1 0.9935, val mF1 0.2471
Time: 0:00:49.998485
Epoch 39/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 39/70: train loss 0.0349, val loss 3.2598
Epoch 39/70: train mF1 0.9883, val mF1 0.3079
Time: 0:00:48.541326
Epoch 40/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 40/70: train loss 0.0306, val loss 5.5357
Epoch 40/70: train mF1 0.9888, val mF1 0.2475
Time: 0:00:49.414141
Epoch 41/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 41/70: train loss 0.0372, val loss 5.1893
Epoch 41/70: train mF1 0.9867, val mF1 0.2718
Time: 0:00:47.989301
Epoch 42/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 42/70: train loss 0.0379, val loss 3.7717
Epoch 42/70: train mF1 0.9881, val mF1 0.3328
Time: 0:00:49.889040
Epoch 43/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 43/70: train loss 0.0577, val loss 4.1610
Epoch 43/70: train mF1 0.9805, val mF1 0.2908
Time: 0:00:47.690418
Epoch 44/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 44/70: train loss 0.0165, val loss 3.2765
Epoch 44/70: train mF1 0.9931, val mF1 0.3098
Time: 0:00:49.685915
Epoch 45/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 45/70: train loss 0.0107, val loss 3.4151
Epoch 45/70: train mF1 0.9960, val mF1 0.3439
Time: 0:00:47.804523
Epoch 46/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 46/70: train loss 0.0256, val loss 4.0527
Epoch 46/70: train mF1 0.9912, val mF1 0.3183
Time: 0:00:50.137951
Epoch 47/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 47/70: train loss 0.0195, val loss 3.6285
Epoch 47/70: train mF1 0.9944, val mF1 0.3151
Time: 0:00:48.076226
Epoch 48/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 48/70: train loss 0.0324, val loss 11.3710
Epoch 48/70: train mF1 0.9888, val mF1 0.1849
Time: 0:00:49.870930
Epoch 49/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 49/70: train loss 0.0396, val loss 3.9029
Epoch 49/70: train mF1 0.9841, val mF1 0.3265
Time: 0:00:47.655419
Epoch 50/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 50/70: train loss 0.0112, val loss 4.0816
Epoch 50/70: train mF1 0.9963, val mF1 0.2950
Time: 0:00:50.172494
Epoch 51/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 51/70: train loss 0.0135, val loss 3.4306
Epoch 51/70: train mF1 0.9955, val mF1 0.3231
Time: 0:00:47.987688
Epoch 52/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 52/70: train loss 0.0289, val loss 4.0759
Epoch 52/70: train mF1 0.9904, val mF1 0.2712
Time: 0:00:49.801441
Epoch 53/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 53/70: train loss 0.0218, val loss 4.7905
Epoch 53/70: train mF1 0.9919, val mF1 0.2681
Time: 0:00:47.896780
Epoch 54/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 54/70: train loss 0.0447, val loss 5.6510
Epoch 54/70: train mF1 0.9835, val mF1 0.1985
Time: 0:00:50.512894
Epoch 55/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 55/70: train loss 0.0453, val loss 4.3294
Epoch 55/70: train mF1 0.9843, val mF1 0.2771
Time: 0:00:48.274232
Epoch 56/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 56/70: train loss 0.0116, val loss 3.5819
Epoch 56/70: train mF1 0.9954, val mF1 0.3091
Time: 0:00:50.175404
Epoch 57/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 57/70: train loss 0.0069, val loss 4.2605
Epoch 57/70: train mF1 0.9976, val mF1 0.2842
Time: 0:00:48.037090
Epoch 58/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 58/70: train loss 0.0081, val loss 4.5734
Epoch 58/70: train mF1 0.9979, val mF1 0.3492
Time: 0:00:50.701290
Epoch 59/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 59/70: train loss 0.0224, val loss 3.9854
Epoch 59/70: train mF1 0.9930, val mF1 0.3333
Time: 0:00:48.186646
Epoch 60/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 60/70: train loss 0.0134, val loss 3.9714
Epoch 60/70: train mF1 0.9956, val mF1 0.2754
Time: 0:00:50.012910
Epoch 61/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 61/70: train loss 0.0247, val loss 3.2394
Epoch 61/70: train mF1 0.9921, val mF1 0.3147
Time: 0:00:49.737404
Epoch 62/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 62/70: train loss 0.0300, val loss 4.2016
Epoch 62/70: train mF1 0.9881, val mF1 0.3197
Time: 0:00:48.807574
Epoch 63/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 63/70: train loss 0.0203, val loss 3.7794
Epoch 63/70: train mF1 0.9947, val mF1 0.3032
Time: 0:00:49.440434
Epoch 64/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 64/70: train loss 0.0203, val loss 3.8964
Epoch 64/70: train mF1 0.9941, val mF1 0.3244
Time: 0:00:48.795937
Epoch 65/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 65/70: train loss 0.0164, val loss 5.8865
Epoch 65/70: train mF1 0.9940, val mF1 0.2398
Time: 0:00:50.044663
Epoch 66/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 66/70: train loss 0.0158, val loss 4.3062
Epoch 66/70: train mF1 0.9937, val mF1 0.3163
Time: 0:00:49.255226
Epoch 67/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 67/70: train loss 0.0225, val loss 4.0044
Epoch 67/70: train mF1 0.9915, val mF1 0.3346
Time: 0:00:49.538547
Epoch 68/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 68/70: train loss 0.0129, val loss 4.1315
Epoch 68/70: train mF1 0.9951, val mF1 0.3148
Time: 0:00:48.483679
Epoch 69/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 69/70: train loss 0.0073, val loss 3.5847
Epoch 69/70: train mF1 0.9973, val mF1 0.3379
Time: 0:00:50.465537
Epoch 70/70
Training
Batch: 0  over  156
Batch: 50  over  156
Batch: 100  over  156
Batch: 150  over  156
Validation
Batch: 0  over  51
Batch: 50  over  51
Epoch 70/70: train loss 0.0066, val loss 4.2217
Epoch 70/70: train mF1 0.9981, val mF1 0.3242
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
main.py:271: RuntimeWarning: invalid value encountered in divide
  confusion_matrix_normalized = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]
Time: 0:00:48.309051
Testing
Batch: 0  over  49
Test F1 by class: [0.23106948 0.35510204 0.47319451 0.13605442 0.48667851 0.        ]
Test mF1: 0.28034982558840615
Plot saved at: ../../results/resnet18_128_l1/stratified_shuffling_by_zone/128_resnet18_strat_zone_homogene_lr3_2023_med_bs64/metrics_test/test_preds.png
