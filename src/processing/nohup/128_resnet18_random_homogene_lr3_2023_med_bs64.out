----------------------- UNet -----------------------
Patch size: 128
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: 2023
Patches: homogeneous
Batch size: 64
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  2023  year
30093 homogeneous masks found
18151  kept masks from 2023 zones
The data are from 2023
Only mediterranean zones kept
Number of masks: 16302
Image shape: torch.Size([64, 4, 128, 128]), Mask shape: torch.Size([64])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask: [0], dtype: torch.uint8
Train: 10890 images, Val: 3630 images, Test: 3631 images
Train: 60.00%, Val: 20.00%, Test: 20.00%
Creating model...
Model settings:
Pretrained: None
Classes: 6
The model is Resnet18
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: CrossEntropy
Optimizer: Adam
Training...
Epoch 1/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 1/70: train loss 1.2199, val loss 2.2687
Epoch 1/70: train mF1 0.4615, val mF1 0.2503
Time: 0:00:54.417398
Epoch 2/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 2/70: train loss 1.0167, val loss 1.5527
Epoch 2/70: train mF1 0.5489, val mF1 0.4582
Time: 0:00:55.647566
Epoch 3/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 3/70: train loss 0.9008, val loss 1.0412
Epoch 3/70: train mF1 0.6066, val mF1 0.5610
Time: 0:00:53.193982
Epoch 4/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 4/70: train loss 0.8472, val loss 2.3279
Epoch 4/70: train mF1 0.6409, val mF1 0.2776
Time: 0:00:55.058011
Epoch 5/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 5/70: train loss 0.7933, val loss 1.3320
Epoch 5/70: train mF1 0.6603, val mF1 0.4584
Time: 0:00:54.201671
Epoch 6/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 6/70: train loss 0.7589, val loss 1.6869
Epoch 6/70: train mF1 0.6704, val mF1 0.4141
Time: 0:00:55.005498
Epoch 7/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 7/70: train loss 0.7137, val loss 1.1127
Epoch 7/70: train mF1 0.6934, val mF1 0.5794
Time: 0:00:54.814426
Epoch 8/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 8/70: train loss 0.6428, val loss 1.6420
Epoch 8/70: train mF1 0.7321, val mF1 0.4823
Time: 0:00:53.379741
Epoch 9/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 9/70: train loss 0.6310, val loss 0.9144
Epoch 9/70: train mF1 0.7350, val mF1 0.6072
Time: 0:00:55.700116
Epoch 10/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 10/70: train loss 0.5684, val loss 1.1570
Epoch 10/70: train mF1 0.7561, val mF1 0.5734
Time: 0:00:53.398324
Epoch 11/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 11/70: train loss 0.5443, val loss 1.0246
Epoch 11/70: train mF1 0.7707, val mF1 0.6191
Time: 0:00:54.580931
Epoch 12/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 12/70: train loss 0.4806, val loss 1.7422
Epoch 12/70: train mF1 0.7919, val mF1 0.5190
Time: 0:00:53.529460
Epoch 13/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 13/70: train loss 0.4104, val loss 1.1233
Epoch 13/70: train mF1 0.8209, val mF1 0.5981
Time: 0:00:55.184063
Epoch 14/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 14/70: train loss 0.3602, val loss 0.9608
Epoch 14/70: train mF1 0.8422, val mF1 0.6573
Time: 0:00:54.757931
Epoch 15/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 15/70: train loss 0.2788, val loss 1.0651
Epoch 15/70: train mF1 0.8840, val mF1 0.6477
Time: 0:00:52.795861
Epoch 16/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 16/70: train loss 0.2376, val loss 1.4033
Epoch 16/70: train mF1 0.8985, val mF1 0.6051
Time: 0:00:55.430804
Epoch 17/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 17/70: train loss 0.1688, val loss 1.0840
Epoch 17/70: train mF1 0.9312, val mF1 0.6920
Time: 0:00:52.784609
Epoch 18/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 18/70: train loss 0.1592, val loss 1.0204
Epoch 18/70: train mF1 0.9327, val mF1 0.6895
Time: 0:00:54.478253
Epoch 19/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 19/70: train loss 0.1063, val loss 1.3303
Epoch 19/70: train mF1 0.9556, val mF1 0.6820
Time: 0:00:53.094983
Epoch 20/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 20/70: train loss 0.1525, val loss 1.4440
Epoch 20/70: train mF1 0.9430, val mF1 0.6517
Time: 0:00:55.273691
Epoch 21/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 21/70: train loss 0.1130, val loss 1.1993
Epoch 21/70: train mF1 0.9576, val mF1 0.6877
Time: 0:00:54.625273
Epoch 22/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 22/70: train loss 0.0803, val loss 1.3859
Epoch 22/70: train mF1 0.9686, val mF1 0.6599
Time: 0:00:54.523698
Epoch 23/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 23/70: train loss 0.0433, val loss 1.3183
Epoch 23/70: train mF1 0.9846, val mF1 0.7080
Time: 0:00:56.743617
Epoch 24/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 24/70: train loss 0.0341, val loss 1.6946
Epoch 24/70: train mF1 0.9900, val mF1 0.6439
Time: 0:00:53.200168
Epoch 25/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 25/70: train loss 0.1287, val loss 1.2821
Epoch 25/70: train mF1 0.9551, val mF1 0.6970
Time: 0:00:54.141010
Epoch 26/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 26/70: train loss 0.0804, val loss 3.0140
Epoch 26/70: train mF1 0.9671, val mF1 0.5270
Time: 0:00:52.779119
Epoch 27/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 27/70: train loss 0.0381, val loss 1.4919
Epoch 27/70: train mF1 0.9871, val mF1 0.6890
Time: 0:00:55.179391
Epoch 28/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 28/70: train loss 0.0282, val loss 2.0799
Epoch 28/70: train mF1 0.9911, val mF1 0.5917
Time: 0:00:54.532291
Epoch 29/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 29/70: train loss 0.1011, val loss 2.1855
Epoch 29/70: train mF1 0.9651, val mF1 0.5818
Time: 0:00:53.290929
Epoch 30/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 30/70: train loss 0.0658, val loss 1.6437
Epoch 30/70: train mF1 0.9744, val mF1 0.6639
Time: 0:00:54.873373
Epoch 31/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 31/70: train loss 0.0455, val loss 2.0501
Epoch 31/70: train mF1 0.9847, val mF1 0.6205
Time: 0:00:52.350892
Epoch 32/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 32/70: train loss 0.0414, val loss 1.6532
Epoch 32/70: train mF1 0.9847, val mF1 0.6708
Time: 0:00:54.293550
Epoch 33/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 33/70: train loss 0.0349, val loss 2.9922
Epoch 33/70: train mF1 0.9914, val mF1 0.5827
Time: 0:00:52.231349
Epoch 34/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 34/70: train loss 0.1100, val loss 1.4058
Epoch 34/70: train mF1 0.9622, val mF1 0.6784
Time: 0:00:54.930266
Epoch 35/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 35/70: train loss 0.0258, val loss 1.4479
Epoch 35/70: train mF1 0.9911, val mF1 0.7003
Time: 0:00:54.579767
Epoch 36/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 36/70: train loss 0.0238, val loss 1.4633
Epoch 36/70: train mF1 0.9915, val mF1 0.7036
Time: 0:00:55.006832
Epoch 37/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 37/70: train loss 0.0127, val loss 1.6950
Epoch 37/70: train mF1 0.9978, val mF1 0.6830
Time: 0:00:56.418216
Epoch 38/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 38/70: train loss 0.0424, val loss 1.5259
Epoch 38/70: train mF1 0.9862, val mF1 0.6890
Time: 0:00:53.383440
Epoch 39/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 39/70: train loss 0.0412, val loss 2.2485
Epoch 39/70: train mF1 0.9877, val mF1 0.5852
Time: 0:00:56.488205
Epoch 40/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 40/70: train loss 0.0638, val loss 1.4915
Epoch 40/70: train mF1 0.9745, val mF1 0.6877
Time: 0:00:54.775118
Epoch 41/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 41/70: train loss 0.0374, val loss 1.6970
Epoch 41/70: train mF1 0.9858, val mF1 0.6511
Time: 0:00:55.516377
Epoch 42/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 42/70: train loss 0.0371, val loss 1.6730
Epoch 42/70: train mF1 0.9876, val mF1 0.6821
Time: 0:00:55.971004
Epoch 43/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 43/70: train loss 0.0420, val loss 2.2946
Epoch 43/70: train mF1 0.9839, val mF1 0.6274
Time: 0:00:54.426549
Epoch 44/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 44/70: train loss 0.0099, val loss 1.5830
Epoch 44/70: train mF1 0.9972, val mF1 0.7028
Time: 0:00:56.869489
Epoch 45/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 45/70: train loss 0.0080, val loss 1.8534
Epoch 45/70: train mF1 0.9980, val mF1 0.6529
Time: 0:00:53.515103
Epoch 46/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 46/70: train loss 0.0387, val loss 1.7525
Epoch 46/70: train mF1 0.9863, val mF1 0.6650
Time: 0:00:55.788063
Epoch 47/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 47/70: train loss 0.0320, val loss 1.8140
Epoch 47/70: train mF1 0.9875, val mF1 0.6593
Time: 0:00:56.877713
Epoch 48/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 48/70: train loss 0.0384, val loss 1.7325
Epoch 48/70: train mF1 0.9859, val mF1 0.6785
Time: 0:00:54.580383
Epoch 49/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 49/70: train loss 0.0266, val loss 2.0293
Epoch 49/70: train mF1 0.9910, val mF1 0.6343
Time: 0:00:55.359655
Epoch 50/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 50/70: train loss 0.0678, val loss 1.8392
Epoch 50/70: train mF1 0.9746, val mF1 0.6654
Time: 0:00:53.840420
Epoch 51/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 51/70: train loss 0.0379, val loss 1.6020
Epoch 51/70: train mF1 0.9836, val mF1 0.6978
Time: 0:00:56.874416
Epoch 52/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 52/70: train loss 0.0104, val loss 1.7252
Epoch 52/70: train mF1 0.9973, val mF1 0.6765
Time: 0:00:54.668610
Epoch 53/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 53/70: train loss 0.0151, val loss 1.6655
Epoch 53/70: train mF1 0.9950, val mF1 0.6907
Time: 0:00:56.068032
Epoch 54/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 54/70: train loss 0.0037, val loss 1.7451
Epoch 54/70: train mF1 0.9992, val mF1 0.6816
Time: 0:00:56.267817
Epoch 55/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 55/70: train loss 0.0048, val loss 1.8491
Epoch 55/70: train mF1 0.9991, val mF1 0.6700
Time: 0:00:54.747878
Epoch 56/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 56/70: train loss 0.0964, val loss 1.6581
Epoch 56/70: train mF1 0.9697, val mF1 0.6887
Time: 0:00:56.115182
Epoch 57/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 57/70: train loss 0.0230, val loss 2.8847
Epoch 57/70: train mF1 0.9910, val mF1 0.5948
Time: 0:00:53.420390
Epoch 58/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 58/70: train loss 0.0092, val loss 1.6340
Epoch 58/70: train mF1 0.9962, val mF1 0.7044
Time: 0:00:57.422404
Epoch 59/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 59/70: train loss 0.0070, val loss 1.6612
Epoch 59/70: train mF1 0.9984, val mF1 0.6949
Time: 0:00:55.646462
Epoch 60/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 60/70: train loss 0.0111, val loss 1.7932
Epoch 60/70: train mF1 0.9965, val mF1 0.6767
Time: 0:00:55.741581
Epoch 61/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 61/70: train loss 0.0132, val loss 1.8545
Epoch 61/70: train mF1 0.9955, val mF1 0.6777
Time: 0:00:56.750858
Epoch 62/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 62/70: train loss 0.0248, val loss 2.4800
Epoch 62/70: train mF1 0.9920, val mF1 0.5773
Time: 0:00:53.436649
Epoch 63/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 63/70: train loss 0.0482, val loss 1.7238
Epoch 63/70: train mF1 0.9829, val mF1 0.6808
Time: 0:00:54.899820
Epoch 64/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 64/70: train loss 0.0416, val loss 1.7063
Epoch 64/70: train mF1 0.9855, val mF1 0.6737
Time: 0:00:53.197379
Epoch 65/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 65/70: train loss 0.0544, val loss 1.6524
Epoch 65/70: train mF1 0.9803, val mF1 0.6966
Time: 0:00:56.388236
Epoch 66/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 66/70: train loss 0.0088, val loss 1.7510
Epoch 66/70: train mF1 0.9983, val mF1 0.6758
Time: 0:00:54.693035
Epoch 67/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 67/70: train loss 0.0237, val loss 1.7566
Epoch 67/70: train mF1 0.9928, val mF1 0.7003
Time: 0:00:55.269262
Epoch 68/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 68/70: train loss 0.0053, val loss 1.6603
Epoch 68/70: train mF1 0.9984, val mF1 0.7085
Time: 0:00:55.745362
Epoch 69/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 69/70: train loss 0.0039, val loss 1.7346
Epoch 69/70: train mF1 0.9987, val mF1 0.6991
Time: 0:00:52.773724
Epoch 70/70
Training
Batch: 0  over  171
Batch: 50  over  171
Batch: 100  over  171
Batch: 150  over  171
Validation
Batch: 0  over  57
Batch: 50  over  57
Epoch 70/70: train loss 0.0017, val loss 1.7565
Epoch 70/70: train mF1 0.9997, val mF1 0.7120
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Time: 0:00:54.873311
Testing
Batch: 0  over  57
Batch: 50  over  57
Test F1 by class: [0.59735099 0.52750809 0.77855823 0.46680942 0.67169811 0.58287796]
Test mF1: 0.6041338007461529
Plot saved at: ../../results/resnet18_128_l1/random_shuffling/128_resnet18_random_homogene_lr3_2023_med_bs64/metrics_test/test_preds.png
