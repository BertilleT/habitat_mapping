----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: all
Patches: all
Batch size: 16
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Image shape: torch.Size([16, 4, 256, 256]), Mask shape: torch.Size([16, 7])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask unique values: [0. 1.], dtype: torch.float32
Mask:  tensor([0., 0., 0., 0., 0., 1., 0.])
Train: 6397 images, Val: 2133 images, Test: 2133 images
Train: 59.99%, Val: 20.00%, Test: 20.00%
Creating model...
Model settings:
Pretrained: False
Classes: 6
The model is Resnet18
Using BCEWithDigits criterion
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: BCEWithDigits
Optimizer: Adam
Training...
Epoch 1/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 1/70: train loss 0.4210, val loss 0.4078
Epoch 1/70: train mF1 0.2479, val mF1 0.2517
Time: 0:01:10.827321
Epoch 2/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 2/70: train loss 0.3924, val loss 0.4033
Epoch 2/70: train mF1 0.2809, val mF1 0.3116
Time: 0:01:07.508782
Epoch 3/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 3/70: train loss 0.3806, val loss 0.3776
Epoch 3/70: train mF1 0.3228, val mF1 0.3247
Time: 0:01:07.481251
Epoch 4/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 4/70: train loss 0.3713, val loss 0.3966
Epoch 4/70: train mF1 0.3474, val mF1 0.3405
Time: 0:01:09.249185
Epoch 5/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 5/70: train loss 0.3658, val loss 0.3754
Epoch 5/70: train mF1 0.3766, val mF1 0.3240
Time: 0:01:06.784431
Epoch 6/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 6/70: train loss 0.3572, val loss 0.3707
Epoch 6/70: train mF1 0.3978, val mF1 0.4047
Time: 0:01:09.451440
Epoch 7/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 7/70: train loss 0.3500, val loss 0.3776
Epoch 7/70: train mF1 0.4284, val mF1 0.4368
Time: 0:01:07.928242
Epoch 8/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 8/70: train loss 0.3437, val loss 0.3627
Epoch 8/70: train mF1 0.4421, val mF1 0.4158
Time: 0:01:07.274633
Epoch 9/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 9/70: train loss 0.3374, val loss 0.3590
Epoch 9/70: train mF1 0.4800, val mF1 0.4727
Time: 0:01:09.158652
Epoch 10/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 10/70: train loss 0.3307, val loss 0.3307
Epoch 10/70: train mF1 0.4880, val mF1 0.4480
Time: 0:01:07.319374
Epoch 11/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 11/70: train loss 0.3245, val loss 0.3503
Epoch 11/70: train mF1 0.5118, val mF1 0.4699
Time: 0:01:09.663644
Epoch 12/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 12/70: train loss 0.3187, val loss 0.3256
Epoch 12/70: train mF1 0.5169, val mF1 0.5514
Time: 0:01:06.808771
Epoch 13/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 13/70: train loss 0.3088, val loss 0.4299
Epoch 13/70: train mF1 0.5476, val mF1 0.3941
Time: 0:01:08.949269
Epoch 14/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 14/70: train loss 0.3010, val loss 0.3660
Epoch 14/70: train mF1 0.5666, val mF1 0.5041
Time: 0:01:08.022932
Epoch 15/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 15/70: train loss 0.2965, val loss 0.3468
Epoch 15/70: train mF1 0.5821, val mF1 0.5131
Time: 0:01:07.585289
Epoch 16/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 16/70: train loss 0.2860, val loss 0.3164
Epoch 16/70: train mF1 0.5986, val mF1 0.5667
Time: 0:01:09.953685
Epoch 17/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 17/70: train loss 0.2773, val loss 0.3365
Epoch 17/70: train mF1 0.6244, val mF1 0.5731
Time: 0:01:08.049942
Epoch 18/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 18/70: train loss 0.2599, val loss 0.3270
Epoch 18/70: train mF1 0.6561, val mF1 0.6042
Time: 0:01:09.927219
Epoch 19/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 19/70: train loss 0.2496, val loss 0.3299
Epoch 19/70: train mF1 0.6709, val mF1 0.5888
Time: 0:01:09.278295
Epoch 20/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 20/70: train loss 0.2313, val loss 0.3874
Epoch 20/70: train mF1 0.7083, val mF1 0.5223
Time: 0:01:08.613485
Epoch 21/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 21/70: train loss 0.2109, val loss 0.3801
Epoch 21/70: train mF1 0.7408, val mF1 0.5552
Time: 0:01:10.879114
Epoch 22/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 22/70: train loss 0.1844, val loss 0.3935
Epoch 22/70: train mF1 0.7850, val mF1 0.5602
Time: 0:01:08.676360
Epoch 23/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 23/70: train loss 0.1538, val loss 0.4166
Epoch 23/70: train mF1 0.8243, val mF1 0.6059
Time: 0:01:10.325611
Epoch 24/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 24/70: train loss 0.1250, val loss 0.4865
Epoch 24/70: train mF1 0.8653, val mF1 0.5625
Time: 0:01:08.692256
Epoch 25/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 25/70: train loss 0.0982, val loss 0.4985
Epoch 25/70: train mF1 0.8986, val mF1 0.5876
Time: 0:01:10.875006
Epoch 26/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 26/70: train loss 0.0852, val loss 0.4836
Epoch 26/70: train mF1 0.9096, val mF1 0.5919
Time: 0:01:09.923429
Epoch 27/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 27/70: train loss 0.0617, val loss 0.5250
Epoch 27/70: train mF1 0.9397, val mF1 0.5895
Time: 0:01:08.516643
Epoch 28/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 28/70: train loss 0.0545, val loss 0.6403
Epoch 28/70: train mF1 0.9515, val mF1 0.5311
Time: 0:01:10.637294
Epoch 29/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 29/70: train loss 0.0508, val loss 0.7128
Epoch 29/70: train mF1 0.9531, val mF1 0.5495
Time: 0:01:08.976931
Epoch 30/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 30/70: train loss 0.0467, val loss 0.6651
Epoch 30/70: train mF1 0.9560, val mF1 0.5631
Time: 0:01:10.993913
Epoch 31/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 31/70: train loss 0.0370, val loss 0.6171
Epoch 31/70: train mF1 0.9672, val mF1 0.5916
Time: 0:01:08.780323
Epoch 32/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 32/70: train loss 0.0411, val loss 0.6413
Epoch 32/70: train mF1 0.9624, val mF1 0.5925
Time: 0:01:09.450801
Epoch 33/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 33/70: train loss 0.0366, val loss 0.6941
Epoch 33/70: train mF1 0.9669, val mF1 0.5671
Time: 0:01:09.040375
Epoch 34/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 34/70: train loss 0.0314, val loss 0.6780
Epoch 34/70: train mF1 0.9740, val mF1 0.5557
Time: 0:01:08.485828
Epoch 35/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 35/70: train loss 0.0281, val loss 0.7390
Epoch 35/70: train mF1 0.9740, val mF1 0.5899
Time: 0:01:08.791234
Epoch 36/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 36/70: train loss 0.0331, val loss 0.6524
Epoch 36/70: train mF1 0.9724, val mF1 0.5807
Time: 0:01:07.968588
Epoch 37/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 37/70: train loss 0.0247, val loss 0.7015
Epoch 37/70: train mF1 0.9791, val mF1 0.5847
Time: 0:01:08.797630
Epoch 38/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 38/70: train loss 0.0201, val loss 0.7158
Epoch 38/70: train mF1 0.9845, val mF1 0.5946
Time: 0:01:09.074017
Epoch 39/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 39/70: train loss 0.0262, val loss 0.7895
Epoch 39/70: train mF1 0.9792, val mF1 0.5791
Time: 0:01:08.208396
Epoch 40/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 40/70: train loss 0.0270, val loss 0.7469
Epoch 40/70: train mF1 0.9761, val mF1 0.5822
Time: 0:01:07.469843
Epoch 41/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 41/70: train loss 0.0234, val loss 0.7233
Epoch 41/70: train mF1 0.9790, val mF1 0.6044
Time: 0:01:07.655562
Epoch 42/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 42/70: train loss 0.0241, val loss 0.7764
Epoch 42/70: train mF1 0.9797, val mF1 0.5834
Time: 0:01:07.955628
Epoch 43/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 43/70: train loss 0.0284, val loss 0.7061
Epoch 43/70: train mF1 0.9755, val mF1 0.5745
Time: 0:01:07.692521
Epoch 44/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 44/70: train loss 0.0166, val loss 0.8449
Epoch 44/70: train mF1 0.9857, val mF1 0.5328
Time: 0:01:07.344197
Epoch 45/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 45/70: train loss 0.0171, val loss 0.7759
Epoch 45/70: train mF1 0.9868, val mF1 0.5682
Time: 0:01:08.231443
Epoch 46/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 46/70: train loss 0.0214, val loss 0.7701
Epoch 46/70: train mF1 0.9809, val mF1 0.5715
Time: 0:01:08.067437
Epoch 47/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 47/70: train loss 0.0238, val loss 0.7619
Epoch 47/70: train mF1 0.9797, val mF1 0.5687
Time: 0:01:08.300697
Epoch 48/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 48/70: train loss 0.0167, val loss 0.7517
Epoch 48/70: train mF1 0.9845, val mF1 0.5797
Time: 0:01:07.902471
Epoch 49/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 49/70: train loss 0.0131, val loss 0.8366
Epoch 49/70: train mF1 0.9880, val mF1 0.5619
Time: 0:01:08.023680
Epoch 50/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 50/70: train loss 0.0183, val loss 0.7975
Epoch 50/70: train mF1 0.9861, val mF1 0.5867
Time: 0:01:08.785251
Epoch 51/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 51/70: train loss 0.0206, val loss 0.8180
Epoch 51/70: train mF1 0.9840, val mF1 0.5662
Time: 0:01:07.620774
Epoch 52/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 52/70: train loss 0.0168, val loss 0.7761
Epoch 52/70: train mF1 0.9859, val mF1 0.5627
Time: 0:01:07.977464
Epoch 53/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 53/70: train loss 0.0123, val loss 0.8207
Epoch 53/70: train mF1 0.9913, val mF1 0.5686
Time: 0:01:08.201812
Epoch 54/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 54/70: train loss 0.0182, val loss 0.8295
Epoch 54/70: train mF1 0.9847, val mF1 0.5987
Time: 0:01:07.621096
Epoch 55/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 55/70: train loss 0.0155, val loss 0.8777
Epoch 55/70: train mF1 0.9859, val mF1 0.5699
Time: 0:01:07.728306
Epoch 56/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 56/70: train loss 0.0121, val loss 0.8097
Epoch 56/70: train mF1 0.9903, val mF1 0.5773
Time: 0:01:08.201641
Epoch 57/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 57/70: train loss 0.0145, val loss 0.8678
Epoch 57/70: train mF1 0.9888, val mF1 0.5382
Time: 0:01:08.343206
Epoch 58/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 58/70: train loss 0.0146, val loss 0.8193
Epoch 58/70: train mF1 0.9888, val mF1 0.5908
Time: 0:01:08.110198
Epoch 59/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 59/70: train loss 0.0187, val loss 0.7709
Epoch 59/70: train mF1 0.9851, val mF1 0.5859
Time: 0:01:07.804790
Epoch 60/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 60/70: train loss 0.0114, val loss 0.7681
Epoch 60/70: train mF1 0.9917, val mF1 0.5947
Time: 0:01:07.827732
Epoch 61/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 61/70: train loss 0.0098, val loss 0.7750
Epoch 61/70: train mF1 0.9930, val mF1 0.5832
Time: 0:01:07.484555
Epoch 62/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 62/70: train loss 0.0154, val loss 1.0094
Epoch 62/70: train mF1 0.9882, val mF1 0.5118
Time: 0:01:08.472024
Epoch 63/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 63/70: train loss 0.0126, val loss 0.7982
Epoch 63/70: train mF1 0.9882, val mF1 0.5708
Time: 0:01:09.005622
Epoch 64/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 64/70: train loss 0.0077, val loss 0.8126
Epoch 64/70: train mF1 0.9950, val mF1 0.5916
Time: 0:01:08.986246
Epoch 65/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 65/70: train loss 0.0176, val loss 0.9301
Epoch 65/70: train mF1 0.9864, val mF1 0.5450
Time: 0:01:09.275027
Epoch 66/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 66/70: train loss 0.0179, val loss 0.7760
Epoch 66/70: train mF1 0.9862, val mF1 0.5717
Time: 0:01:08.061898
Epoch 67/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 67/70: train loss 0.0108, val loss 0.8241
Epoch 67/70: train mF1 0.9918, val mF1 0.5819
Time: 0:01:07.403838
Epoch 68/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 68/70: train loss 0.0077, val loss 0.8332
Epoch 68/70: train mF1 0.9949, val mF1 0.5931
Time: 0:01:06.525461
Epoch 69/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 69/70: train loss 0.0097, val loss 0.8556
Epoch 69/70: train mF1 0.9933, val mF1 0.5792
Time: 0:01:06.271355
Epoch 70/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 70/70: train loss 0.0114, val loss 0.7902
Epoch 70/70: train mF1 0.9912, val mF1 0.5881
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/bertille/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Time: 0:01:06.426650
Testing
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Test F1 by class: [0.13436693 0.62773723 0.78356164 0.68698061 0.4375     0.37426901
 0.77620632]
Test mF1: 0.5458031047484521
Confusion matrix:  [[[1772   17]
  [ 318   26]]

 [[1663   68]
  [ 187  215]]

 [[1245  181]
  [ 135  572]]

 [[1659  138]
  [  88  248]]

 [[1883   29]
  [ 151   70]]

 [[1994   16]
  [  91   32]]

 [[ 662  324]
  [ 214  933]]]
Normalized confusion matrix:  [array([[0.99049748, 0.00950252],
       [0.9244186 , 0.0755814 ]]), array([[0.96071635, 0.03928365],
       [0.46517413, 0.53482587]]), array([[0.87307153, 0.12692847],
       [0.19094767, 0.80905233]]), array([[0.92320534, 0.07679466],
       [0.26190476, 0.73809524]]), array([[0.98483264, 0.01516736],
       [0.68325792, 0.31674208]]), array([[0.9920398, 0.0079602],
       [0.7398374, 0.2601626]]), array([[0.67139959, 0.32860041],
       [0.18657367, 0.81342633]])]
msk_ [1. 0. 0. 0. 0. 0.]
msk_ [0]
msk_ [0. 0. 1. 0. 0. 0.]
msk_ [2]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
msk_ [0. 0. 0. 1. 0. 0.]
msk_ [3]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
msk_ [1. 0. 0. 0. 0. 0.]
msk_ [0]
msk_ [0. 0. 0. 0. 1. 0.]
msk_ [4]
msk_ [0. 0. 0. 1. 0. 0.]
msk_ [3]
msk_ [1. 0. 0. 0. 0. 0.]
msk_ [0]
msk_ [0. 0. 0. 0. 1. 0.]
msk_ [4]
msk_ [0. 0. 0. 0. 0. 1.]
msk_ [5]
msk_ [0. 0. 1. 0. 0. 0.]
msk_ [2]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
msk_ [0. 0. 1. 0. 0. 0.]
msk_ [2]
Plot saved at: ../../results/resnet18_256_l1/random_shuffling/resnet18_random_all_patches_multi_label_70epochs/metrics_test/test_preds.png
