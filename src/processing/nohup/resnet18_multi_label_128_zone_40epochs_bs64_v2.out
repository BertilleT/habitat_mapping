----------------------- UNet -----------------------
Patch size: 128
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.68, 0.2]
Stratified: zone
Year: all
Patches: all
Batch size: 64
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Nbumber of unique zones: 108
Train, val and test zones saved in csv file at: ../../results/resnet18_128_l1/stratified_shuffling_by_zone/seed1/img_ids_by_set.csv
Image shape: torch.Size([64, 4, 128, 128]), Mask shape: torch.Size([64, 7])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask unique values: [0. 1.], dtype: torch.float32
Mask:  tensor([0., 0., 1., 1., 0., 1., 1.])
Train: 25531 images, Val: 9017 images, Test: 8080 images
Train: 59.89%, Val: 21.15%, Test: 18.95%
Creating model...
Model settings:
Pretrained: False
Classes: 7
The model is Resnet18
Using BCEWithDigits criterion
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: BCEWithDigits
Optimizer: Adam
Training...
Epoch 1/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 1/40: train loss 0.4250, val loss 0.4971
Epoch 1/40: train mF1 0.4592, val mF1 0.2933
Time: 0:02:21.133739
Epoch 2/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 2/40: train loss 0.3837, val loss 0.5950
Epoch 2/40: train mF1 0.5341, val mF1 0.3313
Time: 0:02:17.346298
Epoch 3/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 3/40: train loss 0.3635, val loss 0.5678
Epoch 3/40: train mF1 0.5706, val mF1 0.3280
Time: 0:02:19.482974
Epoch 4/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 4/40: train loss 0.3493, val loss 0.6210
Epoch 4/40: train mF1 0.5948, val mF1 0.3056
Time: 0:02:19.445045
Epoch 5/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 5/40: train loss 0.3367, val loss 0.5701
Epoch 5/40: train mF1 0.6131, val mF1 0.3597
Time: 0:02:19.373959
Epoch 6/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 6/40: train loss 0.3262, val loss 0.5552
Epoch 6/40: train mF1 0.6311, val mF1 0.3491
Time: 0:02:24.141443
Epoch 7/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 7/40: train loss 0.3156, val loss 0.6608
Epoch 7/40: train mF1 0.6428, val mF1 0.3254
Time: 0:02:22.058154
Epoch 8/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 8/40: train loss 0.3028, val loss 0.7806
Epoch 8/40: train mF1 0.6627, val mF1 0.2858
Time: 0:02:22.907361
Epoch 9/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 9/40: train loss 0.2918, val loss 0.6213
Epoch 9/40: train mF1 0.6803, val mF1 0.3531
Time: 0:02:23.081693
Epoch 10/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 10/40: train loss 0.2794, val loss 0.5765
Epoch 10/40: train mF1 0.7011, val mF1 0.3838
Time: 0:02:22.944537
Epoch 11/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 11/40: train loss 0.2630, val loss 0.6744
Epoch 11/40: train mF1 0.7193, val mF1 0.3433
Time: 0:02:23.396053
Epoch 12/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 12/40: train loss 0.2444, val loss 0.6609
Epoch 12/40: train mF1 0.7465, val mF1 0.3949
Time: 0:02:22.846233
Epoch 13/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 13/40: train loss 0.2205, val loss 0.7052
Epoch 13/40: train mF1 0.7763, val mF1 0.3368
Time: 0:02:22.373338
Epoch 14/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 14/40: train loss 0.1882, val loss 1.7620
Epoch 14/40: train mF1 0.8150, val mF1 0.2690
Time: 0:02:23.115314
Epoch 15/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 15/40: train loss 0.1524, val loss 1.0239
Epoch 15/40: train mF1 0.8540, val mF1 0.3236
Time: 0:02:22.640857
Epoch 16/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 16/40: train loss 0.1161, val loss 1.0386
Epoch 16/40: train mF1 0.8937, val mF1 0.3683
Time: 0:02:23.959095
Epoch 17/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 17/40: train loss 0.0873, val loss 1.3073
Epoch 17/40: train mF1 0.9234, val mF1 0.3570
Time: 0:02:23.884096
Epoch 18/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 18/40: train loss 0.0664, val loss 1.1833
Epoch 18/40: train mF1 0.9442, val mF1 0.3845
Time: 0:02:21.181582
Epoch 19/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 19/40: train loss 0.0503, val loss 1.4261
Epoch 19/40: train mF1 0.9585, val mF1 0.3690
Time: 0:02:21.093345
Epoch 20/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 20/40: train loss 0.0423, val loss 1.5289
Epoch 20/40: train mF1 0.9657, val mF1 0.3854
Time: 0:02:21.547773
Epoch 21/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 21/40: train loss 0.0386, val loss 1.6372
Epoch 21/40: train mF1 0.9691, val mF1 0.3445
Time: 0:02:22.215015
Epoch 22/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 22/40: train loss 0.0327, val loss 1.5127
Epoch 22/40: train mF1 0.9735, val mF1 0.4009
Time: 0:02:21.185410
Epoch 23/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 23/40: train loss 0.0290, val loss 1.6613
Epoch 23/40: train mF1 0.9774, val mF1 0.3760
Time: 0:02:21.267263
Epoch 24/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 24/40: train loss 0.0297, val loss 1.5438
Epoch 24/40: train mF1 0.9762, val mF1 0.3577
Time: 0:02:20.887561
Epoch 25/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 25/40: train loss 0.0204, val loss 1.6734
Epoch 25/40: train mF1 0.9846, val mF1 0.3587
Time: 0:02:22.070652
Epoch 26/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 26/40: train loss 0.0241, val loss 1.4086
Epoch 26/40: train mF1 0.9814, val mF1 0.4121
Time: 0:02:20.693444
Epoch 27/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 27/40: train loss 0.0232, val loss 1.5713
Epoch 27/40: train mF1 0.9825, val mF1 0.3879
Time: 0:02:23.524614
Epoch 28/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 28/40: train loss 0.0205, val loss 1.5921
Epoch 28/40: train mF1 0.9844, val mF1 0.3672
Time: 0:02:23.120332
Epoch 29/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 29/40: train loss 0.0197, val loss 1.8026
Epoch 29/40: train mF1 0.9853, val mF1 0.3676
Time: 0:02:22.786124
Epoch 30/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 30/40: train loss 0.0174, val loss 2.1766
Epoch 30/40: train mF1 0.9870, val mF1 0.3631
Time: 0:02:22.236338
Epoch 31/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 31/40: train loss 0.0175, val loss 1.7174
Epoch 31/40: train mF1 0.9869, val mF1 0.3669
Time: 0:02:19.790637
Epoch 32/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 32/40: train loss 0.0180, val loss 2.0531
Epoch 32/40: train mF1 0.9867, val mF1 0.3757
Time: 0:02:23.060409
Epoch 33/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 33/40: train loss 0.0177, val loss 1.7768
Epoch 33/40: train mF1 0.9873, val mF1 0.3685
Time: 0:02:24.421285
Epoch 34/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 34/40: train loss 0.0128, val loss 1.8208
Epoch 34/40: train mF1 0.9910, val mF1 0.3702
Time: 0:02:24.172739
Epoch 35/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 35/40: train loss 0.0148, val loss 1.6650
Epoch 35/40: train mF1 0.9885, val mF1 0.3705
Time: 0:02:24.896468
Epoch 36/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 36/40: train loss 0.0153, val loss 1.8273
Epoch 36/40: train mF1 0.9883, val mF1 0.3700
Time: 0:02:24.885907
Epoch 37/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 37/40: train loss 0.0136, val loss 1.6393
Epoch 37/40: train mF1 0.9901, val mF1 0.3566
Time: 0:02:24.711132
Epoch 38/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 38/40: train loss 0.0130, val loss 1.7737
Epoch 38/40: train mF1 0.9907, val mF1 0.3577
Time: 0:02:29.983504
Epoch 39/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 39/40: train loss 0.0140, val loss 1.7668
Epoch 39/40: train mF1 0.9893, val mF1 0.3784
Time: 0:02:23.342866
Epoch 40/40
Training
Batch: 0  over  399
Batch: 50  over  399
Batch: 100  over  399
Batch: 150  over  399
Batch: 200  over  399
Batch: 250  over  399
Batch: 300  over  399
Batch: 350  over  399
Validation
Batch: 0  over  141
Batch: 50  over  141
Batch: 100  over  141
Epoch 40/40: train loss 0.0138, val loss 1.7051
Epoch 40/40: train mF1 0.9900, val mF1 0.3777
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Time: 0:02:23.864736
Testing
Batch: 0  over  127
Batch: 50  over  127
Batch: 100  over  127
Test F1 by class: [0.31563508 0.14612676 0.4819701  0.20619861 0.41513697 0.04342273
 0.28889662]
Test mF1: 0.27105526759453213
Plot saved at: ../../results/resnet18_128_l1/stratified_shuffling_by_zone/all/resnet18_multi_label_128_zone_40epochs_bs64_v2/metrics_test/test_preds.png
Reassembling the patches...
Reassembling the patches...
Reassembling the patches...
