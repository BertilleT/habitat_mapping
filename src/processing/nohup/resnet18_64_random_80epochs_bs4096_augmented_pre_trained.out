----------------------- UNet -----------------------
Patch size: 64
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: all
Patches: all
Batch size: 4096
Normalisation: channel_by_channel
Data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Image: min: 0.0, max: 1.0
Mask unique values: [0. 1.]
Train: 99564 images, Val: 33188 images, Test: 37716 images
Train: 58.41%, Val: 19.47%, Test: 22.12%
Creating model...
Model settings:
Pretrained: True
Classes: 7
The model is Resnet18
Pretrained weights loaded
Using BCEWithDigits criterion
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: BCEWithDigits
Optimizer: Adam
Training...
Epoch 1/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 1/80: train loss 0.4223, val loss 0.3863
Epoch 1/80: train mF1 0.3553, val mF1 0.4399
Time: 0:08:25.778718
Epoch 2/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 2/80: train loss 0.3494, val loss 0.3370
Epoch 2/80: train mF1 0.4715, val mF1 0.5285
Time: 0:08:31.746070
Epoch 3/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 3/80: train loss 0.3321, val loss 0.3408
Epoch 3/80: train mF1 0.5140, val mF1 0.5098
Time: 0:08:34.004366
Epoch 4/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 4/80: train loss 0.3184, val loss 0.3078
Epoch 4/80: train mF1 0.5412, val mF1 0.5685
Time: 0:08:35.103013
Epoch 5/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 5/80: train loss 0.3089, val loss 0.3261
Epoch 5/80: train mF1 0.5601, val mF1 0.5271
Time: 0:08:35.799670
Epoch 6/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 6/80: train loss 0.3027, val loss 0.3283
Epoch 6/80: train mF1 0.5738, val mF1 0.5392
Time: 0:08:35.443887
Epoch 7/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 7/80: train loss 0.2954, val loss 0.2981
Epoch 7/80: train mF1 0.5853, val mF1 0.6052
Time: 0:08:35.505116
Epoch 8/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 8/80: train loss 0.2905, val loss 0.2947
Epoch 8/80: train mF1 0.5950, val mF1 0.5951
Time: 0:08:33.704055
Epoch 9/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 9/80: train loss 0.2857, val loss 0.2843
Epoch 9/80: train mF1 0.6034, val mF1 0.6219
Time: 0:08:34.389494
Epoch 10/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 10/80: train loss 0.2816, val loss 0.2789
Epoch 10/80: train mF1 0.6113, val mF1 0.6163
Time: 0:08:34.727200
Epoch 11/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 11/80: train loss 0.2783, val loss 0.2834
Epoch 11/80: train mF1 0.6169, val mF1 0.6101
Time: 0:08:34.968611
Epoch 12/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 12/80: train loss 0.2756, val loss 0.2980
Epoch 12/80: train mF1 0.6199, val mF1 0.5985
Time: 0:08:35.772024
Epoch 13/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 13/80: train loss 0.2706, val loss 0.2890
Epoch 13/80: train mF1 0.6306, val mF1 0.6083
Time: 0:08:34.628038
Epoch 14/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 14/80: train loss 0.2677, val loss 0.2805
Epoch 14/80: train mF1 0.6360, val mF1 0.6273
Time: 0:08:36.219548
Epoch 15/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 15/80: train loss 0.2642, val loss 0.3081
Epoch 15/80: train mF1 0.6393, val mF1 0.5981
Time: 0:08:34.899876
Epoch 16/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 16/80: train loss 0.2629, val loss 0.2774
Epoch 16/80: train mF1 0.6417, val mF1 0.6449
Time: 0:08:35.694026
Epoch 17/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 17/80: train loss 0.2588, val loss 0.2726
Epoch 17/80: train mF1 0.6512, val mF1 0.6466
Time: 0:08:37.955091
Epoch 18/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 18/80: train loss 0.2565, val loss 0.2859
Epoch 18/80: train mF1 0.6530, val mF1 0.6322
Time: 0:08:54.996951
Epoch 19/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 19/80: train loss 0.2545, val loss 0.2735
Epoch 19/80: train mF1 0.6556, val mF1 0.6446
Time: 0:08:59.915017
Epoch 20/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 20/80: train loss 0.2514, val loss 0.2587
Epoch 20/80: train mF1 0.6622, val mF1 0.6610
Time: 0:08:59.588587
Epoch 21/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 21/80: train loss 0.2489, val loss 0.2575
Epoch 21/80: train mF1 0.6656, val mF1 0.6691
Time: 0:08:55.078121
Epoch 22/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 22/80: train loss 0.2465, val loss 0.2598
Epoch 22/80: train mF1 0.6691, val mF1 0.6667
Time: 0:08:53.520341
Epoch 23/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 23/80: train loss 0.2452, val loss 0.2643
Epoch 23/80: train mF1 0.6724, val mF1 0.6623
Time: 0:09:00.174562
Epoch 24/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 24/80: train loss 0.2426, val loss 0.2776
Epoch 24/80: train mF1 0.6765, val mF1 0.6395
Time: 0:08:59.359317
Epoch 25/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 25/80: train loss 0.2403, val loss 0.2613
Epoch 25/80: train mF1 0.6805, val mF1 0.6713
Time: 0:09:02.434893
Epoch 26/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 26/80: train loss 0.2368, val loss 0.2677
Epoch 26/80: train mF1 0.6842, val mF1 0.6599
Time: 0:09:02.034485
Epoch 27/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 27/80: train loss 0.2361, val loss 0.2671
Epoch 27/80: train mF1 0.6874, val mF1 0.6597
Time: 0:09:00.823761
Epoch 28/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 28/80: train loss 0.2336, val loss 0.2593
Epoch 28/80: train mF1 0.6902, val mF1 0.6648
Time: 0:09:02.225721
Epoch 29/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 29/80: train loss 0.2309, val loss 0.2693
Epoch 29/80: train mF1 0.6944, val mF1 0.6604
Time: 0:08:58.913710
Epoch 30/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 30/80: train loss 0.2287, val loss 0.2772
Epoch 30/80: train mF1 0.6988, val mF1 0.6539
Time: 0:09:01.870413
Epoch 31/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 31/80: train loss 0.2272, val loss 0.2660
Epoch 31/80: train mF1 0.7025, val mF1 0.6715
Time: 0:08:57.566317
Epoch 32/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 32/80: train loss 0.2265, val loss 0.2828
Epoch 32/80: train mF1 0.7047, val mF1 0.6511
Time: 0:08:55.907203
Epoch 33/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 33/80: train loss 0.2228, val loss 0.2704
Epoch 33/80: train mF1 0.7089, val mF1 0.6698
Time: 0:09:07.737843
Epoch 34/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 34/80: train loss 0.2206, val loss 0.2583
Epoch 34/80: train mF1 0.7128, val mF1 0.6790
Time: 0:08:55.470882
Epoch 35/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 35/80: train loss 0.2192, val loss 0.2862
Epoch 35/80: train mF1 0.7139, val mF1 0.6475
Time: 0:08:53.595054
Epoch 36/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 36/80: train loss 0.2175, val loss 0.2833
Epoch 36/80: train mF1 0.7159, val mF1 0.6560
Time: 0:09:01.129394
Epoch 37/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 37/80: train loss 0.2146, val loss 0.2720
Epoch 37/80: train mF1 0.7221, val mF1 0.6637
Time: 0:08:56.266588
Epoch 38/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 38/80: train loss 0.2141, val loss 0.2704
Epoch 38/80: train mF1 0.7212, val mF1 0.6739
Time: 0:08:58.052142
Epoch 39/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 39/80: train loss 0.2113, val loss 0.2579
Epoch 39/80: train mF1 0.7264, val mF1 0.6921
Time: 0:08:54.604095
Epoch 40/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 40/80: train loss 0.2103, val loss 0.2583
Epoch 40/80: train mF1 0.7296, val mF1 0.6905
Time: 0:08:57.109267
Epoch 41/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 41/80: train loss 0.2079, val loss 0.2838
Epoch 41/80: train mF1 0.7337, val mF1 0.6721
Time: 0:09:01.043023
Epoch 42/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 42/80: train loss 0.2054, val loss 0.2815
Epoch 42/80: train mF1 0.7380, val mF1 0.6639
Time: 0:09:01.364292
Epoch 43/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 43/80: train loss 0.2028, val loss 0.2803
Epoch 43/80: train mF1 0.7420, val mF1 0.6759
Time: 0:09:01.714768
Epoch 44/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 44/80: train loss 0.2019, val loss 0.2827
Epoch 44/80: train mF1 0.7422, val mF1 0.6678
Time: 0:09:01.593786
Epoch 45/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 45/80: train loss 0.1996, val loss 0.2690
Epoch 45/80: train mF1 0.7455, val mF1 0.6815
Time: 0:09:02.790133
Epoch 46/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 46/80: train loss 0.1971, val loss 0.2673
Epoch 46/80: train mF1 0.7499, val mF1 0.6968
Time: 0:09:03.150133
Epoch 47/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 47/80: train loss 0.1937, val loss 0.3305
Epoch 47/80: train mF1 0.7552, val mF1 0.6375
Time: 0:09:07.736140
Epoch 48/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 48/80: train loss 0.1926, val loss 0.2948
Epoch 48/80: train mF1 0.7569, val mF1 0.6609
Time: 0:09:08.663044
Epoch 49/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 49/80: train loss 0.1920, val loss 0.2901
Epoch 49/80: train mF1 0.7591, val mF1 0.6660
Time: 0:08:59.942769
Epoch 50/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 50/80: train loss 0.1903, val loss 0.2806
Epoch 50/80: train mF1 0.7618, val mF1 0.6730
Time: 0:08:49.223777
Epoch 51/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 51/80: train loss 0.1882, val loss 0.2922
Epoch 51/80: train mF1 0.7617, val mF1 0.6731
Time: 0:09:09.899712
Epoch 52/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 52/80: train loss 0.1874, val loss 0.2681
Epoch 52/80: train mF1 0.7643, val mF1 0.6893
Time: 0:08:59.942584
Epoch 53/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 53/80: train loss 0.1847, val loss 0.2716
Epoch 53/80: train mF1 0.7703, val mF1 0.6896
Time: 0:09:05.771204
Epoch 54/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 54/80: train loss 0.1837, val loss 0.2999
Epoch 54/80: train mF1 0.7710, val mF1 0.6844
Time: 0:08:57.730611
Epoch 55/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 55/80: train loss 0.1803, val loss 0.2757
Epoch 55/80: train mF1 0.7756, val mF1 0.6873
Time: 0:09:24.060571
Epoch 56/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 56/80: train loss 0.1787, val loss 0.2766
Epoch 56/80: train mF1 0.7792, val mF1 0.6891
Time: 0:09:19.158772
Epoch 57/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 57/80: train loss 0.1764, val loss 0.2802
Epoch 57/80: train mF1 0.7835, val mF1 0.6920
Time: 0:09:00.302507
Epoch 58/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 58/80: train loss 0.1752, val loss 0.2770
Epoch 58/80: train mF1 0.7846, val mF1 0.6923
Time: 0:08:57.797945
Epoch 59/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 59/80: train loss 0.1735, val loss 0.2793
Epoch 59/80: train mF1 0.7859, val mF1 0.6889
Time: 0:09:13.207904
Epoch 60/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 60/80: train loss 0.1725, val loss 0.2839
Epoch 60/80: train mF1 0.7886, val mF1 0.6790
Time: 0:09:04.831674
Epoch 61/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 61/80: train loss 0.1705, val loss 0.2945
Epoch 61/80: train mF1 0.7913, val mF1 0.6842
Time: 0:09:05.404506
Epoch 62/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 62/80: train loss 0.1689, val loss 0.3191
Epoch 62/80: train mF1 0.7929, val mF1 0.6577
Time: 0:09:08.446424
Epoch 63/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 63/80: train loss 0.1670, val loss 0.2869
Epoch 63/80: train mF1 0.7953, val mF1 0.6923
Time: 0:09:09.816753
Epoch 64/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 64/80: train loss 0.1652, val loss 0.3128
Epoch 64/80: train mF1 0.7986, val mF1 0.6667
Time: 0:09:07.977964
Epoch 65/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 65/80: train loss 0.1626, val loss 0.2895
Epoch 65/80: train mF1 0.8030, val mF1 0.6898
Time: 0:09:06.973602
Epoch 66/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 66/80: train loss 0.1620, val loss 0.3033
Epoch 66/80: train mF1 0.8032, val mF1 0.6805
Time: 0:09:12.829675
Epoch 67/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 67/80: train loss 0.1602, val loss 0.2996
Epoch 67/80: train mF1 0.8060, val mF1 0.6912
Time: 0:09:09.410404
Epoch 68/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 68/80: train loss 0.1587, val loss 0.2959
Epoch 68/80: train mF1 0.8085, val mF1 0.6858
Time: 0:09:12.520750
Epoch 69/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 69/80: train loss 0.1568, val loss 0.3012
Epoch 69/80: train mF1 0.8111, val mF1 0.6838
Time: 0:09:06.140507
Epoch 70/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 70/80: train loss 0.1539, val loss 0.3233
Epoch 70/80: train mF1 0.8147, val mF1 0.6759
Time: 0:09:13.947302
Epoch 71/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 71/80: train loss 0.1541, val loss 0.2873
Epoch 71/80: train mF1 0.8141, val mF1 0.6921
Time: 0:09:09.315319
Epoch 72/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 72/80: train loss 0.1503, val loss 0.3102
Epoch 72/80: train mF1 0.8209, val mF1 0.6761
Time: 0:09:06.994845
Epoch 73/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 73/80: train loss 0.1507, val loss 0.2954
Epoch 73/80: train mF1 0.8206, val mF1 0.6936
Time: 0:09:06.545591
Epoch 74/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 74/80: train loss 0.1495, val loss 0.3165
Epoch 74/80: train mF1 0.8212, val mF1 0.6789
Time: 0:09:08.651731
Epoch 75/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 75/80: train loss 0.1469, val loss 0.3165
Epoch 75/80: train mF1 0.8248, val mF1 0.6809
Time: 0:09:08.873776
Epoch 76/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 76/80: train loss 0.1449, val loss 0.3502
Epoch 76/80: train mF1 0.8292, val mF1 0.6501
Time: 0:09:06.941096
Epoch 77/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 77/80: train loss 0.1457, val loss 0.3241
Epoch 77/80: train mF1 0.8279, val mF1 0.6764
Time: 0:09:02.020544
Epoch 78/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 78/80: train loss 0.1414, val loss 0.3136
Epoch 78/80: train mF1 0.8324, val mF1 0.6844
Time: 0:09:02.916025
Epoch 79/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 79/80: train loss 0.1409, val loss 0.3463
Epoch 79/80: train mF1 0.8357, val mF1 0.6632
Time: 0:09:04.363850
Epoch 80/80
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 80/80: train loss 0.1399, val loss 0.3128
Epoch 80/80: train mF1 0.8363, val mF1 0.6855
/home/bertille/.local/lib/python3.8/site-packages/pydantic/main.py:347: UserWarning: Pydantic serializer warnings:
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Time: 0:08:55.681279
Testing
Batch: 0  over  10
Test F1 by class: [0.57452891 0.58837468 0.78241462 0.76253041 0.69240469 0.56267864
 0.44774715]
Test mF1: 0.6300970174497469
Plot saved at: ../../results/resnet18_64_l1/random_shuffling/all/resnet18_multi_label_64_random_80epochs_bs4096_augmented_pre_trained/metrics_test/test_preds.png
Reassembling the patches...
f1_by_class:  [0.46681255 0.16410256 0.02414487 0.         0.0060423  0.
 0.        ]
post_f1_by_class [0.61249161 0.06525912 0.02691337 0.         0.00638978 0.
 0.        ]
Reassembling the patches...
Reassembling the patches...
f1_by_class:  [0.14173228 0.14364641 0.52731591 0.         0.01680672 0.
 0.        ]
post_f1_by_class [0.         0.12429379 0.55230126 0.         0.01680672 0.
 0.        ]
Reassembling the patches...
Reassembling the patches...
f1_by_class:  [0.26815642 0.16666667 0.79409698 0.         0.         0.
 0.        ]
post_f1_by_class [0.32       0.08547009 0.79438059 0.         0.         0.
 0.        ]
Reassembling the patches...
