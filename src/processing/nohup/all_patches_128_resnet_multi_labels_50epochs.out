----------------------- UNet -----------------------
Patch size: 128
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: all
Patches: all
Batch size: 64
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Image shape: torch.Size([64, 4, 128, 128]), Mask shape: torch.Size([64, 7])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask unique values: [0. 1.], dtype: torch.float32
Mask:  tensor([0., 1., 0., 0., 0., 0., 1.])
Train: 25576 images, Val: 8526 images, Test: 8526 images
Train: 60.00%, Val: 20.00%, Test: 20.00%
Creating model...
Model settings:
Pretrained: False
Classes: 6
The model is Resnet18
Using BCEWithDigits criterion
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: BCEWithDigits
Optimizer: Adam
Training...
Epoch 1/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 1/50: train loss 0.3749, val loss 0.3824
Epoch 1/50: train mF1 0.3570, val mF1 0.4048
Time: 0:02:13.694819
Epoch 2/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 2/50: train loss 0.3376, val loss 0.3887
Epoch 2/50: train mF1 0.4633, val mF1 0.3799
Time: 0:02:12.382515
Epoch 3/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 3/50: train loss 0.3181, val loss 0.3953
Epoch 3/50: train mF1 0.5209, val mF1 0.4393
Time: 0:02:14.036608
Epoch 4/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 4/50: train loss 0.3045, val loss 0.3598
Epoch 4/50: train mF1 0.5597, val mF1 0.4838
Time: 0:02:13.034507
Epoch 5/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 5/50: train loss 0.2892, val loss 0.3302
Epoch 5/50: train mF1 0.5979, val mF1 0.5102
Time: 0:02:12.603228
Epoch 6/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 6/50: train loss 0.2801, val loss 0.3067
Epoch 6/50: train mF1 0.6181, val mF1 0.5902
Time: 0:02:12.956239
Epoch 7/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 7/50: train loss 0.2703, val loss 0.3298
Epoch 7/50: train mF1 0.6367, val mF1 0.5305
Time: 0:02:12.467962
Epoch 8/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 8/50: train loss 0.2592, val loss 0.4567
Epoch 8/50: train mF1 0.6611, val mF1 0.3945
Time: 0:02:11.784821
Epoch 9/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 9/50: train loss 0.2484, val loss 0.2965
Epoch 9/50: train mF1 0.6802, val mF1 0.6073
Time: 0:02:12.607629
Epoch 10/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 10/50: train loss 0.2351, val loss 0.3804
Epoch 10/50: train mF1 0.7031, val mF1 0.5083
Time: 0:02:13.198121
Epoch 11/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 11/50: train loss 0.2158, val loss 0.3499
Epoch 11/50: train mF1 0.7318, val mF1 0.5839
Time: 0:02:10.427947
Epoch 12/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 12/50: train loss 0.1960, val loss 0.3435
Epoch 12/50: train mF1 0.7642, val mF1 0.6626
Time: 0:02:12.092838
Epoch 13/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 13/50: train loss 0.1662, val loss 0.3380
Epoch 13/50: train mF1 0.8071, val mF1 0.6313
Time: 0:02:10.599085
Epoch 14/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 14/50: train loss 0.1312, val loss 0.3877
Epoch 14/50: train mF1 0.8558, val mF1 0.6232
Time: 0:02:10.981616
Epoch 15/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 15/50: train loss 0.0973, val loss 0.4084
Epoch 15/50: train mF1 0.8995, val mF1 0.6584
Time: 0:02:10.937572
Epoch 16/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 16/50: train loss 0.0703, val loss 0.4922
Epoch 16/50: train mF1 0.9307, val mF1 0.6297
Time: 0:02:10.959102
Epoch 17/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 17/50: train loss 0.0504, val loss 0.6119
Epoch 17/50: train mF1 0.9515, val mF1 0.6068
Time: 0:02:10.262603
Epoch 18/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 18/50: train loss 0.0415, val loss 0.5581
Epoch 18/50: train mF1 0.9613, val mF1 0.6308
Time: 0:02:10.717947
Epoch 19/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 19/50: train loss 0.0314, val loss 0.5783
Epoch 19/50: train mF1 0.9707, val mF1 0.6438
Time: 0:02:10.804742
Epoch 20/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 20/50: train loss 0.0276, val loss 0.6822
Epoch 20/50: train mF1 0.9750, val mF1 0.6200
Time: 0:02:10.608491
Epoch 21/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 21/50: train loss 0.0283, val loss 0.7699
Epoch 21/50: train mF1 0.9737, val mF1 0.6034
Time: 0:02:10.781387
Epoch 22/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 22/50: train loss 0.0233, val loss 0.6996
Epoch 22/50: train mF1 0.9782, val mF1 0.6352
Time: 0:02:09.929940
Epoch 23/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 23/50: train loss 0.0177, val loss 0.7120
Epoch 23/50: train mF1 0.9851, val mF1 0.6369
Time: 0:02:10.628552
Epoch 24/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 24/50: train loss 0.0241, val loss 0.7411
Epoch 24/50: train mF1 0.9776, val mF1 0.5988
Time: 0:02:10.551791
Epoch 25/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 25/50: train loss 0.0187, val loss 0.8198
Epoch 25/50: train mF1 0.9839, val mF1 0.5940
Time: 0:02:09.381219
Epoch 26/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 26/50: train loss 0.0183, val loss 0.6985
Epoch 26/50: train mF1 0.9839, val mF1 0.6441
Time: 0:02:10.635255
Epoch 27/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 27/50: train loss 0.0160, val loss 0.8407
Epoch 27/50: train mF1 0.9856, val mF1 0.6110
Time: 0:02:10.833710
Epoch 28/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 28/50: train loss 0.0173, val loss 1.0193
Epoch 28/50: train mF1 0.9843, val mF1 0.5055
Time: 0:02:10.602102
Epoch 29/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 29/50: train loss 0.0173, val loss 0.7434
Epoch 29/50: train mF1 0.9837, val mF1 0.6271
Time: 0:02:10.469399
Epoch 30/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 30/50: train loss 0.0148, val loss 0.7165
Epoch 30/50: train mF1 0.9872, val mF1 0.6563
Time: 0:02:11.169166
Epoch 31/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 31/50: train loss 0.0141, val loss 0.7478
Epoch 31/50: train mF1 0.9878, val mF1 0.6364
Time: 0:02:11.584867
Epoch 32/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 32/50: train loss 0.0125, val loss 0.8516
Epoch 32/50: train mF1 0.9900, val mF1 0.6094
Time: 0:02:10.759652
Epoch 33/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 33/50: train loss 0.0141, val loss 0.7619
Epoch 33/50: train mF1 0.9873, val mF1 0.6353
Time: 0:02:11.448666
Epoch 34/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 34/50: train loss 0.0125, val loss 0.7700
Epoch 34/50: train mF1 0.9893, val mF1 0.6437
Time: 0:02:10.887524
Epoch 35/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 35/50: train loss 0.0145, val loss 0.8176
Epoch 35/50: train mF1 0.9871, val mF1 0.6239
Time: 0:02:10.826499
Epoch 36/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 36/50: train loss 0.0077, val loss 0.8301
Epoch 36/50: train mF1 0.9934, val mF1 0.6473
Time: 0:02:10.523165
Epoch 37/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 37/50: train loss 0.0115, val loss 0.7711
Epoch 37/50: train mF1 0.9896, val mF1 0.6303
Time: 0:02:10.016081
Epoch 38/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 38/50: train loss 0.0106, val loss 0.7792
Epoch 38/50: train mF1 0.9914, val mF1 0.6470
Time: 0:02:10.320866
Epoch 39/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 39/50: train loss 0.0091, val loss 0.9908
Epoch 39/50: train mF1 0.9924, val mF1 0.6149
Time: 0:02:11.008444
Epoch 40/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 40/50: train loss 0.0132, val loss 0.8164
Epoch 40/50: train mF1 0.9891, val mF1 0.6209
Time: 0:02:11.688424
Epoch 41/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 41/50: train loss 0.0101, val loss 0.7797
Epoch 41/50: train mF1 0.9912, val mF1 0.6260
Time: 0:02:13.641693
Epoch 42/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 42/50: train loss 0.0091, val loss 0.8119
Epoch 42/50: train mF1 0.9915, val mF1 0.6449
Time: 0:02:11.745317
Epoch 43/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 43/50: train loss 0.0081, val loss 0.8335
Epoch 43/50: train mF1 0.9928, val mF1 0.6111
Time: 0:02:11.054295
Epoch 44/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 44/50: train loss 0.0095, val loss 0.8410
Epoch 44/50: train mF1 0.9917, val mF1 0.6208
Time: 0:02:12.030291
Epoch 45/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 45/50: train loss 0.0117, val loss 0.8302
Epoch 45/50: train mF1 0.9890, val mF1 0.6337
Time: 0:02:11.402811
Epoch 46/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 46/50: train loss 0.0085, val loss 0.8728
Epoch 46/50: train mF1 0.9926, val mF1 0.6377
Time: 0:02:10.983130
Epoch 47/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 47/50: train loss 0.0078, val loss 0.9468
Epoch 47/50: train mF1 0.9937, val mF1 0.6292
Time: 0:02:10.778804
Epoch 48/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 48/50: train loss 0.0068, val loss 0.8681
Epoch 48/50: train mF1 0.9943, val mF1 0.6372
Time: 0:02:11.483975
Epoch 49/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 49/50: train loss 0.0104, val loss 0.8637
Epoch 49/50: train mF1 0.9909, val mF1 0.6225
Time: 0:02:10.845330
Epoch 50/50
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 50/50: train loss 0.0095, val loss 0.8535
Epoch 50/50: train mF1 0.9916, val mF1 0.6328
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Time: 0:02:10.625511
Testing
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Test F1 by class: [0.37860465 0.46898638 0.74510542 0.73953488 0.60449438 0.48477467
 0.84637728]
Test mF1: 0.609696809604363
Confusion matrix:  [[[6783  255]
  [1081  407]]

 [[7008   76]
  [ 977  465]]

 [[5193  568]
  [ 786 1979]]

 [[6900  221]
  [ 451  954]]

 [[7284  306]
  [ 398  538]]

 [[7904  132]
  [ 291  199]]

 [[1377 1153]
  [ 751 5245]]]
Normalized confusion matrix:  [array([[0.96376812, 0.03623188],
       [0.72647849, 0.27352151]]), array([[0.9892716 , 0.0107284 ],
       [0.67753121, 0.32246879]]), array([[0.90140601, 0.09859399],
       [0.28426763, 0.71573237]]), array([[0.96896503, 0.03103497],
       [0.32099644, 0.67900356]]), array([[0.95968379, 0.04031621],
       [0.42521368, 0.57478632]]), array([[0.98357392, 0.01642608],
       [0.59387755, 0.40612245]]), array([[0.54426877, 0.45573123],
       [0.12525017, 0.87474983]])]
msk_ [0. 0. 0. 0. 1. 0.]
msk_ [4]
msk_ [0. 0. 0. 1. 0. 0.]
msk_ [3]
msk_ [0. 0. 0. 1. 0. 0.]
msk_ [3]
msk_ [1. 0. 0. 0. 0. 0.]
msk_ [0]
msk_ [0. 0. 0. 0. 1. 0.]
msk_ [4]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
msk_ [0. 0. 0. 0. 1. 0.]
msk_ [4]
msk_ [0. 0. 0. 0. 1. 0.]
msk_ [4]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
msk_ [0. 0. 1. 0. 0. 0.]
msk_ [2]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
msk_ [0. 0. 1. 0. 0. 0.]
msk_ [2]
msk_ [0. 0. 1. 0. 0. 0.]
msk_ [2]
msk_ [0. 0. 1. 0. 0. 0.]
msk_ [2]
msk_ [0. 1. 0. 0. 0. 0.]
msk_ [1]
Plot saved at: ../../results/resnet18_128_l1/random_shuffling/resnet18_random_all_patches_128_multi_label_50epochs/metrics_test/test_preds.png
