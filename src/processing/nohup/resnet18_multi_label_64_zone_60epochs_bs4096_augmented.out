INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations
----------------------- UNet -----------------------
Patch size: 64
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.68, 0.2]
Stratified: zone
Year: all
Patches: all
Batch size: 4096
Normalisation: channel_by_channel
Data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Nbumber of unique zones: 108
Train, val and test zones saved in csv file at: ../../results/resnet18_64_l1/stratified_shuffling_by_zone/seed1/img_ids_by_set.csv
Image: min: 0.0, max: 1.0
Mask unique values: [0. 1.]
Train: 102118 images, Val: 36030 images, Test: 32320 images
Train: 59.90%, Val: 21.14%, Test: 18.96%
Creating model...
Model settings:
Pretrained: False
Classes: 7
The model is Resnet18
Using BCEWithDigits criterion
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: BCEWithDigits
Optimizer: Adam
Training...
Epoch 1/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 1/60: train loss 0.4372, val loss 0.5200
Epoch 1/60: train mF1 0.2859, val mF1 0.1571
Time: 0:08:43.341794
Epoch 2/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 2/60: train loss 0.3901, val loss 0.4877
Epoch 2/60: train mF1 0.3833, val mF1 0.2364
Time: 0:08:40.035125
Epoch 3/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 3/60: train loss 0.3760, val loss 0.4570
Epoch 3/60: train mF1 0.4176, val mF1 0.2853
Time: 0:08:42.338436
Epoch 4/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 4/60: train loss 0.3641, val loss 0.4835
Epoch 4/60: train mF1 0.4500, val mF1 0.2384
Time: 0:08:42.545544
Epoch 5/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 5/60: train loss 0.3547, val loss 0.5176
Epoch 5/60: train mF1 0.4751, val mF1 0.2544
Time: 0:08:42.000665
Epoch 6/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 6/60: train loss 0.3499, val loss 0.4712
Epoch 6/60: train mF1 0.4896, val mF1 0.3057
Time: 0:08:47.795792
Epoch 7/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 7/60: train loss 0.3449, val loss 0.4719
Epoch 7/60: train mF1 0.5020, val mF1 0.2998
Time: 0:08:51.896650
Epoch 8/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 8/60: train loss 0.3391, val loss 0.4498
Epoch 8/60: train mF1 0.5139, val mF1 0.3352
Time: 0:08:55.509946
Epoch 9/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 9/60: train loss 0.3338, val loss 0.4944
Epoch 9/60: train mF1 0.5233, val mF1 0.3086
Time: 0:08:56.348891
Epoch 10/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 10/60: train loss 0.3299, val loss 0.5094
Epoch 10/60: train mF1 0.5321, val mF1 0.3099
Time: 0:08:57.386738
Epoch 11/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 11/60: train loss 0.3277, val loss 0.4997
Epoch 11/60: train mF1 0.5389, val mF1 0.3044
Time: 0:08:58.900063
Epoch 12/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 12/60: train loss 0.3223, val loss 0.4668
Epoch 12/60: train mF1 0.5462, val mF1 0.3041
Time: 0:08:53.073311
Epoch 13/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 13/60: train loss 0.3179, val loss 0.4479
Epoch 13/60: train mF1 0.5560, val mF1 0.3212
Time: 0:08:51.781114
Epoch 14/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 14/60: train loss 0.3157, val loss 0.4497
Epoch 14/60: train mF1 0.5604, val mF1 0.3431
Time: 0:09:08.610535
Epoch 15/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 15/60: train loss 0.3114, val loss 0.4115
Epoch 15/60: train mF1 0.5677, val mF1 0.3339
Time: 0:08:55.120972
Epoch 16/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 16/60: train loss 0.3092, val loss 0.4904
Epoch 16/60: train mF1 0.5715, val mF1 0.3311
Time: 0:08:53.843599
Epoch 17/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 17/60: train loss 0.3058, val loss 0.4255
Epoch 17/60: train mF1 0.5770, val mF1 0.3291
Time: 0:09:10.465379
Epoch 18/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 18/60: train loss 0.3021, val loss 0.4492
Epoch 18/60: train mF1 0.5845, val mF1 0.3332
Time: 0:09:05.022438
Epoch 19/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 19/60: train loss 0.3004, val loss 0.4776
Epoch 19/60: train mF1 0.5890, val mF1 0.3427
Time: 0:08:56.567712
Epoch 20/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 20/60: train loss 0.2975, val loss 0.4625
Epoch 20/60: train mF1 0.5927, val mF1 0.3392
Time: 0:09:00.659757
Epoch 21/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 21/60: train loss 0.2955, val loss 0.4426
Epoch 21/60: train mF1 0.5977, val mF1 0.3411
Time: 0:09:01.694745
Epoch 22/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 22/60: train loss 0.2935, val loss 0.4575
Epoch 22/60: train mF1 0.5999, val mF1 0.3192
Time: 0:09:02.080856
Epoch 23/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 23/60: train loss 0.2920, val loss 0.6083
Epoch 23/60: train mF1 0.6025, val mF1 0.3027
Time: 0:09:01.221360
Epoch 24/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 24/60: train loss 0.2899, val loss 0.4343
Epoch 24/60: train mF1 0.6052, val mF1 0.3524
Time: 0:09:02.315840
Epoch 25/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 25/60: train loss 0.2884, val loss 0.4402
Epoch 25/60: train mF1 0.6119, val mF1 0.3243
Time: 0:09:01.166539
Epoch 26/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 26/60: train loss 0.2853, val loss 0.4898
Epoch 26/60: train mF1 0.6153, val mF1 0.3076
Time: 0:09:02.106138
Epoch 27/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 27/60: train loss 0.2839, val loss 0.5408
Epoch 27/60: train mF1 0.6191, val mF1 0.3149
Time: 0:09:01.672363
Epoch 28/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 28/60: train loss 0.2821, val loss 0.4752
Epoch 28/60: train mF1 0.6212, val mF1 0.3169
Time: 0:09:01.407680
Epoch 29/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 29/60: train loss 0.2805, val loss 0.4573
Epoch 29/60: train mF1 0.6216, val mF1 0.3237
Time: 0:09:01.908867
Epoch 30/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 30/60: train loss 0.2798, val loss 0.4668
Epoch 30/60: train mF1 0.6262, val mF1 0.3545
Time: 0:09:01.836864
Epoch 31/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 31/60: train loss 0.2779, val loss 0.5136
Epoch 31/60: train mF1 0.6288, val mF1 0.3252
Time: 0:09:01.335454
Epoch 32/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 32/60: train loss 0.2755, val loss 0.4810
Epoch 32/60: train mF1 0.6330, val mF1 0.3452
Time: 0:09:01.798547
Epoch 33/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 33/60: train loss 0.2727, val loss 0.4739
Epoch 33/60: train mF1 0.6380, val mF1 0.3316
Time: 0:09:01.913809
Epoch 34/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 34/60: train loss 0.2726, val loss 0.4541
Epoch 34/60: train mF1 0.6364, val mF1 0.3437
Time: 0:09:00.382991
Epoch 35/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 35/60: train loss 0.2705, val loss 0.4813
Epoch 35/60: train mF1 0.6411, val mF1 0.3227
Time: 0:09:02.095500
Epoch 36/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 36/60: train loss 0.2686, val loss 0.4993
Epoch 36/60: train mF1 0.6442, val mF1 0.3857
Time: 0:09:00.956953
Epoch 37/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 37/60: train loss 0.2670, val loss 0.4711
Epoch 37/60: train mF1 0.6463, val mF1 0.3625
Time: 0:09:01.494625
Epoch 38/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 38/60: train loss 0.2650, val loss 0.5543
Epoch 38/60: train mF1 0.6512, val mF1 0.2936
Time: 0:09:01.023727
Epoch 39/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 39/60: train loss 0.2649, val loss 0.4577
Epoch 39/60: train mF1 0.6502, val mF1 0.3065
Time: 0:09:00.488947
Epoch 40/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 40/60: train loss 0.2635, val loss 0.4756
Epoch 40/60: train mF1 0.6519, val mF1 0.3456
Time: 0:09:00.671816
Epoch 41/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 41/60: train loss 0.2616, val loss 0.4479
Epoch 41/60: train mF1 0.6557, val mF1 0.3368
Time: 0:09:01.244271
Epoch 42/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 42/60: train loss 0.2612, val loss 0.4802
Epoch 42/60: train mF1 0.6582, val mF1 0.3423
Time: 0:09:00.872536
Epoch 43/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 43/60: train loss 0.2590, val loss 0.5178
Epoch 43/60: train mF1 0.6602, val mF1 0.3379
Time: 0:08:59.387223
Epoch 44/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 44/60: train loss 0.2585, val loss 0.4814
Epoch 44/60: train mF1 0.6610, val mF1 0.3371
Time: 0:08:58.895378
Epoch 45/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 45/60: train loss 0.2580, val loss 0.5632
Epoch 45/60: train mF1 0.6623, val mF1 0.3388
Time: 0:08:59.425165
Epoch 46/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 46/60: train loss 0.2549, val loss 0.5062
Epoch 46/60: train mF1 0.6665, val mF1 0.3362
Time: 0:09:00.891850
Epoch 47/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 47/60: train loss 0.2536, val loss 0.4654
Epoch 47/60: train mF1 0.6696, val mF1 0.3586
Time: 0:09:02.593943
Epoch 48/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 48/60: train loss 0.2539, val loss 0.4774
Epoch 48/60: train mF1 0.6696, val mF1 0.3320
Time: 0:08:58.874908
Epoch 49/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 49/60: train loss 0.2525, val loss 0.4621
Epoch 49/60: train mF1 0.6719, val mF1 0.3653
Time: 0:09:01.685086
Epoch 50/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 50/60: train loss 0.2513, val loss 0.5725
Epoch 50/60: train mF1 0.6729, val mF1 0.3376
Time: 0:09:00.914393
Epoch 51/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 51/60: train loss 0.2498, val loss 0.5359
Epoch 51/60: train mF1 0.6759, val mF1 0.3166
Time: 0:09:00.828204
Epoch 52/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 52/60: train loss 0.2481, val loss 0.4630
Epoch 52/60: train mF1 0.6780, val mF1 0.3313
Time: 0:09:01.664958
Epoch 53/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 53/60: train loss 0.2472, val loss 0.4766
Epoch 53/60: train mF1 0.6813, val mF1 0.3456
Time: 0:09:00.549110
Epoch 54/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 54/60: train loss 0.2460, val loss 0.5153
Epoch 54/60: train mF1 0.6821, val mF1 0.3304
Time: 0:08:59.983590
Epoch 55/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 55/60: train loss 0.2441, val loss 0.5126
Epoch 55/60: train mF1 0.6865, val mF1 0.3379
Time: 0:09:01.296002
Epoch 56/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 56/60: train loss 0.2442, val loss 0.4866
Epoch 56/60: train mF1 0.6849, val mF1 0.3258
Time: 0:09:01.994054
Epoch 57/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 57/60: train loss 0.2421, val loss 0.6058
Epoch 57/60: train mF1 0.6883, val mF1 0.3121
Time: 0:09:01.063590
Epoch 58/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 58/60: train loss 0.2414, val loss 0.4777
Epoch 58/60: train mF1 0.6888, val mF1 0.3489
Time: 0:09:01.370224
Epoch 59/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 59/60: train loss 0.2398, val loss 0.5209
Epoch 59/60: train mF1 0.6919, val mF1 0.3574
Time: 0:09:13.210917
Epoch 60/60
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 60/60: train loss 0.2395, val loss 0.5102
Epoch 60/60: train mF1 0.6926, val mF1 0.3526
/home/bertille/.local/lib/python3.8/site-packages/pydantic/main.py:347: UserWarning: Pydantic serializer warnings:
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/bertille/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Time: 0:09:09.141181
Testing
Batch: 0  over  8
Test F1 by class: [0.43537022 0.1781367  0.65581484 0.34939969 0.57527686 0.09187279
 0.3546798 ]
Test mF1: 0.3772215578754124
Plot saved at: ../../results/resnet18_64_l1/stratified_shuffling_by_zone/all/resnet18_multi_label_64_zone_60epochs_bs4096_augmented/metrics_test/test_preds.png
Reassembling the patches...
Reassembling the patches...
Reassembling the patches...
