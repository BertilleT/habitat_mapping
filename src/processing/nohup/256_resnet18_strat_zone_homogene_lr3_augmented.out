----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.7, 0.2]
Stratified: zone
Year: all
Patches: homogeneous
Batch size: 16
Normalisation: channel_by_channel
Data augmentation
The seed to shuffle the data is  1
The data are from  all  year
5716 homogeneous masks found
['zone5' 'zone171' 'zone12' 'zone37' 'zone156' 'zone22' 'zone129' 'zone50'
 'zone69' 'zone143' 'zone95' 'zone14' 'zone34' 'zone112' 'zone53'
 'zone154' 'zone54' 'zone104' 'zone120' 'zone7' 'zone10' 'zone30'
 'zone123' 'zone98' 'zone67' 'zone21' 'zone33' 'zone158' 'zone148'
 'zone84' 'zone15' 'zone48' 'zone16' 'zone1' 'zone44' 'zone136' 'zone26'
 'zone24' 'zone71' 'zone56' 'zone38' 'zone25' 'zone155' 'zone121'
 'zone162' 'zone132' 'zone78' 'zone2' 'zone101' 'zone19' 'zone75' 'zone68'
 'zone39' 'zone160' 'zone96' 'zone115' 'zone17' 'zone139' 'zone114'
 'zone65' 'zone59' 'zone113' 'zone45' 'zone126' 'zone47' 'zone165'
 'zone170' 'zone66' 'zone161' 'zone142' 'zone144' 'zone63' 'zone76'
 'zone74' 'zone41' 'zone90' 'zone20' 'zone51' 'zone73' 'zone106' 'zone133'
 'zone3' 'zone77' 'zone172' 'zone88' 'zone147' 'zone159' 'zone164'
 'zone100' 'zone134' 'zone137' 'zone28' 'zone6' 'zone80' 'zone85' 'zone97'
 'zone167' 'zone27' 'zone116' 'zone102' 'zone145']
Nbumber of unique zones: 101
Train, val and test zones saved in csv file at: ../../results/resnet18_256_l1/stratified_shuffling_by_zone/seed1/img_ids_by_set.csv
Image shape: torch.Size([16, 4, 256, 256]), Mask shape: torch.Size([16])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask: [2], dtype: torch.uint8
Train: 3288 images, Val: 1123 images, Test: 1305 images
Train: 57.52%, Val: 19.65%, Test: 22.83%
Creating model...
Model settings:
Pretrained: False
Classes: 6
The model is Resnet18
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: CrossEntropy
Optimizer: Adam
Training...
Epoch 1/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 1/70: train loss 1.5070, val loss 1.9171
Epoch 1/70: train mF1 0.3044, val mF1 0.2738
Time: 0:01:01.120771
Epoch 2/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 2/70: train loss 1.3108, val loss 1.8375
Epoch 2/70: train mF1 0.3756, val mF1 0.2428
Time: 0:01:02.764354
Epoch 3/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 3/70: train loss 1.2613, val loss 1.4576
Epoch 3/70: train mF1 0.3920, val mF1 0.3184
Time: 0:01:03.854588
Epoch 4/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 4/70: train loss 1.1957, val loss 1.7248
Epoch 4/70: train mF1 0.4333, val mF1 0.2879
Time: 0:01:00.067539
Epoch 5/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 5/70: train loss 1.1778, val loss 1.8575
Epoch 5/70: train mF1 0.4327, val mF1 0.2922
Time: 0:01:03.352090
Epoch 6/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 6/70: train loss 1.1120, val loss 1.2786
Epoch 6/70: train mF1 0.4826, val mF1 0.3600
Time: 0:01:02.301504
Epoch 7/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 7/70: train loss 1.0958, val loss 1.5316
Epoch 7/70: train mF1 0.4684, val mF1 0.3837
Time: 0:00:59.747228
Epoch 8/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 8/70: train loss 1.0778, val loss 1.7174
Epoch 8/70: train mF1 0.4757, val mF1 0.3368
Time: 0:01:03.079353
Epoch 9/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 9/70: train loss 1.0389, val loss 1.7545
Epoch 9/70: train mF1 0.5073, val mF1 0.3268
Time: 0:01:01.267871
Epoch 10/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 10/70: train loss 1.0359, val loss 1.4239
Epoch 10/70: train mF1 0.5069, val mF1 0.3730
Time: 0:01:00.360097
Epoch 11/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 11/70: train loss 1.0178, val loss 1.3969
Epoch 11/70: train mF1 0.5284, val mF1 0.3481
Time: 0:01:03.320398
Epoch 12/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 12/70: train loss 1.0085, val loss 1.5486
Epoch 12/70: train mF1 0.5276, val mF1 0.3077
Time: 0:01:01.861989
Epoch 13/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 13/70: train loss 0.9919, val loss 1.4056
Epoch 13/70: train mF1 0.5451, val mF1 0.3592
Time: 0:00:59.906369
Epoch 14/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 14/70: train loss 0.9777, val loss 1.5873
Epoch 14/70: train mF1 0.5496, val mF1 0.3551
Time: 0:01:02.924505
Epoch 15/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 15/70: train loss 0.9409, val loss 1.6268
Epoch 15/70: train mF1 0.5656, val mF1 0.3271
Time: 0:01:00.648602
Epoch 16/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 16/70: train loss 0.9382, val loss 1.5750
Epoch 16/70: train mF1 0.5664, val mF1 0.3441
Time: 0:01:00.732358
Epoch 17/70
Training
Batch: 0  over  206
Batch: 50  over  206
Batch: 100  over  206
Batch: 150  over  206
Batch: 200  over  206
Validation
Batch: 0  over  71
Batch: 50  over  71
Epoch 17/70: train loss 0.9113, val loss 1.7718
Epoch 17/70: train mF1 0.5731, val mF1 0.2900
