----------------------- UNet -----------------------
Patch size: 64
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: all
Patches: all
Batch size: 256
Normalisation: channel_by_channel
Data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Image: min: 0.0, max: 1.0
Mask unique values: [0. 1.]
Train: 99564 images, Val: 33188 images, Test: 37716 images
Train: 58.41%, Val: 19.47%, Test: 22.12%
Creating model...
Model settings:
Pretrained: False
Classes: 7
The model is Resnet18
Using BCEWithDigits criterion
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: BCEWithDigits
Optimizer: Adam
Training...
Epoch 1/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 1/10: train loss 0.4071, val loss 0.4354
Epoch 1/10: train mF1 0.3165, val mF1 0.3007
Time: 0:08:31.006018
Epoch 2/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 2/10: train loss 0.3756, val loss 0.3821
Epoch 2/10: train mF1 0.3922, val mF1 0.4056
Time: 0:08:34.521086
Epoch 3/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 3/10: train loss 0.3595, val loss 0.3788
Epoch 3/10: train mF1 0.4363, val mF1 0.4029
Time: 0:08:34.645948
Epoch 4/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 4/10: train loss 0.3485, val loss 0.4970
Epoch 4/10: train mF1 0.4644, val mF1 0.3714
Time: 0:08:33.282314
Epoch 5/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 5/10: train loss 0.3404, val loss 0.3711
Epoch 5/10: train mF1 0.4823, val mF1 0.4644
Time: 0:08:33.737517
Epoch 6/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 6/10: train loss 0.3341, val loss 0.3467
Epoch 6/10: train mF1 0.4953, val mF1 0.5014
Time: 0:08:33.469544
Epoch 7/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 7/10: train loss 0.3275, val loss 0.3466
Epoch 7/10: train mF1 0.5084, val mF1 0.4683
Time: 0:08:43.948749
Epoch 8/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 8/10: train loss 0.3236, val loss 0.3315
Epoch 8/10: train mF1 0.5202, val mF1 0.5255
Time: 0:08:58.033438
Epoch 9/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 9/10: train loss 0.3195, val loss 0.3686
Epoch 9/10: train mF1 0.5260, val mF1 0.4688
Time: 0:08:31.462108
Epoch 10/10
Training
Batch: 0  over  389
Batch: 50  over  389
Batch: 100  over  389
Batch: 150  over  389
Batch: 200  over  389
Batch: 250  over  389
Batch: 300  over  389
Batch: 350  over  389
Validation
Batch: 0  over  130
Batch: 50  over  130
Batch: 100  over  130
Epoch 10/10: train loss 0.3155, val loss 0.3460
Epoch 10/10: train mF1 0.5314, val mF1 0.5035
/home/bertille/.local/lib/python3.8/site-packages/pydantic/main.py:347: UserWarning: Pydantic serializer warnings:
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Time: 0:08:23.288096
Testing
Batch: 0  over  148
Batch: 50  over  148
Batch: 100  over  148
Test F1 by class: [0.46992571 0.27837218 0.73816866 0.67888663 0.62366912 0.3914911
 0.33546015]
Test mF1: 0.5022819359192682
Plot saved at: ../../results/resnet18_64_l1/random_shuffling/all/resnet18_multi_label_64_random_10epochs_bs256_augmented/metrics_test/test_preds.png
Reassembling the patches...
Reassembling the patches...
Reassembling the patches...
