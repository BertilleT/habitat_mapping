INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations
----------------------- UNet -----------------------
Patch size: 64
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: all
Patches: all
Batch size: 4096
Normalisation: channel_by_channel
Data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Image: min: 0.0, max: 1.0
Mask unique values: [0. 1.]
Train: 99564 images, Val: 33188 images, Test: 37716 images
Train: 58.41%, Val: 19.47%, Test: 22.12%
Creating model...
Model settings:
Pretrained: False
Classes: 7
The model is Resnet18
Model from last epoch 53  loaded
Using BCEWithDigits criterion
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: BCEWithDigits
Optimizer: Adam
Training...
training_losses:  [0.4470589196681976, 0.3980597603321075, 0.3841429817676544, 0.3748177027702331, 0.3654228925704956, 0.3588593912124633, 0.3523545026779174, 0.3480574977397919, 0.3407792401313781, 0.3373096454143524, 0.3333978390693664, 0.3319549059867859, 0.325930825471878, 0.3235379719734192, 0.3202709507942199, 0.3184054124355316, 0.3149509513378143, 0.3127314686775208, 0.3102505469322205, 0.3068921625614166, 0.3053452348709106, 0.3028912150859832, 0.3007653033733368, 0.3001762127876282, 0.2974733674526215, 0.2949596965312958, 0.2931261575222015, 0.2927231001853942, 0.2909492349624634, 0.2875990772247314, 0.2867790269851684, 0.2874197816848755, 0.2850562858581543, 0.2824131715297699, 0.2820909166336059, 0.2803661549091339, 0.2777603816986084, 0.2771618390083313, 0.2756534051895141, 0.2747161161899566, 0.2729135227203369, 0.2727839970588684, 0.2699131143093109, 0.268495489358902, 0.2695782148838043, 0.2674441444873809, 0.267746512889862, 0.2657720959186554, 0.2641206085681915, 0.2650785756111145, 0.2613371610641479, 0.25950728058815, 0.2598965305089951, 0.2583120304346085, 0.256333019733429, 0.2562929910421371, 0.2555204325914383, 0.2540624463558197, 0.2528120470046997, 0.252021899819374]
validation_losses:  [0.4605716963609059, 0.3842363688680861, 0.3840892877843644, 0.3929263154665629, 0.3566484914885627, 0.3830955359670851, 0.3580429818895128, 0.3513185348775651, 0.4003470407591926, 0.3349932101037767, 0.3236231936348809, 0.3284903797838423, 0.3779225216971503, 0.3524847726027171, 0.3503036664591895, 0.3261011143525441, 0.3380348748630947, 0.3430840406152937, 0.2979519797696007, 0.3115418818261888, 0.3108838333023919, 0.3090241187148624, 0.3175466126865811, 0.3080760108100043, 0.3191698955165015, 0.3277851674291823, 0.3090164992544386, 0.3139991694026523, 0.2936705019738939, 0.3061067561308543, 0.3171457184685601, 0.2974800864855448, 0.3011389606528812, 0.2993686033619774, 0.2908475465244717, 0.2898684210247463, 0.3066234721077813, 0.2874523500601451, 0.2923984196450975, 0.2814430395762126, 0.3045523762702942, 0.2966908050907982, 0.3131877382596333, 0.2930650115013122, 0.3372315002812279, 0.3037192689047919, 0.2753324806690216, 0.2808154424031575, 0.3064415819115109, 0.2698439723915524, 0.2773066129949357, 0.2824423180686103, 0.2721774876117706, 0.2744668457243178, 0.282553924454583, 0.2843591173489888, 0.2824202113681369, 0.2887153459919823, 0.277838690413369, 0.2873040239016215]
training_metric:  [0.2386745529955696, 0.3369610401768194, 0.3710052045281007, 0.3959902854314942, 0.4241078159108296, 0.4450834993988848, 0.4584563885230383, 0.4716666830638623, 0.489452645479341, 0.4986156458421681, 0.5074472475636205, 0.5089438427700743, 0.5231823489307141, 0.5294928846169473, 0.5350532482826835, 0.5397620687399243, 0.5449510411650583, 0.5532594012075627, 0.5551135540913938, 0.5608392782648862, 0.5657289442241202, 0.5698613480154722, 0.5739244860819819, 0.5756869326764404, 0.5814925421394215, 0.5848285548234681, 0.587346143182046, 0.5903982736705713, 0.5921105076749996, 0.5974185534944094, 0.6003159189040058, 0.5975533624985501, 0.6033011124925086, 0.6067880660833511, 0.6061258150878162, 0.6088497760400571, 0.6145034316401455, 0.615474686887416, 0.618804748595169, 0.6203842744816418, 0.6240290111161384, 0.6224374114071731, 0.6296517972089963, 0.6298293806633947, 0.629450599439698, 0.6318433377289319, 0.6314933335478203, 0.6350247647241801, 0.6391732947818369, 0.6387247857141036, 0.643748092196095, 0.6469018625100388, 0.6460686838647296, 0.6490350051177668, 0.6512621637793446, 0.6525533237404162, 0.653835114104005, 0.6566971962357554, 0.6562713105110706, 0.6595675165062025]
validation_metric:  [0.2293298738186683, 0.3422923610202854, 0.3984887817773104, 0.3826066890429554, 0.4381943522391655, 0.4306863470969988, 0.48331307508911, 0.4621591151849999, 0.4454295217559754, 0.4959849497950371, 0.5331736391523793, 0.5145443871376644, 0.4567879224452493, 0.4978330779724912, 0.4804084250580432, 0.5266823065545508, 0.5064097854639033, 0.5224945003632965, 0.5828066589307628, 0.5719756568690012, 0.5605156836803598, 0.5821778740756922, 0.5612299539629351, 0.5742130157660313, 0.5588577186390962, 0.5313708511688415, 0.5538912179486197, 0.5650226890021598, 0.5903183718625922, 0.5912619304945712, 0.5713923541668422, 0.5884583184962636, 0.5652705402523198, 0.5952125757300293, 0.6018412465200654, 0.6025913199821894, 0.5992303296913123, 0.5999085975473797, 0.617924073580049, 0.6158593051000416, 0.5751600415994887, 0.6128953800951402, 0.5866181844313364, 0.6033609828446185, 0.5591144000248914, 0.5905985589074625, 0.6324799462080285, 0.631981642340519, 0.6081412392960155, 0.6439053365725318, 0.6442048964642766, 0.6166287689204404, 0.6524462209158671, 0.64915145199348, 0.6326031219813713, 0.623943949587644, 0.6272203461437014, 0.6155220709820605, 0.6432688198252122, 0.6196061581446406]
best_val_loss 0.2698439723915524
best_val_metric 0.6524462209158671
Losses and metric loaded
Skipping epoch 1/120
Skipping epoch 2/120
Skipping epoch 3/120
Skipping epoch 4/120
Skipping epoch 5/120
Skipping epoch 6/120
Skipping epoch 7/120
Skipping epoch 8/120
Skipping epoch 9/120
Skipping epoch 10/120
Skipping epoch 11/120
Skipping epoch 12/120
Skipping epoch 13/120
Skipping epoch 14/120
Skipping epoch 15/120
Skipping epoch 16/120
Skipping epoch 17/120
Skipping epoch 18/120
Skipping epoch 19/120
Skipping epoch 20/120
Skipping epoch 21/120
Skipping epoch 22/120
Skipping epoch 23/120
Skipping epoch 24/120
Skipping epoch 25/120
Skipping epoch 26/120
Skipping epoch 27/120
Skipping epoch 28/120
Skipping epoch 29/120
Skipping epoch 30/120
Skipping epoch 31/120
Skipping epoch 32/120
Skipping epoch 33/120
Skipping epoch 34/120
Skipping epoch 35/120
Skipping epoch 36/120
Skipping epoch 37/120
Skipping epoch 38/120
Skipping epoch 39/120
Skipping epoch 40/120
Skipping epoch 41/120
Skipping epoch 42/120
Skipping epoch 43/120
Skipping epoch 44/120
Skipping epoch 45/120
Skipping epoch 46/120
Skipping epoch 47/120
Skipping epoch 48/120
Skipping epoch 49/120
Skipping epoch 50/120
Skipping epoch 51/120
Skipping epoch 52/120
Skipping epoch 53/120
Epoch 54/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 54/120: train loss 0.2505, val loss 0.2897
Epoch 54/120: train mF1 0.6614, val mF1 0.6226
Time: 0:08:17.724027
Epoch 55/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 55/120: train loss 0.2491, val loss 0.2700
Epoch 55/120: train mF1 0.6646, val mF1 0.6498
Time: 0:08:30.925615
Epoch 56/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 56/120: train loss 0.2496, val loss 0.2953
Epoch 56/120: train mF1 0.6634, val mF1 0.6040
Time: 0:08:55.119321
Epoch 57/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 57/120: train loss 0.2484, val loss 0.2666
Epoch 57/120: train mF1 0.6652, val mF1 0.6559
Time: 0:08:38.287770
Epoch 58/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 58/120: train loss 0.2464, val loss 0.2792
Epoch 58/120: train mF1 0.6691, val mF1 0.6313
Time: 0:08:58.894847
Epoch 59/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 59/120: train loss 0.2452, val loss 0.2754
Epoch 59/120: train mF1 0.6711, val mF1 0.6457
Time: 0:08:47.675953
Epoch 60/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 60/120: train loss 0.2449, val loss 0.2702
Epoch 60/120: train mF1 0.6725, val mF1 0.6508
Time: 0:09:46.089529
Epoch 61/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 61/120: train loss 0.2439, val loss 0.2916
Epoch 61/120: train mF1 0.6742, val mF1 0.6332
Time: 0:09:49.357100
Epoch 62/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 62/120: train loss 0.2418, val loss 0.2621
Epoch 62/120: train mF1 0.6776, val mF1 0.6545
Time: 0:09:42.772376
Epoch 63/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 63/120: train loss 0.2423, val loss 0.2678
Epoch 63/120: train mF1 0.6758, val mF1 0.6570
Time: 0:09:34.398529
Epoch 64/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 64/120: train loss 0.2394, val loss 0.2912
Epoch 64/120: train mF1 0.6806, val mF1 0.6186
Time: 0:08:30.382959
Epoch 65/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 65/120: train loss 0.2394, val loss 0.2938
Epoch 65/120: train mF1 0.6801, val mF1 0.6394
Time: 0:08:28.612100
Epoch 66/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 66/120: train loss 0.2381, val loss 0.3306
Epoch 66/120: train mF1 0.6842, val mF1 0.5867
Time: 0:08:29.404405
Epoch 67/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 67/120: train loss 0.2362, val loss 0.3131
Epoch 67/120: train mF1 0.6865, val mF1 0.6086
Time: 0:08:28.331704
Epoch 68/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 68/120: train loss 0.2359, val loss 0.2873
Epoch 68/120: train mF1 0.6863, val mF1 0.6371
Time: 0:08:28.425421
Epoch 69/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 69/120: train loss 0.2345, val loss 0.2884
Epoch 69/120: train mF1 0.6880, val mF1 0.6444
Time: 0:08:28.224607
Epoch 70/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 70/120: train loss 0.2348, val loss 0.2716
Epoch 70/120: train mF1 0.6899, val mF1 0.6540
Time: 0:08:43.933639
Epoch 71/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 71/120: train loss 0.2328, val loss 0.2916
Epoch 71/120: train mF1 0.6919, val mF1 0.6479
Time: 0:09:27.285430
Epoch 72/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 72/120: train loss 0.2324, val loss 0.2692
Epoch 72/120: train mF1 0.6924, val mF1 0.6648
Time: 0:09:17.092268
Epoch 73/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 73/120: train loss 0.2316, val loss 0.2670
Epoch 73/120: train mF1 0.6940, val mF1 0.6672
Time: 0:09:10.674323
Epoch 74/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 74/120: train loss 0.2304, val loss 0.3100
Epoch 74/120: train mF1 0.6965, val mF1 0.6118
Time: 0:09:10.738523
Epoch 75/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 75/120: train loss 0.2291, val loss 0.2734
Epoch 75/120: train mF1 0.6979, val mF1 0.6358
Time: 0:09:41.079892
Epoch 76/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 76/120: train loss 0.2267, val loss 0.3012
Epoch 76/120: train mF1 0.7004, val mF1 0.6146
Time: 0:09:40.321152
Epoch 77/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 77/120: train loss 0.2274, val loss 0.3042
Epoch 77/120: train mF1 0.7010, val mF1 0.6289
Time: 0:09:36.042488
Epoch 78/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 78/120: train loss 0.2256, val loss 0.2602
Epoch 78/120: train mF1 0.7045, val mF1 0.6679
Time: 0:09:46.787486
Epoch 79/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 79/120: train loss 0.2244, val loss 0.2767
Epoch 79/120: train mF1 0.7059, val mF1 0.6534
Time: 0:09:33.110713
Epoch 80/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 80/120: train loss 0.2243, val loss 0.2743
Epoch 80/120: train mF1 0.7069, val mF1 0.6703
Time: 0:09:35.419809
Epoch 81/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 81/120: train loss 0.2236, val loss 0.2683
Epoch 81/120: train mF1 0.7080, val mF1 0.6628
Time: 0:09:36.587478
Epoch 82/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 82/120: train loss 0.2228, val loss 0.3013
Epoch 82/120: train mF1 0.7091, val mF1 0.6279
Time: 0:09:32.934899
Epoch 83/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 83/120: train loss 0.2212, val loss 0.2667
Epoch 83/120: train mF1 0.7116, val mF1 0.6665
Time: 0:09:23.716147
Epoch 84/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 84/120: train loss 0.2196, val loss 0.2710
Epoch 84/120: train mF1 0.7143, val mF1 0.6616
Time: 0:09:27.429528
Epoch 85/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 85/120: train loss 0.2185, val loss 0.2784
Epoch 85/120: train mF1 0.7148, val mF1 0.6503
Time: 0:09:29.873837
Epoch 86/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 86/120: train loss 0.2179, val loss 0.2744
Epoch 86/120: train mF1 0.7159, val mF1 0.6641
Time: 0:09:34.465433
Epoch 87/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 87/120: train loss 0.2163, val loss 0.2784
Epoch 87/120: train mF1 0.7214, val mF1 0.6634
Time: 0:09:33.633656
Epoch 88/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 88/120: train loss 0.2149, val loss 0.2729
Epoch 88/120: train mF1 0.7212, val mF1 0.6795
Time: 0:09:32.155964
Epoch 89/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 89/120: train loss 0.2138, val loss 0.3194
Epoch 89/120: train mF1 0.7225, val mF1 0.6032
Time: 0:09:51.333321
Epoch 90/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 90/120: train loss 0.2139, val loss 0.3100
Epoch 90/120: train mF1 0.7227, val mF1 0.6200
Time: 0:09:40.644787
Epoch 91/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 91/120: train loss 0.2130, val loss 0.2734
Epoch 91/120: train mF1 0.7243, val mF1 0.6649
Time: 0:09:42.484023
Epoch 92/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 92/120: train loss 0.2126, val loss 0.2924
Epoch 92/120: train mF1 0.7247, val mF1 0.6447
Time: 0:09:23.742533
Epoch 93/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 93/120: train loss 0.2116, val loss 0.2727
Epoch 93/120: train mF1 0.7276, val mF1 0.6689
Time: 0:09:23.921930
Epoch 94/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 94/120: train loss 0.2090, val loss 0.2742
Epoch 94/120: train mF1 0.7298, val mF1 0.6597
Time: 0:09:37.494979
Epoch 95/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 95/120: train loss 0.2087, val loss 0.2904
Epoch 95/120: train mF1 0.7312, val mF1 0.6475
Time: 0:09:34.500563
Epoch 96/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 96/120: train loss 0.2091, val loss 0.2785
Epoch 96/120: train mF1 0.7305, val mF1 0.6695
Time: 0:09:37.407382
Epoch 97/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 97/120: train loss 0.2062, val loss 0.2797
Epoch 97/120: train mF1 0.7361, val mF1 0.6688
Time: 0:09:37.573905
Epoch 98/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 98/120: train loss 0.2054, val loss 0.3030
Epoch 98/120: train mF1 0.7369, val mF1 0.6355
Time: 0:09:42.468414
Epoch 99/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 99/120: train loss 0.2049, val loss 0.2806
Epoch 99/120: train mF1 0.7368, val mF1 0.6646
Time: 0:09:46.494160
Epoch 100/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 100/120: train loss 0.2042, val loss 0.2878
Epoch 100/120: train mF1 0.7390, val mF1 0.6637
Time: 0:09:38.543109
Epoch 101/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 101/120: train loss 0.2023, val loss 0.2784
Epoch 101/120: train mF1 0.7420, val mF1 0.6638
Time: 0:09:48.195727
Epoch 102/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 102/120: train loss 0.2019, val loss 0.2775
Epoch 102/120: train mF1 0.7411, val mF1 0.6686
Time: 0:09:43.837032
Epoch 103/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 103/120: train loss 0.1989, val loss 0.2726
Epoch 103/120: train mF1 0.7464, val mF1 0.6708
Time: 0:09:42.296197
Epoch 104/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 104/120: train loss 0.1995, val loss 0.2942
Epoch 104/120: train mF1 0.7481, val mF1 0.6467
Time: 0:09:34.978643
Epoch 105/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 105/120: train loss 0.1976, val loss 0.2701
Epoch 105/120: train mF1 0.7498, val mF1 0.6771
Time: 0:09:45.996148
Epoch 106/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 106/120: train loss 0.1978, val loss 0.2784
Epoch 106/120: train mF1 0.7488, val mF1 0.6700
Time: 0:09:58.375817
Epoch 107/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 107/120: train loss 0.1965, val loss 0.2925
Epoch 107/120: train mF1 0.7500, val mF1 0.6618
Time: 0:09:46.290295
Epoch 108/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 108/120: train loss 0.1954, val loss 0.2851
Epoch 108/120: train mF1 0.7535, val mF1 0.6625
Time: 0:09:35.505440
Epoch 109/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 109/120: train loss 0.1940, val loss 0.2726
Epoch 109/120: train mF1 0.7558, val mF1 0.6733
Time: 0:09:40.848991
Epoch 110/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 110/120: train loss 0.1933, val loss 0.2793
Epoch 110/120: train mF1 0.7560, val mF1 0.6810
Time: 0:09:52.388778
Epoch 111/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 111/120: train loss 0.1908, val loss 0.2894
Epoch 111/120: train mF1 0.7598, val mF1 0.6712
Time: 0:09:49.069310
Epoch 112/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 112/120: train loss 0.1905, val loss 0.2846
Epoch 112/120: train mF1 0.7617, val mF1 0.6634
Time: 0:09:41.950147
Epoch 113/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 113/120: train loss 0.1904, val loss 0.3006
Epoch 113/120: train mF1 0.7620, val mF1 0.6559
Time: 0:09:46.193049
Epoch 114/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 114/120: train loss 0.1895, val loss 0.3264
Epoch 114/120: train mF1 0.7602, val mF1 0.6304
Time: 0:09:57.601713
Epoch 115/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 115/120: train loss 0.1879, val loss 0.2770
Epoch 115/120: train mF1 0.7644, val mF1 0.6762
Time: 0:09:46.252643
Epoch 116/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 116/120: train loss 0.1876, val loss 0.2794
Epoch 116/120: train mF1 0.7640, val mF1 0.6717
Time: 0:09:38.764254
Epoch 117/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 117/120: train loss 0.1850, val loss 0.2990
Epoch 117/120: train mF1 0.7689, val mF1 0.6719
Time: 0:09:19.709892
Epoch 118/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 118/120: train loss 0.1846, val loss 0.3219
Epoch 118/120: train mF1 0.7694, val mF1 0.6470
Time: 0:08:36.145605
Epoch 119/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 119/120: train loss 0.1836, val loss 0.3093
Epoch 119/120: train mF1 0.7727, val mF1 0.6530
Time: 0:08:33.958345
Epoch 120/120
Training
Batch: 0  over  25
Validation
Batch: 0  over  9
Epoch 120/120: train loss 0.1826, val loss 0.2967
Epoch 120/120: train mF1 0.7727, val mF1 0.6706
/home/bertille/.local/lib/python3.8/site-packages/pydantic/main.py:347: UserWarning: Pydantic serializer warnings:
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Time: 0:08:35.865959
Traceback (most recent call last):
  File "main.py", line 262, in <module>
    model.eval()
  File "/home/bertille/.local/lib/python3.8/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/bertille/.local/lib/python3.8/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/bertille/.local/lib/python3.8/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '../../results/resnet18_64_l1/random_shuffling/all/resnet18_multi_label_64_random_60epochs_bs4096_augmented/models/unet_intermed_epoch85.pt'
