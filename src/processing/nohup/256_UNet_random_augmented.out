----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: all
Patches: all
Batch size: 16
Normalisation: channel_by_channel
Data augmentation
The seed to shuffle the data is  1
The data are from  all  year
Image shape: torch.Size([16, 4, 256, 256]), Mask shape: torch.Size([16, 256, 256])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask: [3], dtype: torch.uint8
Train: 6397 images, Val: 2133 images, Test: 2133 images
Train: 59.99%, Val: 20.00%, Test: 20.00%
Creating model...
Model settings:
Pretrained: False
Classes: 6
The model is UNet
Encoder name: efficientnet-b7
Creating optimizer...
Training settings:
Learning rate: 0.001
Criterion: Dice
Optimizer: Adam
Training...
Epoch 1/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 1/70: train loss 0.7291, val loss 0.7761
Epoch 1/70: train mIoU 0.1538, val mIoU 0.1119
Time: 0:15:02.711758
Epoch 2/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 2/70: train loss 0.6952, val loss 0.7099
Epoch 2/70: train mIoU 0.1851, val mIoU 0.1766
Time: 0:14:14.970751
Epoch 3/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 3/70: train loss 0.6549, val loss 0.7097
Epoch 3/70: train mIoU 0.2112, val mIoU 0.1802
Time: 0:14:02.015122
Epoch 4/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 4/70: train loss 0.6374, val loss 0.6028
Epoch 4/70: train mIoU 0.2263, val mIoU 0.2662
Time: 0:14:03.888269
Epoch 5/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 5/70: train loss 0.6252, val loss 0.5986
Epoch 5/70: train mIoU 0.2363, val mIoU 0.2644
Time: 0:14:04.430834
Epoch 6/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 6/70: train loss 0.6225, val loss 0.5698
Epoch 6/70: train mIoU 0.2326, val mIoU 0.2941
Time: 0:14:03.102427
Epoch 7/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 7/70: train loss 0.6044, val loss 0.6521
Epoch 7/70: train mIoU 0.2506, val mIoU 0.2208
Time: 0:14:01.779603
Epoch 8/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 8/70: train loss 0.6110, val loss 0.5669
Epoch 8/70: train mIoU 0.2466, val mIoU 0.2966
Time: 0:14:00.773744
Epoch 9/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 9/70: train loss 0.6007, val loss 0.6307
Epoch 9/70: train mIoU 0.2528, val mIoU 0.2433
Time: 0:14:00.759353
Epoch 10/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 10/70: train loss 0.5835, val loss 0.5659
Epoch 10/70: train mIoU 0.2709, val mIoU 0.2990
Time: 0:14:03.269828
Epoch 11/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 11/70: train loss 0.5922, val loss 0.5363
Epoch 11/70: train mIoU 0.2642, val mIoU 0.3265
Time: 0:14:02.947169
Epoch 12/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 12/70: train loss 0.5916, val loss 0.6007
Epoch 12/70: train mIoU 0.2615, val mIoU 0.2717
Time: 0:14:00.682955
Epoch 13/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 13/70: train loss 0.5839, val loss 0.6042
Epoch 13/70: train mIoU 0.2650, val mIoU 0.2671
Time: 0:14:00.267745
Epoch 14/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 14/70: train loss 0.5731, val loss 0.6255
Epoch 14/70: train mIoU 0.2761, val mIoU 0.2444
Time: 0:14:00.441202
Epoch 15/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 15/70: train loss 0.5669, val loss 0.5728
Epoch 15/70: train mIoU 0.2856, val mIoU 0.2937
Time: 0:14:01.520211
Epoch 16/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 16/70: train loss 0.5672, val loss 0.5171
Epoch 16/70: train mIoU 0.2872, val mIoU 0.3467
Time: 0:14:00.825456
Epoch 17/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 17/70: train loss 0.5687, val loss 0.5256
Epoch 17/70: train mIoU 0.2859, val mIoU 0.3408
Time: 0:14:00.214855
Epoch 18/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 18/70: train loss 0.5677, val loss 0.5384
Epoch 18/70: train mIoU 0.2842, val mIoU 0.3240
Time: 0:14:00.930986
Epoch 19/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 19/70: train loss 0.5565, val loss 0.6073
Epoch 19/70: train mIoU 0.2942, val mIoU 0.2676
Time: 0:14:04.254071
Epoch 20/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 20/70: train loss 0.5523, val loss 0.5660
Epoch 20/70: train mIoU 0.2992, val mIoU 0.3108
Time: 0:14:01.869205
Epoch 21/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 21/70: train loss 0.5442, val loss 0.4979
Epoch 21/70: train mIoU 0.3037, val mIoU 0.3705
Time: 0:14:03.345795
Epoch 22/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 22/70: train loss 0.5480, val loss 0.5388
Epoch 22/70: train mIoU 0.2994, val mIoU 0.3320
Time: 0:14:02.184428
Epoch 23/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 23/70: train loss 0.5454, val loss 0.5184
Epoch 23/70: train mIoU 0.3028, val mIoU 0.3536
Time: 0:14:06.822202
Epoch 24/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 24/70: train loss 0.5333, val loss 0.5187
Epoch 24/70: train mIoU 0.3120, val mIoU 0.3415
Time: 0:14:01.325764
Epoch 25/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 25/70: train loss 0.5417, val loss 0.5289
Epoch 25/70: train mIoU 0.3087, val mIoU 0.3460
Time: 0:14:03.220833
Epoch 26/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 26/70: train loss 0.5504, val loss 0.5249
Epoch 26/70: train mIoU 0.3009, val mIoU 0.3488
Time: 0:14:02.221869
Epoch 27/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 27/70: train loss 0.5467, val loss 0.5380
Epoch 27/70: train mIoU 0.3034, val mIoU 0.3330
Time: 0:14:00.789218
Epoch 28/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 28/70: train loss 0.5383, val loss 0.5533
Epoch 28/70: train mIoU 0.3095, val mIoU 0.3203
Time: 0:14:02.347038
Epoch 29/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 29/70: train loss 0.5309, val loss 0.5449
Epoch 29/70: train mIoU 0.3154, val mIoU 0.3344
Time: 0:14:01.178446
Epoch 30/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 30/70: train loss 0.5208, val loss 0.5194
Epoch 30/70: train mIoU 0.3265, val mIoU 0.3538
Time: 0:14:01.453344
Epoch 31/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 31/70: train loss 0.5295, val loss 0.5379
Epoch 31/70: train mIoU 0.3190, val mIoU 0.3373
Time: 0:14:01.909758
Epoch 32/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 32/70: train loss 0.5267, val loss 0.4959
Epoch 32/70: train mIoU 0.3234, val mIoU 0.3737
Time: 0:14:02.206194
Epoch 33/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 33/70: train loss 0.5271, val loss 0.4932
Epoch 33/70: train mIoU 0.3217, val mIoU 0.3728
Time: 0:14:02.399518
Epoch 34/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 34/70: train loss 0.5142, val loss 0.5349
Epoch 34/70: train mIoU 0.3332, val mIoU 0.3455
Time: 0:14:03.330096
Epoch 35/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 35/70: train loss 0.5144, val loss 0.4838
Epoch 35/70: train mIoU 0.3314, val mIoU 0.3882
Time: 0:14:03.169931
Epoch 36/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 36/70: train loss 0.5037, val loss 0.4962
Epoch 36/70: train mIoU 0.3398, val mIoU 0.3741
Time: 0:14:01.957461
Epoch 37/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 37/70: train loss 0.5107, val loss 0.5098
Epoch 37/70: train mIoU 0.3363, val mIoU 0.3578
Time: 0:14:01.312588
Epoch 38/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 38/70: train loss 0.5065, val loss 0.4967
Epoch 38/70: train mIoU 0.3385, val mIoU 0.3753
Time: 0:14:01.966090
Epoch 39/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 39/70: train loss 0.5017, val loss 0.4941
Epoch 39/70: train mIoU 0.3435, val mIoU 0.3832
Time: 0:14:01.059179
Epoch 40/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 40/70: train loss 0.5122, val loss 0.5162
Epoch 40/70: train mIoU 0.3358, val mIoU 0.3558
Time: 0:13:59.580271
Epoch 41/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 41/70: train loss 0.5162, val loss 0.5059
Epoch 41/70: train mIoU 0.3338, val mIoU 0.3626
Time: 0:14:01.398989
Epoch 42/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 42/70: train loss 0.5001, val loss 0.4960
Epoch 42/70: train mIoU 0.3467, val mIoU 0.3751
Time: 0:14:02.388140
Epoch 43/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 43/70: train loss 0.4960, val loss 0.5332
Epoch 43/70: train mIoU 0.3486, val mIoU 0.3375
Time: 0:14:01.732058
Epoch 44/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 44/70: train loss 0.4995, val loss 0.5112
Epoch 44/70: train mIoU 0.3501, val mIoU 0.3707
Time: 0:14:03.995406
Epoch 45/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 45/70: train loss 0.4934, val loss 0.5272
Epoch 45/70: train mIoU 0.3526, val mIoU 0.3330
Time: 0:14:00.461088
Epoch 46/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 46/70: train loss 0.4907, val loss 0.4732
Epoch 46/70: train mIoU 0.3523, val mIoU 0.3968
Time: 0:14:02.528353
Epoch 47/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 47/70: train loss 0.4760, val loss 0.5059
Epoch 47/70: train mIoU 0.3699, val mIoU 0.3736
Time: 0:14:03.518389
Epoch 48/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 48/70: train loss 0.4924, val loss 0.4606
Epoch 48/70: train mIoU 0.3618, val mIoU 0.4154
Time: 0:14:01.456966
Epoch 49/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 49/70: train loss 0.4800, val loss 0.4742
Epoch 49/70: train mIoU 0.3641, val mIoU 0.3955
Time: 0:14:01.021153
Epoch 50/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 50/70: train loss 0.4777, val loss 0.4754
Epoch 50/70: train mIoU 0.3674, val mIoU 0.3959
Time: 0:14:03.322217
Epoch 51/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 51/70: train loss 0.4858, val loss 0.5354
Epoch 51/70: train mIoU 0.3644, val mIoU 0.3329
Time: 0:14:01.310943
Epoch 52/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 52/70: train loss 0.4885, val loss 0.4866
Epoch 52/70: train mIoU 0.3601, val mIoU 0.3913
Time: 0:14:01.517596
Epoch 53/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 53/70: train loss 0.4793, val loss 0.4495
Epoch 53/70: train mIoU 0.3671, val mIoU 0.4271
Time: 0:14:01.787640
Epoch 54/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 54/70: train loss 0.4718, val loss 0.4799
Epoch 54/70: train mIoU 0.3707, val mIoU 0.4098
Time: 0:14:02.750377
Epoch 55/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 55/70: train loss 0.4733, val loss 0.4725
Epoch 55/70: train mIoU 0.3723, val mIoU 0.3997
Time: 0:14:01.665339
Epoch 56/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 56/70: train loss 0.4778, val loss 0.4940
Epoch 56/70: train mIoU 0.3716, val mIoU 0.3842
Time: 0:14:02.456964
Epoch 57/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 57/70: train loss 0.4752, val loss 0.4935
Epoch 57/70: train mIoU 0.3769, val mIoU 0.3786
Time: 0:14:01.377545
Epoch 58/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 58/70: train loss 0.4826, val loss 0.4509
Epoch 58/70: train mIoU 0.3670, val mIoU 0.4324
Time: 0:14:02.880494
Epoch 59/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 59/70: train loss 0.4772, val loss 0.4989
Epoch 59/70: train mIoU 0.3726, val mIoU 0.3788
Time: 0:14:01.234486
Epoch 60/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 60/70: train loss 0.4647, val loss 0.4830
Epoch 60/70: train mIoU 0.3795, val mIoU 0.3947
Time: 0:14:02.086807
Epoch 61/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 61/70: train loss 0.4760, val loss 0.4332
Epoch 61/70: train mIoU 0.3679, val mIoU 0.4472
Time: 0:14:01.126212
Epoch 62/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 62/70: train loss 0.4650, val loss 0.4434
Epoch 62/70: train mIoU 0.3805, val mIoU 0.4316
Time: 0:14:01.112482
Epoch 63/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 63/70: train loss 0.4705, val loss 0.4245
Epoch 63/70: train mIoU 0.3806, val mIoU 0.4521
Time: 0:14:10.045375
Epoch 64/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 64/70: train loss 0.4547, val loss 0.4766
Epoch 64/70: train mIoU 0.3896, val mIoU 0.3998
Time: 0:14:01.321341
Epoch 65/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 65/70: train loss 0.4709, val loss 0.4730
Epoch 65/70: train mIoU 0.3835, val mIoU 0.4032
Time: 0:14:02.304189
Epoch 66/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 66/70: train loss 0.4551, val loss 0.4497
Epoch 66/70: train mIoU 0.3927, val mIoU 0.4218
Time: 0:14:00.743317
Epoch 67/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 67/70: train loss 0.4593, val loss 0.4482
Epoch 67/70: train mIoU 0.3896, val mIoU 0.4328
Time: 0:14:01.488531
Epoch 68/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 68/70: train loss 0.4625, val loss 0.4562
Epoch 68/70: train mIoU 0.3878, val mIoU 0.4233
Time: 0:13:59.841271
Epoch 69/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 69/70: train loss 0.4652, val loss 0.4504
Epoch 69/70: train mIoU 0.3783, val mIoU 0.4268
Time: 0:14:01.896770
Epoch 70/70
Training
Batch: 0  over  400
Batch: 50  over  400
Batch: 100  over  400
Batch: 150  over  400
Batch: 200  over  400
Batch: 250  over  400
Batch: 300  over  400
Batch: 350  over  400
Validation
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Epoch 70/70: train loss 0.4588, val loss 0.4700
Epoch 70/70: train mIoU 0.3832, val mIoU 0.4073
Time: 0:14:02.876824
Testing
Batch: 0  over  134
Batch: 50  over  134
Batch: 100  over  134
Test IoU by class: {0: 0.33622824902808524, 1: 0.41011295364304007, 2: 0.5965938271578801, 3: 0.5460801048305075, 4: 0.47956835212002813, 5: 0.300996983595326}
Test mIoU: 0.4449300783958112
Test F1 by class: {0: 0.5032497243979734, 1: 0.5816739043259044, 2: 0.7473332503356668, 3: 0.7064059658026228, 4: 0.6482544066759327, 5: 0.462717419626164}
Test mF1: 0.608272445194044
img[i].shape (4, 256, 256)
[[[0.33246073 0.30759162 0.3115183  ... 0.15445027 0.18455498 0.14136125]
  [0.36256546 0.32591623 0.33246073 ... 0.20942408 0.20157067 0.21335079]
  [0.40837696 0.40706807 0.40837696 ... 0.20026179 0.21073298 0.2408377 ]
  ...
  [0.7984293  0.82329845 0.90183246 ... 0.9201571  0.8678011  0.8560209 ]
  [0.7840314  0.8246073  0.8246073  ... 0.89790577 0.9070681  0.9293194 ]
  [0.7356021  0.7814136  0.79581153 ... 0.87827224 0.9031414  0.9698953 ]]

 [[0.35379538 0.34059405 0.35313532 ... 0.16039604 0.17623763 0.16369636]
  [0.3881188  0.3709571  0.36237624 ... 0.2019802  0.1980198  0.21650165]
  [0.41584158 0.41320133 0.4019802  ... 0.20264027 0.22046204 0.25478548]
  ...
  [0.81848186 0.8330033  0.8792079  ... 0.89966995 0.8739274  0.8739274 ]
  [0.7920792  0.82376236 0.83168316 ... 0.8917492  0.90429044 0.9273927 ]
  [0.75115514 0.7828383  0.7960396  ... 0.8534653  0.8937294  0.95445544]]

 [[0.33333334 0.30327868 0.31967214 ... 0.12568305 0.140255   0.12659381]
  [0.36612022 0.356102   0.34517303 ... 0.13296904 0.13023679 0.16393442]
  [0.3970856  0.40619308 0.39799637 ... 0.12568305 0.15118398 0.215847  ]
  ...
  [0.8078324  0.8032787  0.88160294 ... 0.87431693 0.85519123 0.87613845]
  [0.78142077 0.8060109  0.81056464 ... 0.8916211  0.9162113  0.9362477 ]
  [0.73406196 0.7686703  0.7759563  ... 0.8424408  0.8952641  0.9526412 ]]

 [[0.54480445 0.57381606 0.5995587  ... 0.5423528  0.5006742  0.5059862 ]
  [0.52641684 0.5439873  0.5456217  ... 0.6297961  0.6154946  0.63347363]
  [0.49658808 0.50394315 0.5063948  ... 0.6596249  0.6820986  0.71437913]
  ...
  [0.46185592 0.4761574  0.4728885  ... 0.50394315 0.49495363 0.48882443]
  [0.43488744 0.44673723 0.4557267  ... 0.50108284 0.4978139  0.4969967 ]
  [0.41813427 0.42671517 0.43774772 ... 0.5031259  0.5068034  0.50721204]]]
img[i].shape (4, 256, 256)
[[[0.09992132 0.11487018 0.13217938 ... 0.5302911  0.6223446  0.7985838 ]
  [0.12352479 0.12352479 0.16050354 ... 0.6105429  0.7741935  1.        ]
  [0.15420929 0.16286388 0.1793863  ... 0.7828482  0.97718334 1.        ]
  ...
  [0.4303698  0.4571204  0.5578285  ... 0.26593235 0.14240755 0.1361133 ]
  [0.35798582 0.44610542 0.5570417  ... 0.12116444 0.1007081  0.06687648]
  [0.3603462  0.43509048 0.5247836  ... 0.12667191 0.06845004 0.02832416]]

 [[0.12849931 0.1546581  0.16062415 ... 0.5323543  0.63423586 0.8210188 ]
  [0.15236346 0.16704911 0.17530978 ... 0.6167967  0.7925654  1.        ]
  [0.1776044  0.1881597  0.18402937 ... 0.8022028  0.99403393 1.        ]
  ...
  [0.4093621  0.4570904  0.5571363  ... 0.267095   0.15511702 0.12115649]
  [0.36301056 0.44882974 0.5571363  ... 0.1459385  0.10463516 0.06746214]
  [0.36484626 0.43184948 0.5231758  ... 0.10096374 0.05644791 0.03028912]]

 [[0.10788382 0.13762103 0.13900416 ... 0.46887967 0.6065007  0.8430152 ]
  [0.13416322 0.14522822 0.14799447 ... 0.5387275  0.7683264  1.        ]
  [0.16320886 0.16943292 0.15905948 ... 0.7385892  0.98063624 1.        ]
  ...
  [0.39972338 0.4515906  0.5643154  ... 0.29045644 0.18810512 0.14591978]
  [0.32987553 0.43568465 0.5698479  ... 0.13139696 0.09612725 0.05947441]
  [0.33886585 0.41770402 0.538036   ... 0.06362379 0.02005533 0.00622407]]

 [[0.51970446 0.6141215  0.6695402  ... 0.6744663  0.7278325  0.80500823]
  [0.5722496  0.64326763 0.70320195 ... 0.74876845 0.81937605 0.8953202 ]
  [0.60755336 0.66461414 0.68349755 ... 0.84359604 0.91050905 0.9819376 ]
  ...
  [0.4720854  0.48932678 0.54310346 ... 0.37520525 0.30295566 0.23973727]
  [0.4601806  0.48768473 0.5463875  ... 0.26395732 0.22208539 0.19745484]
  [0.45935962 0.48727423 0.5410509  ... 0.1629721  0.12602627 0.1182266 ]]]
img[i].shape (4, 256, 256)
[[[0.2617146  0.23188685 0.25016838 ... 0.26652554 0.30020207 0.20109689]
  [0.25786588 0.25016838 0.22900029 ... 0.23188685 0.22322717 0.1933994 ]
  [0.26652554 0.22418936 0.21168093 ... 0.21360531 0.20302126 0.29923987]
  ...
  [1.         1.         0.9939382  ... 0.19628596 0.14336573 0.11450014]
  [1.         1.         1.         ... 0.12700856 0.08563456 0.05484461]
  [1.         1.         1.         ... 0.14913884 0.13181949 0.08659675]]

 [[0.4092153  0.38248584 0.39139566 ... 0.33284542 0.3194807  0.22210908]
  [0.39012283 0.3894864  0.3723032  ... 0.32138994 0.26984027 0.2227455 ]
  [0.381213   0.35321072 0.339846   ... 0.28002292 0.24438363 0.29720613]
  ...
  [1.         1.         0.93807673 ... 0.283205   0.24056514 0.18901546]
  [1.         1.         1.         ... 0.20556228 0.1661045  0.12282823]
  [1.         1.         1.         ... 0.23229173 0.20365302 0.13873862]]

 [[0.33126295 0.30124223 0.33229813 ... 0.30227745 0.29606625 0.17287785]
  [0.30538303 0.310559   0.29917184 ... 0.23706004 0.1863354  0.16149068]
  [0.31573498 0.26708075 0.25465837 ... 0.21428572 0.18322982 0.2836439 ]
  ...
  [1.         1.         0.9637681  ... 0.23084886 0.21532091 0.17080745]
  [1.         0.91821945 1.         ... 0.16252588 0.11594203 0.07971015]
  [0.9627329  0.90993786 0.994824   ... 0.20703934 0.17184265 0.09213251]]

 [[0.9441019  0.90423346 0.86765313 ... 0.3859433  0.27332512 0.21454994]
  [0.9260173  0.87833947 0.7998356  ... 0.45745993 0.34936294 0.3025072 ]
  [0.8688862  0.8405261  0.7681874  ... 0.39375257 0.35511714 0.36415946]
  ...
  [0.6078915  0.5696671  0.56185776 ... 0.6346075  0.58528566 0.45540485]
  [0.5729552  0.51993424 0.5162351  ... 0.5195232  0.4759556  0.38799834]
  [0.53596383 0.5051377  0.49280724 ... 0.648993   0.5507604  0.39539662]]]
img[i].shape (4, 256, 256)
[[[0.4739229  0.5011338  0.52154195 ... 0.414966   0.48979592 0.5419501 ]
  [0.48979592 0.4829932  0.60770977 ... 0.41723356 0.5442177  0.5782313 ]
  [0.45578232 0.5827664  0.71882087 ... 0.4376417  0.61904764 0.6666667 ]
  ...
  [0.5714286  0.41269842 0.34013605 ... 0.24716553 0.39229023 0.35600907]
  [0.5147392  0.39455783 0.36734694 ... 0.33333334 0.5102041  0.50793654]
  [0.45351472 0.4489796  0.39909297 ... 0.45351472 0.5827664  0.5283447 ]]

 [[0.49019608 0.49859944 0.48879552 ... 0.42436975 0.49439776 0.53361344]
  [0.49019608 0.45938376 0.51960784 ... 0.43417367 0.54761904 0.57703084]
  [0.43557423 0.49019608 0.57563025 ... 0.44537815 0.61484593 0.6554622 ]
  ...
  [0.69187677 0.60084033 0.557423   ... 0.232493   0.37535015 0.41456583]
  [0.6386555  0.56442577 0.5588235  ... 0.2577031  0.42016807 0.52380955]
  [0.61484593 0.5854342  0.5826331  ... 0.34453782 0.464986   0.5028011 ]]

 [[0.48368523 0.512476   0.46449137 ... 0.4049904  0.4568138  0.5163148 ]
  [0.49136275 0.4721689  0.53166986 ... 0.39731285 0.5143954  0.5547025 ]
  [0.45105568 0.512476   0.6103647  ... 0.3934741  0.6276392  0.6660269 ]
  ...
  [0.6429942  0.49520153 0.42802304 ... 0.21113244 0.38771594 0.45297503]
  [0.55086374 0.4069098  0.40882918 ... 0.19193858 0.35700575 0.57581574]
  [0.5239923  0.47792706 0.46449137 ... 0.26103646 0.36852208 0.4568138 ]]

 [[0.3418291  0.3418291  0.34932533 ... 0.28185907 0.3328336  0.38230884]
  [0.3538231  0.3448276  0.32233882 ... 0.3013493  0.35532233 0.4017991 ]
  [0.39730135 0.3418291  0.2893553  ... 0.34032983 0.3868066  0.42128935]
  ...
  [0.81409293 0.7721139  0.8155922  ... 0.6791604  0.69415295 0.6971514 ]
  [0.7856072  0.7796102  0.8545727  ... 0.7151424  0.7241379  0.70614696]
  [0.76911545 0.76911545 0.85607195 ... 0.7721139  0.7436282  0.7031484 ]]]
img[i].shape (4, 256, 256)
[[[0.55714285 0.4892857  0.3642857  ... 0.63928574 0.57857144 0.475     ]
  [0.49285713 0.325      0.31785715 ... 0.55714285 0.475      0.6571429 ]
  [0.44642857 0.30357143 0.48214287 ... 0.43214285 0.5607143  0.675     ]
  ...
  [0.63928574 0.46071428 0.3607143  ... 0.39642859 0.23928571 0.275     ]
  [0.66785717 0.57857144 0.3607143  ... 0.39642859 0.43928573 0.43928573]
  [0.58214283 0.5        0.34642857 ... 0.39642859 0.44642857 0.42142856]]

 [[0.5208333  0.41435185 0.29166666 ... 0.5162037  0.5        0.4699074 ]
  [0.4097222  0.24305555 0.1736111  ... 0.5648148  0.49305555 0.6388889 ]
  [0.37962964 0.18518518 0.33796296 ... 0.5231481  0.6064815  0.6805556 ]
  ...
  [0.599537   0.4652778  0.3611111  ... 0.38194445 0.29166666 0.31712964]
  [0.625      0.5416667  0.33564815 ... 0.39814815 0.41435185 0.4027778 ]
  [0.5648148  0.49074075 0.3472222  ... 0.37037036 0.40046296 0.38657406]]

 [[0.5875371  0.48961425 0.43916914 ... 0.46884274 0.47477746 0.40059346]
  [0.48961425 0.32047477 0.2462908  ... 0.5548961  0.43916914 0.56379825]
  [0.47774482 0.22551928 0.37388724 ... 0.51038575 0.59050447 0.6379822 ]
  ...
  [0.66172105 0.5281899  0.47477746 ... 0.421365   0.3264095  0.3620178 ]
  [0.69436204 0.68249255 0.4866469  ... 0.40356082 0.4480712  0.46587536]
  [0.60534126 0.5875371  0.5192878  ... 0.3264095  0.421365   0.42433235]]

 [[0.38518518 0.28148147 0.19444445 ... 0.73333335 0.7092593  0.68333334]
  [0.2888889  0.17222223 0.03148148 ... 0.76111114 0.7462963  0.7351852 ]
  [0.25       0.13148148 0.01666667 ... 0.8425926  0.8388889  0.85      ]
  ...
  [0.57592595 0.5425926  0.4037037  ... 0.41666666 0.5148148  0.6537037 ]
  [0.48518518 0.43333334 0.31666666 ... 0.38703704 0.44074073 0.5925926 ]
  [0.4        0.33888888 0.27407408 ... 0.3888889  0.42962962 0.53333336]]]
img[i].shape (4, 256, 256)
[[[0.18502674 0.18074866 0.21069519 ... 0.22780749 0.21283422 0.20320855]
  [0.12620321 0.1657754  0.18502674 ... 0.21604279 0.20855615 0.22459893]
  [0.12406417 0.16256684 0.18716578 ... 0.1764706  0.17967914 0.19786096]
  ...
  [0.31336898 0.25882354 0.23315509 ... 1.         0.99251336 0.77433157]
  [0.29090908 0.2737968  0.19679144 ... 1.         0.8        0.5262032 ]
  [0.29518718 0.30160427 0.2812834  ... 1.         0.631016   0.34117648]]

 [[0.33485955 0.30410716 0.30069023 ... 0.33417618 0.31982505 0.32597554]
  [0.28907263 0.2863391  0.2924896  ... 0.3184583  0.31504133 0.3334928 ]
  [0.30000684 0.28975603 0.29727328 ... 0.28428894 0.2863391  0.2856557 ]
  ...
  [0.42369986 0.37654617 0.3485273  ... 1.         1.         0.82006425]
  [0.40388164 0.3854302  0.3334928  ... 1.         0.84534955 0.5726782 ]
  [0.40798196 0.4216497  0.4059318  ... 1.         0.6758696  0.39841455]]

 [[0.26372445 0.2615716  0.22712594 ... 0.28525296 0.27233586 0.28417653]
  [0.21959096 0.24434876 0.25726587 ... 0.26803014 0.27233586 0.3035522 ]
  [0.22497308 0.24434876 0.27125943 ... 0.24111949 0.25296018 0.24004306]
  ...
  [0.38320774 0.32185146 0.2863294  ... 1.         1.         0.8643703 ]
  [0.35952637 0.3476857  0.29386437 ... 1.         0.8826695  0.5974166 ]
  [0.38751346 0.4262648  0.4187298  ... 1.         0.74273413 0.4962325 ]]

 [[0.6500744  0.5982336  0.60399365 ... 0.8180771  0.8060769  0.8180771 ]
  [0.59535354 0.5881534  0.5943935  ... 0.74175584 0.75711614 0.7892766 ]
  [0.66543466 0.626554   0.622714   ... 0.6798349  0.6601546  0.626074  ]
  ...
  [0.72879565 0.7014352  0.6519944  ... 0.61647385 0.5483128  0.54159266]
  [0.75423604 0.7379158  0.6716747  ... 0.5358326  0.4115106  0.37503   ]
  [0.73311573 0.73695576 0.72063553 ... 0.43935102 0.3063889  0.25454807]]]
img[i].shape (4, 256, 256)
[[[3.98042411e-01 2.42251217e-01 2.38988578e-01 ... 1.11745514e-01
   1.14192493e-01 1.81892335e-01]
  [2.92822182e-01 2.03099504e-01 2.01468185e-01 ... 1.50081560e-01
   3.01794447e-02 1.05220228e-01]
  [2.34910280e-01 2.17781410e-01 2.66721040e-01 ... 1.88417614e-01
   1.12561174e-01 1.48450240e-01]
  ...
  [2.28384987e-01 2.13703096e-01 2.13703096e-01 ... 0.00000000e+00
   0.00000000e+00 0.00000000e+00]
  [2.46329531e-01 1.73735723e-01 2.01468185e-01 ... 0.00000000e+00
   0.00000000e+00 0.00000000e+00]
  [2.23491028e-01 1.76998362e-01 2.01468185e-01 ... 0.00000000e+00
   0.00000000e+00 0.00000000e+00]]

 [[4.78283614e-01 3.40659350e-01 2.99843013e-01 ... 2.09314495e-01
   1.96755633e-01 2.32339084e-01]
  [3.51648360e-01 2.50130832e-01 2.46991098e-01 ... 2.11407647e-01
   1.27681836e-01 1.83150187e-01]
  [3.02459449e-01 2.48560965e-01 2.45944530e-01 ... 2.36002088e-01
   1.73207745e-01 2.00418636e-01]
  ...
  [3.35949779e-01 3.33856612e-01 3.29670340e-01 ... 5.23286231e-04
   0.00000000e+00 0.00000000e+00]
  [3.38042915e-01 2.94086874e-01 3.14495027e-01 ... 0.00000000e+00
   0.00000000e+00 0.00000000e+00]
  [3.30193609e-01 2.84667701e-01 3.05075884e-01 ... 0.00000000e+00
   0.00000000e+00 0.00000000e+00]]

 [[4.72803354e-01 3.00418407e-01 2.43514642e-01 ... 1.69037655e-01
   1.52301252e-01 2.20083684e-01]
  [3.23849380e-01 2.05857739e-01 2.27615058e-01 ... 1.88284516e-01
   8.36820081e-02 1.82426780e-01]
  [2.93723851e-01 2.51882851e-01 2.41004184e-01 ... 2.35146448e-01
   1.56485349e-01 1.91631794e-01]
  ...
  [2.64435142e-01 2.70292878e-01 2.71966517e-01 ... 1.42259412e-02
   9.20502096e-03 0.00000000e+00]
  [2.65271962e-01 2.17573225e-01 2.56903768e-01 ... 0.00000000e+00
   0.00000000e+00 0.00000000e+00]
  [2.65271962e-01 1.95815906e-01 2.25941420e-01 ... 0.00000000e+00
   0.00000000e+00 0.00000000e+00]]

 [[6.59351647e-01 5.86533666e-01 5.34164608e-01 ... 8.05985034e-01
   8.27930152e-01 8.40399027e-01]
  [6.07980072e-01 5.25685787e-01 4.49875325e-01 ... 7.85536170e-01
   6.80797994e-01 7.22194493e-01]
  [5.73067307e-01 4.82294261e-01 4.12967592e-01 ... 7.61596024e-01
   6.28927708e-01 6.46882772e-01]
  ...
  [1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 3.14214453e-02
   2.34413967e-02 1.04738157e-02]
  [1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 9.97506198e-04
   0.00000000e+00 0.00000000e+00]
  [1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 0.00000000e+00
   0.00000000e+00 0.00000000e+00]]]
img[i].shape (4, 256, 256)
[[[0.53561646 0.5        0.4630137  ... 0.6547945  0.6342466  0.51643836]
  [0.4890411  0.47534245 0.47260273 ... 0.6657534  0.6178082  0.52465755]
  [0.4958904  0.46986303 0.49452055 ... 0.7561644  0.86164385 0.6917808 ]
  ...
  [0.22328767 0.23835616 0.3109589  ... 0.6041096  0.6369863  0.53424656]
  [0.2369863  0.27260274 0.24657534 ... 0.63013697 0.6534247  0.61917806]
  [0.26575342 0.26712328 0.23561645 ... 0.59041095 0.6958904  0.6808219 ]]

 [[0.52380955 0.48677248 0.47001764 ... 0.69135803 0.6693122  0.57054675]
  [0.4823633  0.4770723  0.47530866 ... 0.7160494  0.68430334 0.61199296]
  [0.47619048 0.47354499 0.505291   ... 0.803351   0.8518519  0.71869487]
  ...
  [0.31040564 0.33421516 0.35361552 ... 0.5485009  0.56790125 0.5044092 ]
  [0.329806   0.3271605  0.28306878 ... 0.5537919  0.5617284  0.55202824]
  [0.32804233 0.30511463 0.25837743 ... 0.51763666 0.5802469  0.59347445]]

 [[0.5123859  0.46675357 0.47457626 ... 0.7822686  0.7640157  0.60495436]
  [0.46023467 0.4589309  0.47718382 ... 0.779661   0.7444589  0.64276403]
  [0.43285528 0.43546283 0.505867   ... 0.87353325 0.9661017  0.7783572 ]
  ...
  [0.2398957  0.25684485 0.26336375 ... 0.5488918  0.55801827 0.47718382]
  [0.25032595 0.2398957  0.16558018 ... 0.5189048  0.5280313  0.5267275 ]
  [0.25684485 0.23337679 0.16558018 ... 0.46153846 0.55149937 0.5762712 ]]

 [[0.27483273 0.27174473 0.27431807 ... 0.5280494  0.49459597 0.46114257]
  [0.28306744 0.27740607 0.2835821  ... 0.5681935  0.5367988  0.49974266]
  [0.29387546 0.28924343 0.29747814 ... 0.57591355 0.5923829  0.55275345]
  ...
  [1.         1.         1.         ... 0.30571282 0.30571282 0.3237262 ]
  [1.         1.         1.         ... 0.32424086 0.33607823 0.35306227]
  [1.         1.         1.         ... 0.3576943  0.37004632 0.37982503]]]
img[i].shape (4, 256, 256)
[[[1.7897092e-01 1.7685470e-01 1.7368039e-01 ... 4.5347359e-02
   4.1719571e-02 3.5370942e-02]
  [1.7625007e-01 1.7534313e-01 1.7005260e-01 ... 4.5196202e-02
   4.2777676e-02 3.7487153e-02]
  [1.6189007e-01 1.6355282e-01 1.6279702e-01 ... 4.9579781e-02
   3.9905678e-02 4.8672833e-02]
  ...
  [0.0000000e+00 1.8138945e-03 2.8719995e-03 ... 1.9650523e-01
   2.0285386e-01 2.0391196e-01]
  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.0890017e-01
   2.0965597e-01 2.1963239e-01]
  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.1751617e-01
   2.0950481e-01 2.1131870e-01]]

 [[1.9591606e-01 1.9418362e-01 1.9048774e-01 ... 7.7994525e-02
   7.5453609e-02 7.1180254e-02]
  [1.8933278e-01 1.8956377e-01 1.8413545e-01 ... 7.8687504e-02
   7.6955058e-02 7.3028192e-02]
  [1.7339432e-01 1.7547324e-01 1.7489576e-01 ... 8.5848264e-02
   7.9726964e-02 8.6887725e-02]
  ...
  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.5435710e-01
   2.5920793e-01 2.5932342e-01]
  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.6509824e-01
   2.6544473e-01 2.7145052e-01]
  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.7283648e-01
   2.6752365e-01 2.6636869e-01]]

 [[2.0516102e-01 2.0366816e-01 1.9918959e-01 ... 1.0108765e-01
   9.9168263e-02 9.1490723e-02]
  [1.9940285e-01 2.0110898e-01 1.9257838e-01 ... 9.6395820e-02
   9.4049901e-02 8.9784600e-02]
  [1.7871614e-01 1.8340798e-01 1.8362124e-01 ... 1.0919172e-01
   1.0258051e-01 1.2198763e-01]
  ...
  [2.1326508e-04 2.7724463e-03 3.8387715e-03 ... 2.4184261e-01
   2.4632117e-01 2.4632117e-01]
  [0.0000000e+00 1.4928556e-03 2.7724463e-03 ... 2.5890383e-01
   2.5399873e-01 2.6487523e-01]
  [1.9193857e-03 4.2653017e-04 0.0000000e+00 ... 2.6722115e-01
   2.5570485e-01 2.5271913e-01]]

 [[2.1075985e-01 2.0975143e-01 2.1008757e-01 ... 3.5614043e-01
   3.5076219e-01 3.4572008e-01]
  [1.9781846e-01 2.0033950e-01 2.0084371e-01 ... 3.5782114e-01
   3.5042605e-01 3.4370327e-01]
  [1.8403670e-01 1.8723004e-01 1.8857460e-01 ... 3.5832536e-01
   3.5160255e-01 3.4958571e-01]
  ...
  [0.0000000e+00 6.7228021e-04 3.3614010e-04 ... 3.1899697e-01
   3.2000539e-01 3.2185414e-01]
  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 3.2656011e-01
   3.2740045e-01 3.2891309e-01]
  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 3.3529976e-01
   3.3378711e-01 3.3462748e-01]]]
img[i].shape (4, 256, 256)
[[[0.46710527 0.39078948 0.3736842  ... 0.17763157 0.23947369 0.24473684]
  [0.55657893 0.3263158  0.27763158 ... 0.10526316 0.26710525 0.21973684]
  [0.44473684 0.25921053 0.3223684  ... 0.09078947 0.13026316 0.27631578]
  ...
  [0.11578947 0.21315789 0.1368421  ... 0.53289473 0.79473686 0.7223684 ]
  [0.10526316 0.09473684 0.         ... 0.7368421  0.69736844 0.58157897]
  [0.         0.02368421 0.09078947 ... 0.6723684  0.5236842  0.36447367]]

 [[0.51314855 0.44065386 0.37668797 ... 0.09097371 0.15067519 0.19260839]
  [0.56218904 0.39872068 0.36247334 ... 0.02061123 0.15209666 0.18834399]
  [0.5209666  0.43141437 0.5074627  ... 0.04193319 0.07462686 0.21606255]
  ...
  [0.22814499 0.2594172  0.29282162 ... 0.5444208  0.7228145  0.71073204]
  [0.10874201 0.05117271 0.1108742  ... 0.7064677  0.7050462  0.554371  ]
  [0.         0.02842928 0.18905473 ... 0.6702203  0.5060412  0.3105899 ]]

 [[0.60489184 0.46660396 0.39416745 ... 0.         0.08654751 0.14393227]
  [0.63405454 0.46754467 0.44120413 ... 0.         0.05738476 0.11006586]
  [0.65098774 0.60865474 0.6462841  ... 0.         0.         0.12511759]
  ...
  [0.20413923 0.20319849 0.2869238  ... 0.5399812  0.63405454 0.65945435]
  [0.05926623 0.         0.11288805 ... 0.6763876  0.6491063  0.51646286]
  [0.         0.09971778 0.261524   ... 0.61523986 0.39793038 0.2784572 ]]

 [[0.2749504  0.24785195 0.1956378  ... 0.92333114 0.89689356 0.9299405 ]
  [0.32385987 0.24917382 0.18638466 ... 0.9504296  0.8526107  0.8876405 ]
  [0.28552544 0.20819564 0.18638466 ... 0.9239921  0.8631857  0.8830139 ]
  ...
  [0.6054197  0.48512888 0.4560476  ... 0.28883013 0.23925975 0.23859881]
  [0.333113   0.23198943 0.3159286  ... 0.311963   0.36153337 0.3912756 ]
  [0.2161269  0.2247191  0.36483806 ... 0.38003966 0.4871117  0.4699273 ]]]
img[i].shape (4, 256, 256)
[[[0.23875433 0.24359861 0.16055363 ... 0.3183391  0.29757786 0.34602076]
  [0.31903115 0.30034602 0.24705882 ... 0.29550174 0.27404845 0.29550174]
  [0.34532872 0.3100346  0.2885813  ... 0.3183391  0.31418684 0.26989618]
  ...
  [0.08512111 0.08235294 0.1266436  ... 0.14117648 0.13494809 0.11695502]
  [0.04913495 0.06228374 0.12456747 ... 0.20415226 0.19930796 0.19861592]
  [0.03944637 0.0532872  0.11280277 ... 0.31764707 0.32110727 0.2782007 ]]

 [[0.2685665  0.28195164 0.23272884 ... 0.36010364 0.33981    0.36183074]
  [0.32297063 0.324266   0.28583765 ... 0.33290157 0.31692573 0.32512954]
  [0.36787564 0.34974092 0.31951642 ... 0.37003455 0.35621762 0.31692573]
  ...
  [0.14507772 0.13255613 0.15328152 ... 0.19991365 0.19991365 0.20034543]
  [0.12737478 0.12435233 0.15198618 ... 0.24050087 0.24525043 0.2525907 ]
  [0.12780656 0.12478411 0.14378238 ... 0.3255613  0.32642487 0.2974957 ]]

 [[0.29610807 0.31838754 0.25154912 ... 0.38801086 0.35180673 0.361554  ]
  [0.33300844 0.32604608 0.28357586 ... 0.30864024 0.30585533 0.32743856]
  [0.39706188 0.3573766  0.33509713 ... 0.3866184  0.37687114 0.32117245]
  ...
  [0.09837778 0.08445311 0.12065724 ... 0.21116759 0.22439602 0.19306551]
  [0.06426234 0.07122467 0.12204971 ... 0.24528302 0.26268885 0.27522105]
  [0.06565481 0.07540207 0.11508738 ... 0.34762934 0.3490218  0.30794403]]

 [[0.49756992 0.5606893  0.5638452  ... 0.644638   0.64779395 0.644638  ]
  [0.5165057  0.5537461  0.5537461  ... 0.58214986 0.57141954 0.60108566]
  [0.5228177  0.546803   0.53354794 ... 0.5733131  0.5543773  0.5922489 ]
  ...
  [1.         1.         1.         ... 0.37196237 0.38269266 0.39594772]
  [1.         1.         1.         ... 0.36754403 0.37322477 0.38332388]
  [1.         1.         1.         ... 0.36438805 0.37385595 0.38458624]]]
img[i].shape (4, 256, 256)
[[[0.8114374  0.8124678  0.82843894 ... 0.13034518 0.12879959 0.124678  ]
  [0.81555897 0.84853166 0.8371973  ... 0.13858835 0.13292117 0.1241628 ]
  [0.82225657 0.8521381  0.86604846 ... 0.14477074 0.13755795 0.13601236]
  ...
  [0.6584235  0.5929933  0.49046883 ... 0.77073675 0.7846471  0.77846473]
  [0.576507   0.62339    0.51107675 ... 0.7712519  0.8016486  0.80113345]
  [0.4971664  0.49252963 0.52344155 ... 0.7418856  0.8031942  0.82483256]]

 [[0.82213974 0.81977725 0.838002   ... 0.16334796 0.16503544 0.16841039]
  [0.8153898  0.83732706 0.84238946 ... 0.16638543 0.17077287 0.17144786]
  [0.8299021  0.8464394  0.8646642  ... 0.16976038 0.1761728  0.18157274]
  ...
  [0.6395545  0.5858927  0.5123186  ... 0.7532906  0.7654404  0.76476544]
  [0.5599055  0.6091799  0.54100573 ... 0.7516031  0.7752278  0.77792776]
  [0.522106   0.50759363 0.52413094 ... 0.7374283  0.7762403  0.7924401 ]]

 [[0.76082003 0.75170845 0.78246015 ... 0.13667426 0.13724373 0.14578588]
  [0.74658316 0.78416854 0.7853075  ... 0.14407745 0.14806378 0.15091117]
  [0.77733487 0.79897493 0.8211845  ... 0.16514806 0.16628702 0.1714123 ]
  ...
  [0.68507975 0.55979496 0.51082003 ... 0.702164   0.70615035 0.7107062 ]
  [0.5859909  0.67027336 0.5774487  ... 0.7107062  0.72323465 0.72835994]
  [0.54157174 0.53416854 0.55979496 ... 0.70102507 0.7300683  0.75      ]]

 [[0.5731707  0.5785061  0.58536583 ... 0.4702744  0.4695122  0.4832317 ]
  [0.56897867 0.58231705 0.5956555  ... 0.49771342 0.5186738  0.5259146 ]
  [0.57012194 0.5849848  0.5987043  ... 0.5362043  0.57050306 0.5613567 ]
  ...
  [0.5884146  0.5777439  0.5476372  ... 0.52515244 0.5453506  0.5659299 ]
  [0.55983233 0.57240856 0.55678356 ... 0.53125    0.5499238  0.56669205]
  [0.5297256  0.5293445  0.53887194 ... 0.543064   0.55792683 0.5659299 ]]]
img[i].shape (4, 256, 256)
/home/bertille/.local/lib/python3.8/site-packages/pydantic/main.py:347: UserWarning: Pydantic serializer warnings:
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
/home/bertille/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/bertille/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[[[0.2452381  0.26309523 0.28690475 ... 0.2345238  0.27857143 0.3797619 ]
  [0.19761905 0.2809524  0.24166666 ... 0.30119047 0.3        0.33333334]
  [0.19166666 0.25119048 0.22738095 ... 0.4154762  0.3595238  0.32857144]
  ...
  [0.2        0.25357142 0.38333333 ... 0.         0.01547619 0.07380953]
  [0.10714286 0.17023809 0.24047619 ... 0.0452381  0.02142857 0.05      ]
  [0.08452381 0.11190476 0.17380953 ... 0.03571429 0.05833333 0.0452381 ]]

 [[0.20459953 0.22045995 0.24187154 ... 0.23949246 0.27359238 0.36954796]
  [0.15384616 0.21411578 0.20063442 ... 0.29500395 0.31007138 0.3592387 ]
  [0.1403648  0.18477398 0.18715306 ... 0.4147502  0.3743061  0.3663759 ]
  ...
  [0.16256939 0.25614592 0.41712925 ... 0.         0.02141158 0.08485329]
  [0.08485329 0.14591594 0.25614592 ... 0.04520222 0.02616971 0.06344172]
  [0.0555115  0.08247422 0.14908803 ... 0.04599524 0.05709754 0.04599524]]

 [[0.24129651 0.2364946  0.26530612 ... 0.18007202 0.21728691 0.32893157]
  [0.16686675 0.2484994  0.22208883 ... 0.24729893 0.27611044 0.34453782]
  [0.1632653  0.22208883 0.20768307 ... 0.40216085 0.35054022 0.34933972]
  ...
  [0.1572629  0.26050422 0.4789916  ... 0.         0.05402161 0.15126051]
  [0.07563026 0.13685474 0.2785114  ... 0.06482593 0.04681873 0.11284514]
  [0.05522209 0.07442977 0.1452581  ... 0.09123649 0.07442977 0.06842737]]

 [[0.08079368 0.10218024 0.10990316 ... 0.29109487 0.2762431  0.25485653]
  [0.07544704 0.1051506  0.10277431 ... 0.35109606 0.34456128 0.32852137]
  [0.09505139 0.11109131 0.10218024 ... 0.3974336  0.3980277  0.38852254]
  ...
  [0.08138775 0.0926751  0.13485415 ... 0.30178815 0.2946593  0.30238223]
  [0.0267332  0.03326799 0.09089289 ... 0.27743125 0.27030239 0.2697083 ]
  [0.         0.00772293 0.05940712 ... 0.26673797 0.24594547 0.23941068]]]
img[i].shape (4, 256, 256)
[[[0.13756613 0.13756613 0.15079366 ... 0.46560848 0.4074074  0.39417988]
  [0.22751322 0.21428572 0.23544973 ... 0.42328042 0.521164   0.44973546]
  [0.30952382 0.3068783  0.33068782 ... 0.43650794 0.5        0.4814815 ]
  ...
  [0.08465608 0.06878307 0.08465608 ... 0.23280424 0.3042328  0.20899472]
  [0.12698413 0.0952381  0.1005291  ... 0.27248678 0.31746033 0.24338624]
  [0.15343915 0.11375661 0.14285715 ... 0.33862433 0.37566137 0.36772487]]

 [[0.12530607 0.13250756 0.14547026 ... 0.5775601  0.56315714 0.5732392 ]
  [0.19300014 0.20308223 0.22612703 ... 0.58044076 0.5977243  0.5862019 ]
  [0.2822987  0.29382113 0.3197465  ... 0.55595565 0.56747806 0.5790004 ]
  ...
  [0.09650007 0.07921648 0.08353738 ... 0.2765375  0.32694802 0.28085843]
  [0.11234337 0.09650007 0.10082097 ... 0.31398532 0.3341495  0.2923808 ]
  [0.14114936 0.12098517 0.15411206 ... 0.3874406  0.3874406  0.3730376 ]]

 [[0.11888112 0.11655012 0.12354312 ... 0.45687646 0.45920745 0.4755245 ]
  [0.12121212 0.11421911 0.16317016 ... 0.4918415  0.53613055 0.4988345 ]
  [0.19347319 0.16783217 0.22843823 ... 0.46853146 0.49417248 0.4965035 ]
  ...
  [0.09557109 0.10489511 0.12121212 ... 0.1958042  0.2820513  0.2144522 ]
  [0.13286713 0.11421911 0.12354312 ... 0.25174826 0.29137528 0.20512821]
  [0.14685315 0.12354312 0.17715618 ... 0.34265736 0.35431236 0.33333334]]

 [[0.246061   0.18106376 0.13028465 ... 0.91199255 0.9186664  0.88210547]
  [0.24867249 0.17438993 0.11171401 ... 0.94042885 0.9613208  0.9279517 ]
  [0.28668427 0.20746888 0.12564199 ... 0.9401387  0.9534863  0.94391084]
  ...
  [0.13260598 0.10852218 0.09575487 ... 0.3957868  0.371703   0.35922584]
  [0.16075212 0.14334214 0.14363231 ... 0.43379858 0.40478194 0.3957868 ]
  [0.20572788 0.19296056 0.2158837  ... 0.50692046 0.4790645  0.46542668]]]
img[i].shape (4, 256, 256)
[[[0.10393258 0.09269663 0.15168539 ... 1.         1.         0.9044944 ]
  [0.01123596 0.02808989 0.03370786 ... 1.         1.         1.        ]
  [0.17134832 0.03089888 0.0252809  ... 1.         0.9522472  0.76123595]
  ...
  [0.71067417 0.8398876  0.5898876  ... 0.42696628 0.4550562  0.33988765]
  [0.49438202 0.57865167 0.5449438  ... 0.5702247  0.6797753  0.58146065]
  [0.58426964 0.502809   0.502809   ... 0.67134833 0.55898875 0.6151685 ]]

 [[0.20248668 0.1616341  0.20959148 ... 1.         1.         0.95204264]
  [0.09058615 0.06927176 0.07637656 ... 1.         1.         1.        ]
  [0.1705151  0.03552398 0.04262878 ... 1.         1.         0.88987565]
  ...
  [0.7015986  0.76554173 0.58614564 ... 0.37655416 0.39076376 0.28596804]
  [0.55772644 0.60746    0.5506217  ... 0.47602132 0.54706925 0.4973357 ]
  [0.6554174  0.5843694  0.5239787  ... 0.57015985 0.56838363 0.6021314 ]]

 [[0.22779043 0.16628702 0.18678816 ... 0.9817768  1.         0.92938495]
  [0.07972665 0.0546697  0.01366743 ... 1.         1.         1.        ]
  [0.13667426 0.         0.         ... 1.         1.         0.7927107 ]
  ...
  [0.64692485 0.71753985 0.5284738  ... 0.38041002 0.38268793 0.27790433]
  [0.523918   0.56947607 0.5193622  ... 0.49658313 0.5876993  0.53758544]
  [0.59681094 0.5444191  0.49658313 ... 0.5899772  0.61047834 0.66287017]]

 [[0.8979206  0.731569   0.6899811  ... 0.63137996 0.72967863 0.67296785]
  [0.7372401  0.64272213 0.6540643  ... 0.57088846 0.6389414  0.5784499 ]
  [0.6540643  0.5992439  0.610586   ... 0.5330813  0.59546316 0.56143665]
  ...
  [0.75803405 0.705104   0.56710774 ... 0.31758034 0.22306238 0.10964083]
  [0.8279773  0.71455574 0.5652174  ... 0.2911153  0.20226844 0.11720227]
  [0.9697543  0.8279773  0.59546316 ... 0.30245748 0.24952741 0.1625709 ]]]
img[i].shape (4, 256, 256)
[[[0.18911918 0.2396373  0.26295337 ... 0.3380829  0.40025908 0.37046632]
  [0.1761658  0.22150259 0.19948186 ... 0.32124352 0.35103628 0.34326425]
  [0.28626943 0.28367877 0.25777203 ... 0.3341969  0.2681347  0.16321243]
  ...
  [0.21373057 0.2357513  0.30181348 ... 0.43782383 0.43911916 0.35880828]
  [0.2357513  0.26943004 0.34196892 ... 0.37176165 0.3277202  0.28626943]
  [0.34067357 0.3834197  0.4676166  ... 0.31088084 0.2992228  0.2966321 ]]

 [[0.29365668 0.31757817 0.34562403 ... 0.43636063 0.47430503 0.42728698]
  [0.29860595 0.31675327 0.2895323  ... 0.4652314  0.48420358 0.43718553]
  [0.3736699  0.36047184 0.33820012 ... 0.45780748 0.38934258 0.2854079 ]
  ...
  [0.3538728  0.31510353 0.3654211  ... 0.57329047 0.5840139  0.53534603]
  [0.3876928  0.35222304 0.40254062 ... 0.52214795 0.49410212 0.4652314 ]
  [0.44295967 0.44295967 0.49162748 ... 0.48420358 0.4652314  0.44708407]]

 [[0.23479319 0.25547445 0.3163017  ... 0.3272506  0.40145984 0.37226278]
  [0.24939173 0.2676399  0.23844282 ... 0.33454987 0.41240877 0.40632603]
  [0.3077859  0.3053528  0.29683697 ... 0.33819953 0.31751823 0.24939173]
  ...
  [0.2591241  0.21654502 0.29805353 ... 0.5182482  0.50851583 0.41970804]
  [0.29805353 0.24452555 0.3296837  ... 0.44768855 0.38929442 0.34428224]
  [0.39537713 0.38686132 0.44647202 ... 0.3673966  0.35036495 0.33454987]]

 [[0.39162996 0.42158592 0.42070484 ... 0.7898678  0.8        0.730837  ]
  [0.4246696  0.42995596 0.42158592 ... 0.8405286  0.80528635 0.6801762 ]
  [0.5154185  0.5030837  0.47180617 ... 0.8290749  0.73568285 0.56431717]
  ...
  [0.57621145 0.51013213 0.44581497 ... 0.9171806  0.9035242  0.9242291 ]
  [0.65286344 0.5722467  0.49295154 ... 0.9753304  0.9462555  0.9511013 ]
  [0.7171806  0.64096916 0.5497797  ... 1.         0.9797357  0.9634361 ]]]
Plot saved at: ../../results/unet_256_l1/random_shuffling/UNet_random_data_augmentation/metrics_test/test_preds.png
