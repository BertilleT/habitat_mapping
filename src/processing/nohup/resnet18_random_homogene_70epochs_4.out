INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.10 (you have 1.4.9). Upgrade using: pip install --upgrade albumentations
----------------------- UNet -----------------------
Patch size: 256
Classification level: 1
Using device: cuda
Loading data...
Data loading settings:
Splitting data: [0.6, 0.2]
Stratified: random
Year: all
Patches: homogeneous
Batch size: 16
Normalisation: channel_by_channel
No data augmentation
The seed to shuffle the data is  1
The data are from  all  year
5716 homogeneous masks found
Image shape: torch.Size([16, 4, 256, 256]), Mask shape: torch.Size([16])
Image: min: 0.0, max: 1.0, dtype: torch.float32
Mask: [1], dtype: torch.uint8
Train: 3429 images, Val: 1143 images, Test: 1144 images
Train: 59.99%, Val: 20.00%, Test: 20.01%
Creating model...
Model settings:
Pretrained: None
Classes: 6
The model is Resnet18
Creating optimizer...
Training settings:
Learning rate: 0.0001
Criterion: CrossEntropy
Optimizer: Adam
Training...
Epoch 1/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 1/70: train loss 1.1879, val loss 1.0195
Epoch 1/70: train mF1 0.4182, val mF1 0.4955
Time: 0:00:34.108886
Epoch 2/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 2/70: train loss 0.9276, val loss 1.0050
Epoch 2/70: train mF1 0.5654, val mF1 0.5267
Time: 0:00:33.878870
Epoch 3/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 3/70: train loss 0.7602, val loss 1.0100
Epoch 3/70: train mF1 0.6450, val mF1 0.5653
Time: 0:00:33.824408
Epoch 4/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 4/70: train loss 0.5992, val loss 0.9993
Epoch 4/70: train mF1 0.7153, val mF1 0.5901
Time: 0:00:33.970397
Epoch 5/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 5/70: train loss 0.3844, val loss 1.1629
Epoch 5/70: train mF1 0.8134, val mF1 0.5731
Time: 0:00:34.342714
Epoch 6/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 6/70: train loss 0.2642, val loss 1.0021
Epoch 6/70: train mF1 0.8758, val mF1 0.6346
Time: 0:00:34.293507
Epoch 7/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 7/70: train loss 0.1829, val loss 1.6339
Epoch 7/70: train mF1 0.9211, val mF1 0.5715
Time: 0:00:34.055554
Epoch 8/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 8/70: train loss 0.1262, val loss 1.2236
Epoch 8/70: train mF1 0.9529, val mF1 0.5859
Time: 0:00:33.883978
Epoch 9/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 9/70: train loss 0.0955, val loss 1.3823
Epoch 9/70: train mF1 0.9671, val mF1 0.5934
Time: 0:00:33.422797
Epoch 10/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 10/70: train loss 0.0703, val loss 1.4164
Epoch 10/70: train mF1 0.9753, val mF1 0.6255
Time: 0:00:33.551038
Epoch 11/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 11/70: train loss 0.0869, val loss 1.1970
Epoch 11/70: train mF1 0.9687, val mF1 0.6172
Time: 0:00:33.892212
Epoch 12/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 12/70: train loss 0.0652, val loss 1.2289
Epoch 12/70: train mF1 0.9821, val mF1 0.6177
Time: 0:00:33.677227
Epoch 13/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 13/70: train loss 0.0516, val loss 1.1059
Epoch 13/70: train mF1 0.9816, val mF1 0.6118
Time: 0:00:34.412547
Epoch 14/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 14/70: train loss 0.0645, val loss 1.3225
Epoch 14/70: train mF1 0.9780, val mF1 0.6057
Time: 0:00:35.174380
Epoch 15/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 15/70: train loss 0.0604, val loss 1.3820
Epoch 15/70: train mF1 0.9817, val mF1 0.6097
Time: 0:00:33.685774
Epoch 16/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 16/70: train loss 0.0532, val loss 1.4419
Epoch 16/70: train mF1 0.9834, val mF1 0.5697
Time: 0:00:33.268826
Epoch 17/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 17/70: train loss 0.0510, val loss 1.4092
Epoch 17/70: train mF1 0.9854, val mF1 0.5907
Time: 0:00:33.266558
Epoch 18/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 18/70: train loss 0.0488, val loss 1.2905
Epoch 18/70: train mF1 0.9863, val mF1 0.6189
Time: 0:00:34.145372
Epoch 19/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 19/70: train loss 0.0665, val loss 1.4712
Epoch 19/70: train mF1 0.9772, val mF1 0.5731
Time: 0:00:33.701227
Epoch 20/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 20/70: train loss 0.0857, val loss 2.1726
Epoch 20/70: train mF1 0.9679, val mF1 0.5317
Time: 0:00:34.049540
Epoch 21/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 21/70: train loss 0.0517, val loss 1.5124
Epoch 21/70: train mF1 0.9827, val mF1 0.6283
Time: 0:00:34.260868
Epoch 22/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 22/70: train loss 0.0337, val loss 1.5228
Epoch 22/70: train mF1 0.9891, val mF1 0.5968
Time: 0:00:33.856805
Epoch 23/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 23/70: train loss 0.0261, val loss 1.1720
Epoch 23/70: train mF1 0.9912, val mF1 0.6298
Time: 0:00:33.497105
Epoch 24/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 24/70: train loss 0.0555, val loss 1.8433
Epoch 24/70: train mF1 0.9788, val mF1 0.5675
Time: 0:00:33.637384
Epoch 25/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 25/70: train loss 0.0329, val loss 1.4472
Epoch 25/70: train mF1 0.9905, val mF1 0.6052
Time: 0:00:33.364144
Epoch 26/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 26/70: train loss 0.0406, val loss 1.4629
Epoch 26/70: train mF1 0.9839, val mF1 0.6074
Time: 0:00:33.434212
Epoch 27/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 27/70: train loss 0.0501, val loss 2.2207
Epoch 27/70: train mF1 0.9781, val mF1 0.4546
Time: 0:00:33.197766
Epoch 28/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 28/70: train loss 0.0428, val loss 1.4506
Epoch 28/70: train mF1 0.9837, val mF1 0.6094
Time: 0:00:33.552583
Epoch 29/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 29/70: train loss 0.0472, val loss 3.3409
Epoch 29/70: train mF1 0.9813, val mF1 0.4710
Time: 0:00:33.504162
Epoch 30/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 30/70: train loss 0.0627, val loss 1.8083
Epoch 30/70: train mF1 0.9817, val mF1 0.5624
Time: 0:00:33.265830
Epoch 31/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 31/70: train loss 0.0272, val loss 1.3679
Epoch 31/70: train mF1 0.9934, val mF1 0.6511
Time: 0:00:33.639033
Epoch 32/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 32/70: train loss 0.0194, val loss 1.3001
Epoch 32/70: train mF1 0.9941, val mF1 0.6264
Time: 0:00:33.100602
Epoch 33/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 33/70: train loss 0.0546, val loss 1.7249
Epoch 33/70: train mF1 0.9820, val mF1 0.5397
Time: 0:00:33.400470
Epoch 34/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 34/70: train loss 0.0557, val loss 1.5563
Epoch 34/70: train mF1 0.9823, val mF1 0.6247
Time: 0:00:33.448526
Epoch 35/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 35/70: train loss 0.0286, val loss 1.6279
Epoch 35/70: train mF1 0.9917, val mF1 0.6031
Time: 0:00:33.119753
Epoch 36/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 36/70: train loss 0.0139, val loss 1.2722
Epoch 36/70: train mF1 0.9968, val mF1 0.6517
Time: 0:00:34.072968
Epoch 37/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 37/70: train loss 0.0160, val loss 1.4795
Epoch 37/70: train mF1 0.9945, val mF1 0.6049
Time: 0:00:33.905342
Epoch 38/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 38/70: train loss 0.0164, val loss 1.4118
Epoch 38/70: train mF1 0.9955, val mF1 0.6152
Time: 0:00:34.061497
Epoch 39/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 39/70: train loss 0.0340, val loss 1.8062
Epoch 39/70: train mF1 0.9854, val mF1 0.5950
Time: 0:00:33.537295
Epoch 40/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 40/70: train loss 0.0250, val loss 1.5798
Epoch 40/70: train mF1 0.9916, val mF1 0.5818
Time: 0:00:33.249503
Epoch 41/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 41/70: train loss 0.0553, val loss 1.7718
Epoch 41/70: train mF1 0.9805, val mF1 0.5387
Time: 0:00:33.729907
Epoch 42/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 42/70: train loss 0.0701, val loss 1.5026
Epoch 42/70: train mF1 0.9725, val mF1 0.6177
Time: 0:00:33.912221
Epoch 43/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 43/70: train loss 0.0490, val loss 1.8240
Epoch 43/70: train mF1 0.9837, val mF1 0.5630
Time: 0:00:33.471650
Epoch 44/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 44/70: train loss 0.0350, val loss 1.7309
Epoch 44/70: train mF1 0.9897, val mF1 0.6168
Time: 0:00:33.468214
Epoch 45/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 45/70: train loss 0.0104, val loss 1.7717
Epoch 45/70: train mF1 0.9971, val mF1 0.5726
Time: 0:00:33.459627
Epoch 46/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 46/70: train loss 0.0116, val loss 1.6153
Epoch 46/70: train mF1 0.9974, val mF1 0.6187
Time: 0:00:33.564535
Epoch 47/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 47/70: train loss 0.0162, val loss 1.5007
Epoch 47/70: train mF1 0.9950, val mF1 0.6366
Time: 0:00:33.442221
Epoch 48/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 48/70: train loss 0.0159, val loss 2.2017
Epoch 48/70: train mF1 0.9939, val mF1 0.5551
Time: 0:00:33.715519
Epoch 49/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 49/70: train loss 0.0223, val loss 1.8857
Epoch 49/70: train mF1 0.9932, val mF1 0.5352
Time: 0:00:33.625539
Epoch 50/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 50/70: train loss 0.0350, val loss 2.0255
Epoch 50/70: train mF1 0.9867, val mF1 0.5521
Time: 0:00:33.257385
Epoch 51/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 51/70: train loss 0.0545, val loss 1.9005
Epoch 51/70: train mF1 0.9816, val mF1 0.5812
Time: 0:00:34.282229
Epoch 52/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 52/70: train loss 0.0629, val loss 1.7940
Epoch 52/70: train mF1 0.9750, val mF1 0.5968
Time: 0:00:33.783341
Epoch 53/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 53/70: train loss 0.0337, val loss 1.7197
Epoch 53/70: train mF1 0.9913, val mF1 0.5873
Time: 0:00:33.780577
Epoch 54/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 54/70: train loss 0.0457, val loss 1.6296
Epoch 54/70: train mF1 0.9827, val mF1 0.6122
Time: 0:00:33.940378
Epoch 55/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 55/70: train loss 0.0457, val loss 2.0254
Epoch 55/70: train mF1 0.9857, val mF1 0.5889
Time: 0:00:33.656878
Epoch 56/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 56/70: train loss 0.0391, val loss 1.5823
Epoch 56/70: train mF1 0.9904, val mF1 0.5957
Time: 0:00:33.781897
Epoch 57/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 57/70: train loss 0.0187, val loss 1.6188
Epoch 57/70: train mF1 0.9948, val mF1 0.6256
Time: 0:00:33.518803
Epoch 58/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 58/70: train loss 0.0101, val loss 1.5539
Epoch 58/70: train mF1 0.9968, val mF1 0.6195
Time: 0:00:33.608547
Epoch 59/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 59/70: train loss 0.0129, val loss 1.6226
Epoch 59/70: train mF1 0.9958, val mF1 0.6110
Time: 0:00:33.366446
Epoch 60/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 60/70: train loss 0.0069, val loss 1.7453
Epoch 60/70: train mF1 0.9988, val mF1 0.6064
Time: 0:00:34.001317
Epoch 61/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 61/70: train loss 0.0243, val loss 2.3186
Epoch 61/70: train mF1 0.9938, val mF1 0.5607
Time: 0:00:33.740787
Epoch 62/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 62/70: train loss 0.0230, val loss 1.7321
Epoch 62/70: train mF1 0.9923, val mF1 0.6175
Time: 0:00:33.333235
Epoch 63/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 63/70: train loss 0.0143, val loss 1.6956
Epoch 63/70: train mF1 0.9950, val mF1 0.5846
Time: 0:00:34.032450
Epoch 64/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 64/70: train loss 0.0166, val loss 1.8494
Epoch 64/70: train mF1 0.9940, val mF1 0.5537
Time: 0:00:33.971498
Epoch 65/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 65/70: train loss 0.0392, val loss 1.8090
Epoch 65/70: train mF1 0.9852, val mF1 0.5947
Time: 0:00:33.493725
Epoch 66/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 66/70: train loss 0.0530, val loss 1.7867
Epoch 66/70: train mF1 0.9819, val mF1 0.5817
Time: 0:00:33.988250
Epoch 67/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 67/70: train loss 0.0257, val loss 1.7519
Epoch 67/70: train mF1 0.9917, val mF1 0.6003
Time: 0:00:33.552538
Epoch 68/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 68/70: train loss 0.0506, val loss 1.6965
Epoch 68/70: train mF1 0.9797, val mF1 0.5510
Time: 0:00:33.179209
Epoch 69/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 69/70: train loss 0.0141, val loss 1.5653
Epoch 69/70: train mF1 0.9983, val mF1 0.6267
Time: 0:00:33.778016
Epoch 70/70
Training
Batch: 0  over  215
Batch: 50  over  215
Batch: 100  over  215
Batch: 150  over  215
Batch: 200  over  215
Validation
Batch: 0  over  72
Batch: 50  over  72
Epoch 70/70: train loss 0.0395, val loss 1.8018
Epoch 70/70: train mF1 0.9849, val mF1 0.6030
/home/bertille/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Time: 0:00:34.018445
Testing
Batch: 0  over  72
Batch: 50  over  72
Test F1 by class: [0.46153846 0.63201663 0.81367925 0.73740053 0.65201465 0.4       ]
Test mF1: 0.6161082535594572
Plot saved at: ../../results/resnet18_256_l1/random_shuffling/resnet18_random_homogene_70epochs/metrics_test/test_preds.png
